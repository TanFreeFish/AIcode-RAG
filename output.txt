├── ./
│   ├── config.py
│   └── run_demo.py
│   ├── RAG/
│   │   ├── __init__.py
│   │   ├── document_loader.py
│   │   ├── embeddings.py
│   │   ├── retriever.py
│   │   ├── text_splitter.py
│   │   └── vector_store.py
│   ├── backend/
│   │   ├── __init__.py
│   │   ├── ai_service.py
│   │   ├── main.py
│   │   └── requirements.txt
│   └── frontend/
│       ├── index.html
│       ├── script.js
│       └── style.css
=== FILE: config.py ===
import os

# 基础路径配置
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, 'data')
DOCUMENTS_DIR = os.path.join(DATA_DIR, 'documents')
VECTOR_STORE_DIR = os.path.join(DATA_DIR, 'vector_store')

# RAG配置
RAG_CONFIG = {
    # 文档加载配置
    "document_loader": {
        "extensions": [".txt", ".pdf", ".docx", ".pptx", ".md"]
    },
    
    # 文本分割配置 - 优化参数
    "text_splitter": {
        "chunk_size": 1500,  # 增大块大小
        "chunk_overlap": 100  # 减小重叠
    },
    
    # 嵌入模型配置 - 使用更快的模型
    "embeddings": {
        "model_type": "ollama",  # ollama 或 huggingface
        "model_name": "all-minilm"  # 更轻量级的模型
    },
    
    # 向量存储配置
    "vector_store": {
        "type": "annoy",  # 确保与实现一致
        "index_name": "document_index"
    },
    
    # 检索器配置
    "retriever": {
        "top_k": 4,
        "score_threshold": 0.6
    }
}
=== END ===

=== FILE: run_demo.py ===
# run_demo.py
import subprocess
import webbrowser
import time
import os
import shutil
import sys
from pathlib import Path
import requests
import logging

# 设置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

if getattr(sys, 'frozen', False):
    BASE_DIR = Path(sys.executable).parent
else:
    BASE_DIR = Path(__file__).resolve().parent

# 添加项目根目录到Python路径
sys.path.append(str(BASE_DIR))

def initialize_rag():
    """初始化RAG系统，强制重建向量索引"""
    # 删除现有向量库
    vector_store_dir = BASE_DIR / "data" / "vector_store"
    if vector_store_dir.exists():
        logger.info("Removing existing vector store...")
        shutil.rmtree(vector_store_dir)
    
    # 检查Ollama服务是否运行
    logger.info("Checking Ollama service...")
    try:
        response = requests.get("http://localhost:11434", timeout=5)
        if response.status_code != 200:
            logger.error("Ollama service not running. Please start Ollama first.")
            return None
    except Exception as e:
        logger.error(f"Ollama service not running: {str(e)}. Please start Ollama first.")
        return None
    
    # 导入RAG初始化函数
    try:
        from RAG import initialize_rag_system
    except ImportError as e:
        logger.error(f"Error importing RAG module: {str(e)}")
        return None
    
    # 创建RAG系统
    logger.info("Initializing RAG system...")
    try:
        rag_retriever = initialize_rag_system(force_rebuild=True)
        logger.info("RAG system initialized successfully")
        return rag_retriever
    except Exception as e:
        logger.error(f"Error initializing RAG system: {str(e)}")
        return None

def run_demo():
    # 初始化RAG系统
    rag_retriever = initialize_rag()
    if not rag_retriever:
        logger.error("Failed to initialize RAG system. Exiting.")
        return
    
    # 启动后端
    logger.info("Starting backend server...")
    backend_process = subprocess.Popen(
        [sys.executable, "-m", "uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"],
        cwd=BASE_DIR,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # 等待服务器启动
    time.sleep(5)
    
    # 检查服务器是否启动
    try:
        response = requests.get("http://localhost:8000", timeout=5)
        if response.status_code == 200:
            logger.info("Backend server started successfully")
        else:
            logger.warning(f"Backend returned status code: {response.status_code}")
    except Exception as e:
        logger.error(f"Failed to connect to backend: {str(e)}")
        backend_process.terminate()
        return
    
    # 打开前端
    logger.info("Opening chat interface in browser...")
    webbrowser.open('http://localhost:8000')
    
    logger.info("Chat interface opened. Press Ctrl+C to stop.")
    
    try:
        backend_process.wait()
    except KeyboardInterrupt:
        logger.info("Stopping server...")
        backend_process.terminate()
        logger.info("Server stopped")

if __name__ == "__main__":
    run_demo()
=== END ===

=== FILE: RAG\__init__.py ===
# RAG/__init__.py
from .document_loader import DocumentLoader
from .text_splitter import TextSplitter
from .embeddings import EmbeddingModel
from .vector_store import VectorStore
from .retriever import Retriever
from config import DOCUMENTS_DIR, VECTOR_STORE_DIR
import os
import time
from tqdm import tqdm
import logging

# 设置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def initialize_rag_system(force_rebuild=False):
    """初始化RAG系统"""
    # 检查向量存储是否存在
    vector_store = VectorStore()
    if not force_rebuild and vector_store.index_path.exists():
        logger.info("Using existing vector store")
        return Retriever()
    
    logger.info("Building new vector store...")
    
    # 加载文档
    loader = DocumentLoader()
    documents = loader.load_documents()
    if not documents:
        logger.warning("No documents found to build vector store")
        return Retriever()
    
    # 分割文档
    splitter = TextSplitter()
    chunks = splitter.split_documents(documents)
    logger.info(f"Split {len(documents)} documents into {len(chunks)} chunks")
    
    # 检查是否有文本块
    if not chunks:
        logger.warning("No text chunks created from documents")
        return Retriever()
    
    # 生成嵌入 - 添加批处理和进度条
    embedding_model = EmbeddingModel()
    texts = [chunk["text"] for chunk in chunks]
    
    # 分批处理嵌入生成
    batch_size = 32
    embeddings = []
    logger.info("Generating embeddings...")
    start_time = time.time()
    
    # 使用tqdm显示进度条
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding chunks"):
        batch_texts = texts[i:i+batch_size]
        batch_embeddings = embedding_model.embed_texts(batch_texts)
        
        # 检查嵌入是否有效
        if not batch_embeddings or any(len(emb) == 0 for emb in batch_embeddings):
            logger.warning(f"Some embeddings were empty in batch starting at index {i}")
        
        embeddings.extend(batch_embeddings)
    
    # 检查嵌入数量是否匹配
    if len(embeddings) != len(chunks):
        logger.warning(f"Embeddings count ({len(embeddings)}) doesn't match chunks count ({len(chunks)})")
    
    logger.info(f"Embeddings generated in {time.time()-start_time:.2f} seconds")
    
    # 添加到向量存储
    vector_store.add_chunks(chunks, embeddings)
    
    logger.info("Vector store built successfully")
    return Retriever()
=== END ===

=== FILE: RAG\document_loader.py ===
import os
from pathlib import Path
from config import DOCUMENTS_DIR, RAG_CONFIG
import PyPDF2
from docx import Document
import markdown

class DocumentLoader:
    def __init__(self):
        self.extensions = RAG_CONFIG["document_loader"]["extensions"]
        self.documents_dir = Path(DOCUMENTS_DIR)
        self.documents_dir.mkdir(parents=True, exist_ok=True)
    
    def load_documents(self):
        """加载文档目录中的所有支持文档"""
        documents = []
        
        for ext in self.extensions:
            for file_path in self.documents_dir.glob(f"*{ext}"):
                content = self._load_file(file_path)
                if content:
                    documents.append({
                        "file_path": str(file_path),
                        "content": content
                    })
        
        return documents
    
    def _load_file(self, file_path):
        """根据文件类型加载内容"""
        ext = file_path.suffix.lower()
        
        try:
            if ext == ".txt":
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            
            elif ext == ".pdf":
                content = []
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    for page in pdf_reader.pages:
                        content.append(page.extract_text())
                return "\n".join(content)
            
            elif ext == ".docx":
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs])
            
            elif ext == ".md":
                with open(file_path, 'r', encoding='utf-8') as f:
                    return markdown.markdown(f.read())
            
            else:
                print(f"Unsupported file type: {ext}")
                return None
        except Exception as e:
            print(f"Error loading {file_path}: {str(e)}")
            return None

    def add_document(self, file_path):
        """添加单个文档到存储"""
        dest_path = self.documents_dir / Path(file_path).name
        # 实际项目中应处理文件覆盖等问题
        with open(file_path, 'rb') as src, open(dest_path, 'wb') as dst:
            dst.write(src.read())
        return str(dest_path)
=== END ===

=== FILE: RAG\embeddings.py ===
import requests
import numpy as np
from typing import List
from config import RAG_CONFIG
import logging

# 设置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmbeddingModel:
    def __init__(self):
        config = RAG_CONFIG["embeddings"]
        self.model_type = config["model_type"]
        self.model_name = config["model_name"]
        self.dim = 384 if "minilm" in self.model_name.lower() else 768
    
    def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """将文本列表转换为嵌入向量"""
        if not texts:
            return []
            
        if self.model_type == "ollama":
            return self._embed_with_ollama(texts)
        elif self.model_type == "huggingface":
            return self._embed_with_huggingface(texts)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
    
    def _embed_with_ollama(self, texts: List[str]) -> List[List[float]]:
        """使用Ollama API生成嵌入 - 增强错误处理和重试机制"""
        embeddings = []
        for text in texts:
            for attempt in range(3):  # 最多重试3次
                try:
                    response = requests.post(
                        "http://localhost:11434/api/embeddings",
                        json={
                            "model": self.model_name,
                            "prompt": text
                        },
                        timeout=30
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        if "embedding" in data and data["embedding"]:
                            embedding = data["embedding"]
                            # 检查嵌入向量维度
                            if len(embedding) == self.dim:
                                embeddings.append(embedding)
                                break
                            else:
                                logger.warning(f"Embedding dimension mismatch: expected {self.dim}, got {len(embedding)}")
                        else:
                            logger.warning(f"Empty embedding for text: {text[:50]}...")
                    else:
                        logger.error(f"Error embedding text: {response.status_code} - {response.text}")
                    
                    # 最后一次尝试仍然失败
                    if attempt == 2:
                        logger.error(f"Failed to generate embedding after 3 attempts for text: {text[:50]}...")
                        embeddings.append([])
                except Exception as e:
                    logger.error(f"Ollama embedding error: {str(e)}")
                    if attempt == 2:
                        embeddings.append([])
        return embeddings
    """
     def _embed_with_huggingface(self, texts: List[str]) -> List[List[float]]:
        ""使用Hugging Face模型生成嵌入（真实实现）""
        # 实际实现应该使用transformers库
        try:
            from transformers import AutoTokenizer, AutoModel
            import torch
            
            # 加载模型和tokenizer
            tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            model = AutoModel.from_pretrained(self.model_name)
            
            # 编码文本
            inputs = tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                return_tensors="pt", 
                max_length=512
            )
            
            # 生成嵌入
            with torch.no_grad():
                outputs = model(**inputs)
            
            # 提取[CLS]标记的嵌入作为文本表示
            embeddings = outputs.last_hidden_state[:, 0, :].numpy().tolist()
            return embeddings
        except ImportError:
            logger.error("Hugging Face transformers library not installed")
            return [np.random.rand(self.dim).tolist() for _ in texts]
        except Exception as e:
            logger.error(f"Hugging Face embedding error: {str(e)}")
            return [np.random.rand(self.dim).tolist() for _ in texts]
    """
   
=== END ===

=== FILE: RAG\retriever.py ===
from .embeddings import EmbeddingModel
from .vector_store import VectorStore
from config import RAG_CONFIG
from pathlib import Path
class Retriever:
    def __init__(self):
        self.embedding_model = EmbeddingModel()
        self.vector_store = VectorStore()
        self.vector_store.load_index()
        self.top_k = RAG_CONFIG["retriever"]["top_k"]
        self.score_threshold = RAG_CONFIG["retriever"]["score_threshold"]
    
    def retrieve(self, query: str) -> str:
        """检索与查询相关的上下文"""
        # 生成查询嵌入
        query_embedding = self.embedding_model.embed_texts([query])
        if not query_embedding or not query_embedding[0]:
            return ""
        
        # 执行相似度搜索
        results = self.vector_store.similarity_search(
            query_embedding[0], 
            top_k=self.top_k
        )
        
        # 构建上下文
        context = []
        for score, chunk_id, chunk_data in results:
            if score >= self.score_threshold:
                context.append({
                    "text": chunk_data["text"],
                    "source": chunk_data["source"],
                    "score": round(score, 3)
                })
        
        # 格式化上下文
        return self._format_context(context)
    
    def _format_context(self, context_items) -> str:
        """格式化检索到的上下文"""
        if not context_items:
            return ""
        
        context_str = "检索到的相关上下文信息：\n\n"
        for i, item in enumerate(context_items, 1):
            source_name = Path(item["source"]).name
            context_str += f"### 上下文片段 {i} (来源: {source_name}, 相似度: {item['score']})\n"
            context_str += f"{item['text']}\n\n"
        
        return context_str.strip()
=== END ===

=== FILE: RAG\text_splitter.py ===
import re
from langchain_text_splitters import RecursiveCharacterTextSplitter
from config import RAG_CONFIG
from pathlib import Path

class TextSplitter:
    def __init__(self):
        config = RAG_CONFIG["text_splitter"]
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=config["chunk_size"],
            chunk_overlap=config["chunk_overlap"],
            separators=["\n\n", "\n", "。", "？", "！", "；", " ", ""],
            keep_separator=True
        )
    
    def split_documents(self, documents):
        """分割文档为文本块"""
        chunks = []
        for doc in documents:
            content = doc["content"]
            # 清理多余空白
            content = re.sub(r'\s+', ' ', content).strip()
            # 分割文本
            split_texts = self.splitter.split_text(content)
            
            for i, text in enumerate(split_texts):
                chunks.append({
                    "text": text,
                    "source": doc["file_path"],
                    "chunk_id": f"{Path(doc['file_path']).stem}_{i}"
                })
        
        return chunks
=== END ===

=== FILE: RAG\vector_store.py ===
# RAG/vector_store.py
from annoy import AnnoyIndex
import json
import numpy as np
import os
from pathlib import Path
from config import VECTOR_STORE_DIR, RAG_CONFIG
import logging

# 设置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorStore:
    def __init__(self):
        self.store_dir = Path(VECTOR_STORE_DIR)
        self.store_dir.mkdir(parents=True, exist_ok=True)
        config = RAG_CONFIG["vector_store"]
        self.store_type = config["type"]
        self.index_name = config["index_name"]
        self.index_path = self.store_dir / f"{self.index_name}.ann"
        self.metadata_path = self.store_dir / f"{self.index_name}_metadata.json"
        self.index = None
        self.metadata = {}
        
        # 从嵌入配置获取维度
        embedding_config = RAG_CONFIG["embeddings"]
        if embedding_config["model_name"] == "all-minilm":
            self.dim = 384  # all-minilm 嵌入维度
        else:
            self.dim = 768  # 默认嵌入维度
            
        self.distance_metric = "angular"  # 余弦相似度
        
        # 确保索引对象被正确创建
        self.load_index()
        
        # 双重检查索引对象
        if self.index is None:
            logger.warning("Index was None after load_index(), creating new index")
            self.index = AnnoyIndex(self.dim, self.distance_metric)

    def load_index(self):
        try:
            if self.index_path.exists():
                logger.info(f"Loading existing index from {self.index_path}")
                self.index = AnnoyIndex(self.dim, self.distance_metric)
                self.index.load(str(self.index_path))
                
                if self.metadata_path.exists():
                    with open(self.metadata_path, 'r', encoding='utf-8') as f:
                        self.metadata = json.load(f)
                logger.info(f"Loaded Annoy index with {len(self.metadata)} chunks")
            else:
                logger.info("No existing index found, creating new index")
                self.index = AnnoyIndex(self.dim, self.distance_metric)
        except Exception as e:
            logger.error(f"Error loading index: {str(e)}")
            # 创建新的索引作为回退
            self.index = AnnoyIndex(self.dim, self.distance_metric)

    def add_chunks(self, chunks, embeddings):
        if not embeddings:
            logger.warning("No embeddings provided, skipping add_chunks")
            return
            
        # 确保索引对象存在
        if self.index is None:
            logger.warning("Index is None, creating new index")
            self.index = AnnoyIndex(self.dim, self.distance_metric)
        
        # 确保嵌入是 numpy 数组
        embeddings = np.array(embeddings).astype('float32')
        
        # 添加向量到 Annoy 索引
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            if embedding.size == 0:
                logger.warning(f"Skipping empty embedding for chunk {i}")
                continue
                
            chunk_id = chunk["chunk_id"]
            self.index.add_item(i, embedding.tolist())
            self.metadata[chunk_id] = {
                "text": chunk["text"],
                "source": chunk["source"],
                "embedding": embedding.tolist()
            }
        
        # 构建索引
        logger.info("Building index...")
        self.index.build(10)  # 默认 10 棵树
        logger.info("Index built successfully")
        
        # 保存索引
        self.save_index()

    def similarity_search(self, query_embedding, top_k=5):
        if self.index is None:
            logger.warning("Index is None, cannot perform search")
            return []
            
        if not query_embedding:
            logger.warning("Empty query embedding")
            return []
            
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # 获取最近邻
        try:
            indices, distances = self.index.get_nns_by_vector(
                query_embedding[0], 
                top_k, 
                include_distances=True
            )
        except Exception as e:
            logger.error(f"Error in similarity_search: {str(e)}")
            return []
            
        results = []
        for idx, distance in zip(indices, distances):
            if idx < len(self.metadata):
                chunk_ids = list(self.metadata.keys())
                if idx < len(chunk_ids):
                    chunk_id = chunk_ids[idx]
                    chunk_data = self.metadata[chunk_id]
                    score = 1 - distance  # 转换为相似度得分
                    results.append((score, chunk_id, chunk_data))
        
        return sorted(results, key=lambda x: x[0], reverse=True)

    def save_index(self):
        if self.index is None:
            logger.warning("Index is None, cannot save")
            return
            
        self.index.save(str(self.index_path))
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        logger.info(f"Saved Annoy index with {len(self.metadata)} chunks")
=== END ===

=== FILE: backend\__init__.py ===

=== END ===

=== FILE: backend\ai_service.py ===
import requests
from typing import Dict, Any, Optional
from RAG import initialize_rag_system

class AIService:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.model_type = config.get('model_type', 'ollama')
        self.model_name = config.get('model_name', 'qwen:7b')
        self.rag_retriever = initialize_rag_system()
        
    def generate_response(self, prompt: str, use_rag: bool = False) -> str:
        """生成AI回复，支持RAG"""
        rag_context = None
        if use_rag:
            rag_context = self.rag_retriever.retrieve(prompt)
        
        full_prompt = self._build_prompt(prompt, rag_context)
        
        if self.model_type == 'ollama':
            return self._call_ollama(full_prompt)
        elif self.model_type == 'openai':
            return self._call_openai(full_prompt)
        # 可扩展其他API
        
    def _build_prompt(self, prompt: str, context: Optional[str]) -> str:
        """构建最终提示词，整合RAG内容"""
        if context:
            return (
                f"<|im_start|>system\n"
                f"你是一个AI助手，请基于以下上下文信息回答问题：\n\n"
                f"{context}\n"
                f"<|im_end|>\n"
                f"<|im_start|>user\n"
                f"{prompt}\n"
                f"<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )
        return (
            f"<|im_start|>user\n"
            f"{prompt}\n"
            f"<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )
    
    def _call_ollama(self, prompt: str) -> str:
        """调用本地Ollama服务"""
        try:
            url = "http://localhost:11434/api/generate"
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False
            }
            response = requests.post(url, json=payload, timeout=300)
            return response.json().get("response", "")
        except Exception as e:
            return f"Error: {str(e)}"
    
    def _call_openai(self, prompt: str) -> str:
        """调用OpenAI API（预留）"""
        # 实际使用时替换为真实API调用
        return f"OpenAI response to: {prompt}"
    
    # backend/ai_service.py
    def update_config(self, new_config: Dict[str, Any]):
        self.config.update(new_config)
        self.model_type = self.config.get('model_type', self.model_type)
        self.model_name = self.config.get('model_name', self.model_name)
        # 重新初始化 Retriever
        self.rag_retriever = initialize_rag_system()
=== END ===

=== FILE: backend\main.py ===
from fastapi import FastAPI, HTTPException, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import sys
from pathlib import Path

BASE_DIR = Path(__file__).resolve().parent.parent
sys.path.append(str(BASE_DIR / "backend"))

from ai_service import AIService
import os
from RAG.document_loader import DocumentLoader
from fastapi.responses import FileResponse
from RAG import initialize_rag_system

BASE_DIR = Path(__file__).resolve().parent.parent
app = FastAPI()
# 添加静态文件服务
    
app.mount("/static", StaticFiles(directory=BASE_DIR / "frontend"), name="static")

# 允许跨域
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# 初始化服务
ai_config = {"model_type": "ollama", "model_name": "qwen:7b"}
ai_service = AIService(ai_config)

class ChatRequest(BaseModel):
    message: str
    use_rag: bool = False

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    try:
        response = ai_service.generate_response(request.message, request.use_rag)
        return {"response": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/update_config")
async def update_config(new_config: dict):
    ai_service.update_config(new_config)
    return {"status": "config updated"}

# main.py中新增
@app.post("/rebuild_index")
async def rebuild_index():
    ai_service.rag_retriever = initialize_rag_system(force_rebuild=True)
    return {"status": "index rebuilt"}
# 修改上传文档端点

@app.post("/upload_document")
async def upload_document(file: UploadFile = File(...)):
    try:
        documents_dir = BASE_DIR / "data" / "documents"
        documents_dir.mkdir(parents=True, exist_ok=True)
        file_path = documents_dir / file.filename
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        # 重新加载向量存储
        ai_service.rag_retriever.vector_store.load_index()
        return {"status": "success", "file_path": str(file_path)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# 添加前端服务
@app.get("/")
async def serve_frontend():
    return FileResponse(BASE_DIR / "frontend" / "index.html")
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
     
=== END ===

=== FILE: backend\requirements.txt ===
# requirements.txt
fastapi>=0.68.0
uvicorn>=0.18.3
requests>=2.31.0
numpy>=1.24.3
PyPDF2>=3.0.1
python-docx>=0.8.11
markdown>=3.4.1
langchain-text-splitters>=0.0.1
annoy>=1.17.0
tqdm>=4.66.1  # 添加进度条库
=== END ===

=== FILE: frontend\index.html ===
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Chat Module</title>
    <link rel="stylesheet" href="/static/style.css">
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/dompurify@3.0.5/dist/purify.min.js"></script>
</head>
<body>
    <div class="chat-container">
        <div id="chat-history" class="chat-history"></div>
        
        <div class="input-area">
            <input type="text" id="user-input" placeholder="Type your message...">
            <button onclick="sendMessage()">Send</button>
            <label>
                <input type="checkbox" id="use-rag"> Use RAG
            </label>
        </div>
        <!-- 在配置面板下方添加 -->
<div class="document-upload">
    <h3>上传文档</h3>
    <input type="file" id="document-file">
    <button onclick="uploadDocument()">上传</button>
    <div id="upload-status"></div>
</div>
        <div class="config-panel">
            <h3>Configuration</h3>
            <select id="model-type">
                <option value="ollama">Ollama (Local)</option>
                <option value="openai">OpenAI API</option>
            </select>
            <input type="text" id="model-name" placeholder="Model name" value="llama3">
            <button onclick="updateConfig()">Update Config</button>
        </div>
    </div>

    <script src="/static/script.js"></script>
</body>
</html>
=== END ===

=== FILE: frontend\script.js ===
let chatHistory = [];

async function sendMessage() {
    const input = document.getElementById('user-input');
    const message = input.value.trim();
    const useRag = document.getElementById('use-rag').checked;
    
    if (!message) return;
    
    // 添加用户消息
    addMessage('user', message);
    input.value = '';
    
    try {
        // 发送到后端
        const response = await fetch('http://localhost:8000/chat', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                message: message,
                use_rag: useRag
            })
        });
        
        const data = await response.json();
        addMessage('ai', data.response);
    } catch (error) {
        addMessage('ai', `Error: ${error.message}`);
    }
}

async function updateConfig() {
    const modelType = document.getElementById('model-type').value;
    const modelName = document.getElementById('model-name').value;
    
    try {
        await fetch('http://localhost:8000/update_config', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                model_type: modelType,
                model_name: modelName
            })
        });
        alert('Configuration updated successfully!');
    } catch (error) {
        alert(`Update failed: ${error.message}`);
    }
}
async function uploadDocument() {
    const fileInput = document.getElementById('document-file');
    const file = fileInput.files[0];
    const statusDiv = document.getElementById('upload-status');
    
    if (!file) {
        statusDiv.textContent = "请选择文件";
        return;
    }
    
    statusDiv.textContent = "上传中...";
    
    try {
        const formData = new FormData();
        formData.append('file', file);
        
        const response = await fetch('http://localhost:8000/upload_document', {
            method: 'POST',
            body: formData
        });
        
        const data = await response.json();
        if (data.status === 'success') {
            statusDiv.textContent = `上传成功: ${data.file_path}`;
            // 清空文件输入
            fileInput.value = '';
        } else {
            statusDiv.textContent = `上传失败: ${data.detail || '未知错误'}`;
        }
    } catch (error) {
        statusDiv.textContent = `上传错误: ${error.message}`;
    }
}

// 添加页面加载事件
document.addEventListener('DOMContentLoaded', () => {
    // 初始加载完成后滚动到底部
    const chatHistory = document.getElementById('chat-history');
    chatHistory.scrollTop = chatHistory.scrollHeight;
});
function addMessage(role, content) {
    const chatHistoryElement = document.getElementById('chat-history');
    const messageDiv = document.createElement('div');
    messageDiv.className = `message ${role}-message`;
    
    const contentDiv = document.createElement('div');
    contentDiv.className = 'ai-message-content';
    
    // 安全渲染Markdown
    if (role === 'ai') {
        const sanitized = DOMPurify.sanitize(marked.parse(content));
        contentDiv.innerHTML = sanitized;
    } else {
        contentDiv.textContent = content;
    }
    
    messageDiv.appendChild(contentDiv);
    chatHistoryElement.appendChild(messageDiv);
    
    // 滚动到底部
    chatHistoryElement.scrollTop = chatHistoryElement.scrollHeight;
    
    // 保存历史
    chatHistory.push({ role, content });
}

// 输入框回车发送
document.getElementById('user-input').addEventListener('keypress', (e) => {
    if (e.key === 'Enter') sendMessage();
});
=== END ===

=== FILE: frontend\style.css ===
body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background-color: #f5f5f5;
    display: flex;
    justify-content: center;
    padding: 20px;
    line-height: 1.6;
}

.chat-container {
    width: 100%;
    max-width: 800px;
    background: white;
    border-radius: 10px;
    box-shadow: 0 0 20px rgba(0,0,0,0.1);
    overflow: hidden;
    display: flex;
    flex-direction: column;
    height: 90vh;
}
.document-upload {
    padding: 15px;
    background: #f0f0f0;
    border-top: 1px solid #ddd;
    display: flex;
    flex-direction: column;
    gap: 10px;
}

.document-upload h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    color: #444;
}

.document-upload input[type="file"] {
    padding: 8px;
    background: white;
    border: 1px solid #ccc;
    border-radius: 4px;
}
.chat-history {
    flex: 1;
    padding: 20px;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 15px;
}

.message {
    max-width: 80%;
    padding: 15px;
    border-radius: 12px;
    line-height: 1.6;
    box-shadow: 0 2px 5px rgba(0,0,0,0.05);
}

.user-message {
    align-self: flex-end;
    background: linear-gradient(135deg, #4e8cff, #3a75e0);
    color: white;
    border-bottom-right-radius: 5px;
}

.ai-message {
    align-self: flex-start;
    background-color: #fafafa;
    color: #333;
    border-bottom-left-radius: 5px;
}

.ai-message-content {
    overflow-wrap: break-word;
}

/* Markdown样式增强 */
.markdown-content h1, .markdown-content h2, .markdown-content h3 {
    margin-top: 1.2em;
    margin-bottom: 0.6em;
    padding-bottom: 0.2em;
    border-bottom: 1px solid #eee;
    color: #2c3e50;
}

.markdown-content h1 {
    font-size: 1.8em;
}

.markdown-content h2 {
    font-size: 1.5em;
}

.markdown-content h3 {
    font-size: 1.3em;
}

.markdown-content p {
    margin: 0.8em 0;
}

.markdown-content ul, .markdown-content ol {
    margin: 0.8em 0;
    padding-left: 1.5em;
}

.markdown-content li {
    margin: 0.4em 0;
}

.markdown-content blockquote {
    margin: 1em 0;
    padding: 0.8em 1em;
    background-color: #f8f9fa;
    border-left: 4px solid #4e8cff;
    color: #555;
    border-radius: 0 8px 8px 0;
}

/* 代码块样式 - 淡蓝色背景 */
.markdown-content pre {
    background-color: #e6f7ff; /* 淡蓝色背景 */
    padding: 15px;
    border-radius: 8px;
    overflow-x: auto;
    margin: 1.2em 0;
    box-shadow: inset 0 0 5px rgba(0,0,0,0.05);
    border: 1px solid #c0e6ff;
}

.markdown-content code {
    font-family: 'Fira Code', 'Consolas', monospace;
    background-color: rgba(230, 247, 255, 0.3);
    padding: 2px 6px;
    border-radius: 4px;
    color: #d63384;
}

.markdown-content pre code {
    background: none;
    padding: 0;
    border-radius: 0;
    color: #333;
}

/* 表格样式 */
.markdown-content table {
    width: 100%;
    border-collapse: collapse;
    margin: 1.2em 0;
    background-color: #fff;
    box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}

.markdown-content th, .markdown-content td {
    padding: 10px 15px;
    text-align: left;
    border: 1px solid #e1e4e8;
}

.markdown-content th {
    background-color: #f6f8fa;
    font-weight: 600;
}

.markdown-content tr:nth-child(even) {
    background-color: #fafbfc;
}

/* 流程图/图表容器样式 */
.markdown-content .mermaid, 
.markdown-content .flowchart, 
.markdown-content .graphviz {
    background-color: #a5c0cd; /* 淡蓝色背景 */
    padding: 15px;
    border-radius: 8px;
    margin: 1.2em 0;
    overflow-x: auto;
    text-align: center;
    border: 1px solid #c0e6ff;
}

/* 水平线样式 */
.markdown-content hr {
    border: 0;
    height: 1px;
    background: linear-gradient(to right, rgba(0,0,0,0), rgba(78,140,255,0.5), rgba(0,0,0,0));
    margin: 1.5em 0;
}

/* 链接样式 */
.markdown-content a {
    color: #1e6bb8;
    text-decoration: none;
    border-bottom: 1px dashed #4e8cff;
    transition: all 0.2s;
}

.markdown-content a:hover {
    color: #0d4a9e;
    border-bottom: 1px solid #0d4a9e;
}

/* 键盘标签样式 */
.markdown-content kbd {
    background-color: #f6f8fa;
    border: 1px solid #d1d5da;
    border-radius: 4px;
    box-shadow: inset 0 -1px 0 #d1d5da;
    color: #444d56;
    display: inline-block;
    font-family: monospace;
    font-size: 0.9em;
    line-height: 1;
    padding: 3px 5px;
    vertical-align: middle;
}

/* 提示框样式 */
.markdown-content .tip, 
.markdown-content .note, 
.markdown-content .warning {
    padding: 12px 15px;
    margin: 1.2em 0;
    border-radius: 8px;
    border-left: 4px solid;
}

.markdown-content .tip {
    background-color: #e6f7ff;
    border-color: #4e8cff;
}

.markdown-content .note {
    background-color: #fff8e6;
    border-color: #ffc53d;
}

.markdown-content .warning {
    background-color: #ffebee;
    border-color: #f44336;
}

.input-area {
    display: flex;
    padding: 15px;
    background: #f9f9f9;
    border-top: 1px solid #eee;
    gap: 10px;
    align-items: center;
}

.input-area input {
    flex: 1;
    padding: 12px 18px;
    border: 1px solid #ddd;
    border-radius: 25px;
    font-size: 16px;
    outline: none;
    transition: border-color 0.3s;
}

.input-area input:focus {
    border-color: #4e8cff;
    box-shadow: 0 0 0 2px rgba(78, 140, 255, 0.2);
}

.input-area button {
    padding: 12px 24px;
    background: linear-gradient(135deg, #4e8cff, #3a75e0);
    color: white;
    border: none;
    border-radius: 25px;
    cursor: pointer;
    font-weight: bold;
    transition: all 0.2s;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}

.input-area button:hover {
    background: linear-gradient(135deg, #3a75e0, #2a65d0);
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
}

.config-panel {
    padding: 15px;
    background: #f0f0f0;
    border-top: 1px solid #ddd;
    display: flex;
    gap: 10px;
    align-items: center;
    flex-wrap: wrap;
}

.config-panel h3 {
    margin: 0;
    font-size: 16px;
    color: #444;
}

.config-panel select, .config-panel input {
    padding: 8px 12px;
    border: 1px solid #ccc;
    border-radius: 4px;
    background: white;
    font-size: 14px;
}

.config-panel button {
    padding: 8px 15px;
    background: #5a6268;
    color: white;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background 0.2s;
}

.config-panel button:hover {
    background: #484e53;
}

/* 滚动条美化 */
.chat-history::-webkit-scrollbar {
    width: 8px;
}

.chat-history::-webkit-scrollbar-track {
    background: #f1f1f1;
    border-radius: 4px;
}

.chat-history::-webkit-scrollbar-thumb {
    background: #c1c1c1;
    border-radius: 4px;
}

.chat-history::-webkit-scrollbar-thumb:hover {
    background: #a8a8a8;
}
=== END ===

