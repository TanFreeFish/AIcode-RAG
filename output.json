{
  "tree": {
    "name": ".",
    "directories": [
      {
        "name": "RAG",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 0
          },
          {
            "name": "document_loader.py",
            "content_index": 1
          },
          {
            "name": "embeddings.py",
            "content_index": 2
          },
          {
            "name": "retriever.py",
            "content_index": 3
          },
          {
            "name": "text_splitter.py",
            "content_index": 4
          },
          {
            "name": "vector_store.py",
            "content_index": 5
          }
        ]
      },
      {
        "name": "backend",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 6
          },
          {
            "name": "ai_service.py",
            "content_index": 7
          },
          {
            "name": "main.py",
            "content_index": 8
          },
          {
            "name": "requirements.txt",
            "content_index": 9
          }
        ]
      },
      {
        "name": "frontend",
        "directories": [],
        "files": [
          {
            "name": "index.html",
            "content_index": 13
          },
          {
            "name": "script.js",
            "content_index": 14
          },
          {
            "name": "style.css",
            "content_index": 15
          }
        ]
      }
    ],
    "files": [
      {
        "name": "build_embeddings.py",
        "content_index": 10
      },
      {
        "name": "config.py",
        "content_index": 11
      },
      {
        "name": "diagnose.py",
        "content_index": 12
      },
      {
        "name": "run_demo.py",
        "content_index": 16
      }
    ]
  },
  "code_blocks": [
    {
      "filename": "RAG\\__init__.py",
      "content": "# RAG/__init__.py\nfrom .document_loader import DocumentLoader\nfrom .text_splitter import TextSplitter\nfrom .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom .retriever import Retriever\nfrom config import DOCUMENTS_DIR, VECTOR_STORE_DIR\nimport os\nimport time\nfrom tqdm import tqdm\nimport logging\n\n# è®¾ç½®æ—¥å¿—\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef initialize_rag_system(force_rebuild=False):\n    \"\"\"åˆå§‹åŒ–RAGç³»ç»Ÿ\"\"\"\n    vector_store = VectorStore(rebuild_mode=force_rebuild)  # æ·»åŠ é‡å»ºæ¨¡å¼å‚æ•°\n    if not force_rebuild and vector_store.index_path.exists() and vector_store.metadata_path.exists():\n        logger.info(\"Using existing vector store\")\n        return Retriever()\n    \n    # éœ€è¦æ„å»ºå‘é‡åº“\n    logger.info(\"Vector store not found or incomplete. Building new vector store...\")\n    if build_vector_store():\n        return Retriever()\n    else:\n        logger.error(\"Failed to build vector store\")\n        return Retriever()\n\ndef build_vector_store(progress_callback=None):\n    \"\"\"æ‰‹åŠ¨æ„å»ºå‘é‡å­˜å‚¨\"\"\"\n    logger.info(\"Building new vector store...\")\n    \n    # åŠ è½½æ–‡æ¡£\n    loader = DocumentLoader()\n    documents = loader.load_documents()\n    \n    # æ·»åŠ è¿›åº¦å›è°ƒ\n    progress_callback = progress_callback or (lambda **kw: None)\n    progress_callback(stage=\"load\", total=len(documents), current=0, message=\"å¼€å§‹åŠ è½½æ–‡æ¡£\")\n    \n    if not documents:\n        logger.warning(\"No documents found to build vector store\")\n        progress_callback(stage=\"load\", message=\"æœªæ‰¾åˆ°æ–‡æ¡£\", status=\"error\")\n        return False\n    \n    # åˆ†å‰²æ–‡æ¡£\n    splitter = TextSplitter(progress_callback=progress_callback)\n    chunks = splitter.split_documents(documents)\n    \n    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æœ¬å—\n    if not chunks:\n        logger.warning(\"No text chunks created from documents\")\n        progress_callback(stage=\"split\", message=\"æœªç”Ÿæˆæ–‡æœ¬å—\", status=\"error\")\n        return False\n    \n    # ç”ŸæˆåµŒå…¥\n    embedding_model = EmbeddingModel()\n    texts = [chunk[\"text\"] for chunk in chunks]\n    \n    # åˆ†æ‰¹å¤„ç†åµŒå…¥ç”Ÿæˆ\n    batch_size = 32\n    embeddings = []\n    logger.info(\"Generating embeddings...\")\n    start_time = time.time()\n    \n    # æ·»åŠ è¿›åº¦å›è°ƒ\n    total_batches = (len(texts) + batch_size - 1) // batch_size\n    progress_callback(\n        stage=\"embed\",\n        total=total_batches,\n        current=0,\n        message=\"å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡\",\n        details=f\"å…± {len(texts)} ä¸ªæ–‡æœ¬å—ï¼Œåˆ† {total_batches} æ‰¹å¤„ç†\"\n    )\n    \n    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦æ¡\n    for batch_idx, i in enumerate(tqdm(range(0, len(texts), batch_size), desc=\"ç”ŸæˆåµŒå…¥\")):\n        batch_texts = texts[i:i+batch_size]\n        batch_embeddings = embedding_model.embed_texts(batch_texts)\n        \n        # æ›´æ–°è¿›åº¦\n        progress_callback(\n            stage=\"embed\",\n            current=batch_idx + 1,\n            total=total_batches,\n            message=f\"æ­£åœ¨å¤„ç†ç¬¬ {batch_idx+1}/{total_batches} æ‰¹\",\n            details=f\"æ–‡æœ¬å— {i+1}-{min(i+batch_size, len(texts))}\"\n        )\n        \n        # æ£€æŸ¥åµŒå…¥æ˜¯å¦æœ‰æ•ˆ\n        valid_embeddings = []\n        for emb in batch_embeddings:\n            if emb and len(emb) == embedding_model.dim:\n                valid_embeddings.append(emb)\n            else:\n                logger.warning(f\"Invalid embedding at batch index {i}\")\n                valid_embeddings.append([0.0] * embedding_model.dim)  # ä½¿ç”¨é›¶å‘é‡å ä½\n        \n        embeddings.extend(valid_embeddings)\n    \n    # æ£€æŸ¥åµŒå…¥æ•°é‡æ˜¯å¦åŒ¹é…\n    if len(embeddings) != len(chunks):\n        logger.warning(f\"Embeddings count ({len(embeddings)}) doesn't match chunks count ({len(chunks)})\")\n        # å¡«å……ç¼ºå¤±çš„åµŒå…¥\n        embeddings.extend([[0.0] * embedding_model.dim] * (len(chunks) - len(embeddings)))\n    \n    logger.info(f\"Embeddings generated in {time.time()-start_time:.2f} seconds\")\n    \n    # æ·»åŠ åˆ°å‘é‡å­˜å‚¨ï¼ˆä½¿ç”¨é‡å»ºæ¨¡å¼ï¼‰\n    vector_store = VectorStore(rebuild_mode=True)  # å¯ç”¨é‡å»ºæ¨¡å¼\n    \n    # æ·»åŠ ç´¢å¼•æ„å»ºè¿›åº¦å›è°ƒ - ä¿®å¤è¿™é‡Œçš„é—®é¢˜\n    def index_progress(**kwargs):\n        # ç›´æ¥ä¼ é€’æ‰€æœ‰å‚æ•°ï¼Œä¸å†æ·»åŠ é¢å¤–çš„stageå‚æ•°\n        progress_callback(**kwargs)\n    \n    # è°ƒç”¨add_chunkså¹¶æ£€æŸ¥è¿”å›å€¼\n    success = vector_store.add_chunks(chunks, embeddings, progress_callback=index_progress)\n    \n    if success:\n        logger.info(\"Vector store built successfully\")\n        return True\n    else:\n        logger.error(\"Failed to build vector store\")\n        return False"
    },
    {
      "filename": "RAG\\document_loader.py",
      "content": "import os\nfrom pathlib import Path\nfrom config import DOCUMENTS_DIR, RAG_CONFIG\nimport PyPDF2\nfrom docx import Document\nimport markdown\nimport re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentLoader:\n    def __init__(self):\n        self.extensions = RAG_CONFIG[\"document_loader\"][\"extensions\"]\n        self.documents_dir = Path(DOCUMENTS_DIR)\n        self.documents_dir.mkdir(parents=True, exist_ok=True)\n    \n    def load_documents(self):\n        \"\"\"åŠ è½½æ–‡æ¡£ç›®å½•ä¸­çš„æ‰€æœ‰æ”¯æŒæ–‡æ¡£\"\"\"\n        documents = []\n        \n        for ext in self.extensions:\n            for file_path in self.documents_dir.glob(f\"*{ext}\"):\n                content = self._load_file(file_path)\n                if content:\n                    documents.append({\n                        \"file_path\": str(file_path),\n                        \"content\": content\n                    })\n                    logger.info(f\"Loaded document: {file_path.name}\")\n        \n        return documents\n    \n    def _load_file(self, file_path):\n        \"\"\"æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½å†…å®¹\"\"\"\n        ext = file_path.suffix.lower()\n        \n        try:\n            if ext == \".txt\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return f.read()\n            \n            elif ext == \".pdf\":\n                content = []\n                with open(file_path, 'rb') as f:\n                    pdf_reader = PyPDF2.PdfReader(f)\n                    for page in pdf_reader.pages:\n                        text = page.extract_text()\n                        if text:\n                            content.append(text)\n                return \"\\n\".join(content)\n            \n            elif ext == \".json\":  \n                with open(file_path, 'r', encoding='utf-8') as f:\n                    import json\n                    data = json.load(f)\n                    return str(data)  \n            elif ext == \".docx\":\n                doc = Document(file_path)\n                return \"\\n\".join([para.text for para in doc.paragraphs])\n            \n            elif ext == \".md\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return markdown.markdown(f.read())\n            \n            else:\n                logger.warning(f\"Unsupported file type: {ext}\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error loading {file_path}: {str(e)}\")\n            return None\n\n    def add_document(self, file_path):\n        \"\"\"æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°å­˜å‚¨\"\"\"\n        dest_path = self.documents_dir / Path(file_path).name\n        try:\n            with open(file_path, 'rb') as src, open(dest_path, 'wb') as dst:\n                dst.write(src.read())\n            logger.info(f\"Added document: {dest_path}\")\n            return str(dest_path)\n        except Exception as e:\n            logger.error(f\"Failed to add document: {str(e)}\")\n            return None"
    },
    {
      "filename": "RAG\\embeddings.py",
      "content": "import requests\nimport numpy as np\nfrom typing import List\nfrom config import RAG_CONFIG\nimport logging\nimport time\n\n# è®¾ç½®æ—¥å¿—\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingModel:\n    def __init__(self):\n        config = RAG_CONFIG[\"embeddings\"]\n        self.model_type = config[\"model_type\"]\n        self.model_name = config[\"model_name\"]\n        self.dim = config.get(\"dim\", 384)\n        self.api_url = \"http://localhost:11434/api/embeddings\"\n    \n    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºåµŒå…¥å‘é‡\"\"\"\n        if not texts:\n            return []\n            \n        if self.model_type == \"ollama\":\n            return self._embed_with_ollama(texts)\n        elif self.model_type == \"huggingface\":\n            return self._embed_with_huggingface(texts)\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    \n    def _embed_with_ollama(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"ä½¿ç”¨Ollama APIç”ŸæˆåµŒå…¥ - å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶\"\"\"\n        embeddings = []\n        for text in texts:\n            if not text.strip():\n                embeddings.append([])\n                continue\n                \n            for attempt in range(3):  \n                try:\n                    response = requests.post(\n                        self.api_url,\n                        json={\n                            \"model\": self.model_name,\n                            \"prompt\": text\n                        },\n                        timeout=30\n                    )\n                    \n                    if response.status_code == 200:\n                        data = response.json()\n                        if \"embedding\" in data and data[\"embedding\"]:\n                            embedding = data[\"embedding\"]\n                            # æ£€æŸ¥åµŒå…¥ç»´åº¦\n                            if len(embedding) == self.dim:\n                                embeddings.append(embedding)\n                                break\n                            else:\n                                logger.warning(f\"Embedding dimension mismatch: expected {self.dim}, got {len(embedding)}\")\n                                embeddings.append([])\n                                break\n                        else:\n                            logger.warning(f\"Empty embedding for text: {text[:50]}...\")\n                    else:\n                        logger.error(f\"Error embedding text: {response.status_code} - {response.text}\")\n                    \n                   \n                    if attempt == 2:\n                        logger.error(f\"Failed to generate embedding after 3 attempts for text: {text[:50]}...\")\n                        embeddings.append([])\n                except Exception as e:\n                    logger.error(f\"Ollama embedding error: {str(e)}\")\n                    if attempt == 2:\n                        embeddings.append([])\n        return embeddings\n    \n    def _embed_with_huggingface(self, texts: List[str]) -> List[List[float]]:\n        \n        return [[] for _ in texts] "
    },
    {
      "filename": "RAG\\retriever.py",
      "content": "from .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom config import RAG_CONFIG\nfrom pathlib import Path\nimport logging\n\n# è®¾ç½®æ—¥å¿—\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Retriever:\n    def __init__(self):\n        self.embedding_model = EmbeddingModel()\n        self.vector_store = VectorStore()\n        self.vector_store.load_index()\n        self.top_k = RAG_CONFIG[\"retriever\"][\"top_k\"]\n        self.score_threshold = RAG_CONFIG[\"retriever\"][\"score_threshold\"]\n    \n    def retrieve(self, query: str) -> str:\n        \"\"\"æ£€ç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„ä¸Šä¸‹æ–‡\"\"\"\n        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥\n        query_embedding = self.embedding_model.embed_texts([query])\n        if not query_embedding or not query_embedding[0]:\n            logger.warning(f\"Failed to generate embedding for query: '{query}'\")\n            return \"\"\n        \n        # ç¡®ä¿åµŒå…¥å‘é‡æ ¼å¼æ­£ç¡®\n        if not isinstance(query_embedding[0], list) or not all(isinstance(x, float) for x in query_embedding[0]):\n            logger.error(f\"Invalid embedding format for query: '{query}'\")\n            return \"\"\n        \n        # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢\n        results = self.vector_store.similarity_search(\n            query_embedding[0], \n            top_k=self.top_k\n        )\n        \n        # æ„å»ºä¸Šä¸‹æ–‡\n        context = []\n        for score, chunk_id, chunk_data in results:\n            if score >= self.score_threshold:\n                context.append({\n                    \"text\": chunk_data[\"text\"],\n                    \"summary\": chunk_data[\"summary\"],  # åŒ…å«æ‘˜è¦\n                    \"source\": chunk_data[\"source\"],\n                    \"score\": round(score, 3)\n                })\n        \n        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡\n        return self._format_context(context)\n    \n    def _format_context(self, context_items) -> str:\n        \"\"\"æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡\"\"\"\n        if not context_items:\n            return \"æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯\"\n        \n        context_str = \"æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\\n\\n\"\n        for i, item in enumerate(context_items, 1):\n            source_name = Path(item[\"source\"]).name\n            context_str += f\"### ä¸Šä¸‹æ–‡ç‰‡æ®µ {i} (æ¥æº: {source_name}, ç›¸ä¼¼åº¦: {item['score']}, æ‘˜è¦: {item['summary']})\\n\"\n            context_str += f\"{item['text']}\\n\\n\"\n        \n        return context_str.strip()"
    },
    {
      "filename": "RAG\\text_splitter.py",
      "content": "import re\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom config import RAG_CONFIG\nfrom pathlib import Path\nimport logging\nimport requests\nimport time\nfrom tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡åº“\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass TextSplitter:\n    def __init__(self, progress_callback=None):\n        config = RAG_CONFIG[\"text_splitter\"]\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=config[\"chunk_size\"],\n            chunk_overlap=config[\"chunk_overlap\"],\n            separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼Ÿ\", \"ï¼\", \"ï¼›\", \" \", \"\"],\n            keep_separator=True\n        )\n        self.summarizer_config = RAG_CONFIG.get(\"summarizer\", {})\n        self.progress_callback = progress_callback or (lambda **kw: None)\n    \n    def generate_summary(self, text: str) -> str:\n        \"\"\"ä½¿ç”¨Qwenæ¨¡å‹ç”ŸæˆçŸ­è¯­çº§æ‘˜è¦\"\"\"\n        try:\n            prompt = f\"è¯·ç”¨5-10ä¸ªå­—çš„çŸ­è¯­æ€»ç»“ä»¥ä¸‹æ–‡æœ¬çš„æ ¸å¿ƒå†…å®¹ï¼Œä¸è¦è§£é‡Šï¼Œåªè¾“å‡ºçŸ­è¯­ï¼š\\n{text}\"\n            \n            response = requests.post(\n                \"http://localhost:11434/api/generate\",\n                json={\n                    \"model\": self.summarizer_config.get(\"model_name\", \"qwen:7b\"),\n                    \"prompt\": prompt,\n                    \"stream\": False\n                },\n                timeout=30\n            )\n            \n            if response.status_code == 200:\n                return response.json().get(\"response\", \"\").strip().replace('\"', '')\n            else:\n                logger.warning(f\"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {response.status_code} - {response.text}\")\n        except Exception as e:\n            logger.error(f\"æ‘˜è¦ç”Ÿæˆé”™è¯¯: {str(e)}\")\n        \n        # å¤±è´¥æ—¶å›é€€ï¼šä½¿ç”¨å‰Nä¸ªè¯\n        return \" \".join(text.split()[:5])\n    # åœ¨ TextSplitter ç±»ä¸­æ·»åŠ æ™ºèƒ½åˆ†å‰²æ–¹æ³•\n    def _smart_split(self, text: str) -> List[str]:\n        \"\"\"æ”¹è¿›çš„åˆ†å‰²é€»è¾‘ï¼Œç¡®ä¿å¥å­å®Œæ•´æ€§\"\"\"\n        # å®šä¹‰ä¸­æ–‡å¥å­ç»“æŸç¬¦\n        sentence_endings = {'ã€‚', 'ï¼Ÿ', 'ï¼', 'ï¼›', '.', '?', '!', ';'}\n        \n        chunks = []\n        current_chunk = \"\"\n        char_count = 0\n        \n        # æŒ‰å­—ç¬¦è¿­ä»£ï¼Œä¿æŒå¥å­å®Œæ•´æ€§\n        for char in text:\n            current_chunk += char\n            char_count += 1\n            \n            # è¾¾åˆ°æœ€å°åˆ†å‰²é•¿åº¦ä¸”é‡åˆ°å¥å­ç»“æŸç¬¦\n            if char_count >= self.splitter._chunk_size * 0.7 and char in sentence_endings:\n                chunks.append(current_chunk.strip())\n                current_chunk = \"\"\n                char_count = 0\n                \n            # è¾¾åˆ°æœ€å¤§é•¿åº¦å¼ºåˆ¶åˆ†å‰²ï¼ˆé¿å…è¿‡é•¿ï¼‰\n            elif char_count >= self.splitter._chunk_size:\n                # å¯»æ‰¾æœ€è¿‘çš„å¥å­è¾¹ç•Œ\n                for i in range(len(current_chunk)-1, -1, -1):\n                    if current_chunk[i] in sentence_endings:\n                        chunks.append(current_chunk[:i+1].strip())\n                        current_chunk = current_chunk[i+1:]\n                        char_count = len(current_chunk)\n                        break\n                else:  # æ²¡æœ‰æ‰¾åˆ°è¾¹ç•Œåˆ™ç¡¬åˆ†å‰²\n                    chunks.append(current_chunk.strip())\n                    current_chunk = \"\"\n                    char_count = 0\n        \n        # æ·»åŠ å‰©ä½™å†…å®¹\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    def split_documents(self, documents):\n        \"\"\"åˆ†å‰²æ–‡æ¡£ä¸ºæ–‡æœ¬å—å¹¶ç”Ÿæˆæ‘˜è¦\"\"\"\n        chunks = []\n        total_docs = len(documents)\n        \n        # æ·»åŠ è¿›åº¦å›è°ƒ\n        self.progress_callback(stage=\"split\", total=total_docs, current=0, message=\"å¼€å§‹åˆ†å‰²æ–‡æ¡£\")\n        \n        for doc_idx, doc in enumerate(tqdm(documents, desc=\"åˆ†å‰²æ–‡æ¡£\")):\n            content = doc[\"content\"]\n            # æ¸…ç†å¤šä½™ç©ºç™½\n            content = re.sub(r'\\s+', ' ', content).strip()\n            # åˆ†å‰²æ–‡æœ¬\n            split_texts = self._smart_split(content)\n            \n            # æ›´æ–°è¿›åº¦\n            self.progress_callback(\n                stage=\"split\",\n                current=doc_idx + 1,\n                total=total_docs,\n                message=f\"æ­£åœ¨å¤„ç†æ–‡æ¡£: {Path(doc['file_path']).name}\",\n                details=f\"åˆ†å‰²æˆ {len(split_texts)} ä¸ªç‰‡æ®µ\"\n            )\n            \n            for i, text in enumerate(split_texts):\n                # ç”Ÿæˆè¯­ä¹‰æ‘˜è¦\n                summary = self.generate_summary(text)\n                \n                chunks.append({\n                    \"text\": text,\n                    \"summary\": summary,\n                    \"source\": doc[\"file_path\"],\n                    \"chunk_id\": f\"{Path(doc['file_path']).stem}_{i}\"\n                })\n        \n        # å®Œæˆè¿›åº¦\n        self.progress_callback(\n            stage=\"split\",\n            current=total_docs,\n            total=total_docs,\n            message=f\"æ–‡æ¡£åˆ†å‰²å®Œæˆ\",\n            details=f\"å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—\"\n        )\n        \n        return chunks"
    },
    {
      "filename": "RAG\\vector_store.py",
      "content": "# RAG/vector_store.py\nfrom annoy import AnnoyIndex\nimport json\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom config import VECTOR_STORE_DIR, RAG_CONFIG\nimport logging\nfrom tqdm import tqdm\n\n# è®¾ç½®æ—¥å¿—\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass VectorStore:\n    def __init__(self, rebuild_mode=False):\n        self.store_dir = Path(VECTOR_STORE_DIR)\n        self.store_dir.mkdir(parents=True, exist_ok=True)\n        config = RAG_CONFIG[\"vector_store\"]\n        self.store_type = config[\"type\"]\n        self.index_name = config[\"index_name\"]\n        self.index_path = self.store_dir / f\"{self.index_name}.ann\"\n        self.summary_index_path = self.store_dir / f\"{self.index_name}_summary.ann\"\n        self.metadata_path = self.store_dir / f\"{self.index_name}_metadata.json\"\n        self.index = None\n        self.summary_index = None\n        self.metadata = []\n        self.chunk_ids = []\n        self.rebuild_mode = rebuild_mode\n        \n        # ä»åµŒå…¥é…ç½®è·å–ç»´åº¦\n        embedding_config = RAG_CONFIG[\"embeddings\"]\n        self.dim = embedding_config.get(\"dim\", 384)\n        \n        self.distance_metric = \"angular\"\n        \n        # ç¡®ä¿ç´¢å¼•å¯¹è±¡è¢«æ­£ç¡®åˆ›å»º\n        self.load_index()\n        \n    def load_index(self):\n        try:\n            # é‡å»ºæ¨¡å¼æˆ–ç´¢å¼•ä¸å­˜åœ¨æ—¶åˆ›å»ºæ–°ç´¢å¼•\n            if self.rebuild_mode or not (self.index_path.exists() and self.summary_index_path.exists() and self.metadata_path.exists()):\n                logger.info(\"Creating new index (rebuild mode or no existing index)\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n                self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n                self.metadata = []\n                self.chunk_ids = []\n            else:\n                logger.info(f\"Loading existing index from {self.index_path}\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n                self.index.load(str(self.index_path))\n                \n                # åŠ è½½æ‘˜è¦ç´¢å¼•\n                self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n                self.summary_index.load(str(self.summary_index_path))\n                \n                with open(self.metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                # æ­£ç¡®åŠ è½½å…ƒæ•°æ®å’Œå—ID\n                self.metadata = metadata.get(\"chunks\", [])\n                self.chunk_ids = metadata.get(\"chunk_ids\", [])\n                logger.info(f\"Loaded Annoy index with {len(self.metadata)} chunks\")\n        except Exception as e:\n            logger.error(f\"Error loading index: {str(e)}\")\n            # åˆ›å»ºæ–°çš„ç´¢å¼•ä½œä¸ºå›é€€\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n            self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n            self.metadata = []\n            self.chunk_ids = []\n    def add_chunks(self, chunks, embeddings, progress_callback=None):\n        if not embeddings:\n            logger.warning(\"No embeddings provided, skipping add_chunks\")\n            return False\n            \n        # ç¡®ä¿ç´¢å¼•å¯¹è±¡å­˜åœ¨\n        if self.index is None:\n            logger.warning(\"Index is None, creating new index\")\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n        if self.summary_index is None:\n            logger.warning(\"Summary index is None, creating new summary index\")\n            self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n        \n        # è®¾ç½®è¿›åº¦å›è°ƒ\n        progress_callback = progress_callback or (lambda **kw: None)\n        total_chunks = len(chunks)\n        \n        # å‘é€è¿›åº¦å¼€å§‹æ¶ˆæ¯\n        progress_callback(\n            stage=\"index\",\n            total=total_chunks,\n            current=0,\n            message=\"å¼€å§‹æ„å»ºç´¢å¼•\",\n            details=f\"å…± {total_chunks} ä¸ªæ–‡æœ¬å—\"\n        )\n        \n        # é‡ç½®å…ƒæ•°æ®å’Œå—ID\n        self.metadata = []\n        self.chunk_ids = []\n        \n        # æ·»åŠ å‘é‡åˆ°Annoyç´¢å¼• - ä½¿ç”¨è¿ç»­ç´¢å¼•ID\n        valid_count = 0\n        for i, (chunk, embedding) in enumerate(tqdm(zip(chunks, embeddings), desc=\"æ„å»ºç´¢å¼•\")):\n            if not embedding or len(embedding) != self.dim:\n                logger.warning(f\"Skipping invalid embedding for chunk {i}\")\n                continue\n                \n            try:\n                # ç¡®ä¿åµŒå…¥æ˜¯æµ®ç‚¹æ•°åˆ—è¡¨\n                embedding = [float(x) for x in embedding]\n                \n                # å½’ä¸€åŒ–åµŒå…¥å‘é‡\n                embedding_arr = np.array(embedding, dtype=np.float32)\n                norm = np.linalg.norm(embedding_arr)\n                if norm > 0:\n                    embedding_arr = embedding_arr / norm\n                else:\n                    # é›¶å‘é‡å¤„ç† - è·³è¿‡æ— æ•ˆåµŒå…¥\n                    logger.warning(f\"Zero vector embedding for chunk {i}, skipping\")\n                    continue\n                \n                # ä½¿ç”¨è¿ç»­ç´¢å¼•ID (valid_count) è€Œä¸æ˜¯æ–‡ä»¶ç´¢å¼•(i)\n                self.index.add_item(valid_count, embedding_arr)\n                \n                # ç”Ÿæˆæ‘˜è¦åµŒå…¥å¹¶æ·»åŠ åˆ°æ‘˜è¦ç´¢å¼•\n                summary_embedding = self._get_summary_embedding(chunk[\"summary\"])\n                if summary_embedding and len(summary_embedding) == self.dim:\n                    summary_arr = np.array(summary_embedding, dtype=np.float32)\n                    summary_norm = np.linalg.norm(summary_arr)\n                    if summary_norm > 0:\n                        summary_arr = summary_arr / summary_norm\n                    self.summary_index.add_item(valid_count, summary_arr)\n                else:\n                    # ä½¿ç”¨ä¸»åµŒå…¥ä½œä¸ºå›é€€\n                    self.summary_index.add_item(valid_count, embedding_arr)\n                \n                self.metadata.append({\n                    \"text\": chunk[\"text\"],\n                    \"summary\": chunk[\"summary\"],\n                    \"source\": chunk[\"source\"]\n                })\n                self.chunk_ids.append(chunk[\"chunk_id\"])\n                valid_count += 1\n                \n                # æ›´æ–°è¿›åº¦ - æ¯10ä¸ªå—æ›´æ–°ä¸€æ¬¡\n                if (i + 1) % 10 == 0 or (i + 1) == total_chunks:\n                    progress_callback(\n                        stage=\"index\",\n                        current=i + 1,\n                        total=total_chunks,\n                        message=f\"æ­£åœ¨æ·»åŠ æ–‡æœ¬å— {i+1}/{total_chunks}\",\n                        details=f\"æœ‰æ•ˆå—: {valid_count}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Error adding chunk {i}: {str(e)}\")\n        \n        if valid_count == 0:\n            logger.error(\"No valid embeddings added to index\")\n            progress_callback(\n                stage=\"index\",\n                message=\"æœªæ·»åŠ æœ‰æ•ˆåµŒå…¥\",\n                status=\"error\"\n            )\n            return False\n            \n        # æ„å»ºç´¢å¼•\n        logger.info(f\"Building index with {valid_count} items...\")\n        progress_callback(\n            stage=\"index\",\n            message=\"æ­£åœ¨æ„å»ºç´¢å¼•ç»“æ„...\",\n            details=f\"å…± {valid_count} ä¸ªé¡¹ç›®\"\n        )\n        \n        try:\n            self.index.build(10)\n            self.summary_index.build(10)\n        except Exception as e:\n            logger.error(f\"Error building index: {str(e)}\")\n            progress_callback(\n                stage=\"index\",\n                message=\"ç´¢å¼•æ„å»ºå¤±è´¥\",\n                status=\"error\"\n            )\n            return False\n        \n        logger.info(\"Index built successfully\")\n        progress_callback(\n            stage=\"index\",\n            message=\"ç´¢å¼•æ„å»ºå®Œæˆ\",\n            status=\"completed\"\n        )\n        \n        # ä¿å­˜ç´¢å¼•\n        self.save_index()\n        return True\n\n    def _get_summary_embedding(self, summary: str) -> list:\n        \"\"\"è·å–æ‘˜è¦çš„åµŒå…¥å‘é‡\"\"\"\n        try:\n            # åŠ¨æ€å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–\n            from RAG.embeddings import EmbeddingModel\n            embedding_model = EmbeddingModel()\n            if summary and summary.strip():\n                return embedding_model.embed_texts([summary])[0]\n            return None\n        except ImportError as e:\n            logger.error(f\"å¯¼å…¥EmbeddingModelå¤±è´¥: {str(e)}\")\n            return None\n        except Exception as e:\n            logger.error(f\"ç”Ÿæˆæ‘˜è¦åµŒå…¥å¤±è´¥: {str(e)}\")\n            return None\n\n    def similarity_search(self, query_embedding, top_k=5):\n        if self.index is None or self.summary_index is None:\n            logger.warning(\"Index is None, cannot perform search\")\n            return []\n            \n        if not query_embedding or len(query_embedding) != self.dim:\n            logger.error(f\"Invalid query embedding: expected dim={self.dim}, got {len(query_embedding) if query_embedding else 'none'}\")\n            return []\n            \n        # ç¡®ä¿æŸ¥è¯¢åµŒå…¥æ˜¯æµ®ç‚¹æ•°åˆ—è¡¨\n        try:\n            query_embedding = [float(x) for x in query_embedding]\n        except Exception as e:\n            logger.error(f\"Error converting query embedding: {str(e)}\")\n            return []\n        \n        # å½’ä¸€åŒ–æŸ¥è¯¢å‘é‡\n        query_embedding_arr = np.array(query_embedding, dtype=np.float32)\n        query_norm = np.linalg.norm(query_embedding_arr)\n        if query_norm > 0:\n            query_embedding_arr = query_embedding_arr / query_norm\n        else:\n            # é›¶å‘é‡å¤„ç†\n            query_embedding_arr = np.zeros(self.dim, dtype=np.float32)\n        \n        # 1. ä½¿ç”¨æ‘˜è¦ç´¢å¼•è¿›è¡Œåˆæ­¥ç­›é€‰ï¼ˆè·å–æ›´å¤šç»“æœï¼‰\n        try:\n            summary_indices, summary_distances = self.summary_index.get_nns_by_vector(\n                query_embedding_arr, \n                top_k * 3,  # è·å–æ›´å¤šç»“æœç”¨äºäºŒæ¬¡ç­›é€‰\n                include_distances=True,\n                search_k=-1\n            )\n        except Exception as e:\n            logger.error(f\"Error in summary similarity search: {str(e)}\")\n            summary_indices, summary_distances = [], []\n        \n        # 2. å¯¹æ‘˜è¦ç­›é€‰ç»“æœä½¿ç”¨ä¸»ç´¢å¼•è¿›è¡Œç²¾æ’\n        results = []\n        for idx, angular_dist in zip(summary_indices, summary_distances):\n            if idx < len(self.metadata) and idx < len(self.chunk_ids):\n                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n                cosine_sim = 1 - (angular_dist ** 2) / 2.0\n                cosine_sim = max(-1.0, min(1.0, cosine_sim))\n                \n                # è·å–ä¸»ç´¢å¼•ç›¸ä¼¼åº¦ä½œä¸ºç²¾æ’åˆ†æ•°\n                try:\n                    item_vector = self.index.get_item_vector(idx)\n                    main_cosine_sim = np.dot(query_embedding_arr, item_vector)\n                    main_cosine_sim = max(-1.0, min(1.0, main_cosine_sim))\n                except:\n                    main_cosine_sim = cosine_sim\n                \n                # ä½¿ç”¨ä¸»ç´¢å¼•ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆåˆ†æ•°\n                results.append((\n                    main_cosine_sim,  \n                    self.chunk_ids[idx], \n                    self.metadata[idx],\n                    cosine_sim  # æ‘˜è¦ç›¸ä¼¼åº¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰\n                ))\n        \n        # æŒ‰ç²¾æ’åˆ†æ•°é™åºæ’åºå¹¶å–top_k\n        sorted_results = sorted(results, key=lambda x: x[0], reverse=True)[:top_k]\n        return [(score, chunk_id, data) for score, chunk_id, data, _ in sorted_results]\n    \n    def save_index(self):\n        if self.index is None or self.summary_index is None:\n            logger.warning(\"Index is None, cannot save\")\n            return\n            \n        self.index.save(str(self.index_path))\n        self.summary_index.save(str(self.summary_index_path))\n        # ä¿å­˜å…ƒæ•°æ®å’Œå—ID\n        with open(self.metadata_path, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"chunks\": self.metadata,\n                \"chunk_ids\": self.chunk_ids\n            }, f, ensure_ascii=False, indent=2)\n        logger.info(f\"Saved Annoy index with {len(self.metadata)} chunks\")"
    },
    {
      "filename": "backend\\__init__.py",
      "content": ""
    },
    {
      "filename": "backend\\ai_service.py",
      "content": "import requests\nfrom typing import Dict, Any, Optional\nfrom RAG import initialize_rag_system\n\nclass AIService:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.model_type = config.get('model_type', 'ollama')\n        self.model_name = config.get('model_name', 'qwen:7b')\n        self.rag_retriever = initialize_rag_system()\n        \n    def generate_response(self, prompt: str, use_rag: bool = False) -> str:\n        \"\"\"ç”ŸæˆAIå›å¤ï¼Œæ”¯æŒRAG\"\"\"\n        rag_context = None\n        if use_rag:\n            rag_context = self.rag_retriever.retrieve(prompt)\n        \n        full_prompt = self._build_prompt(prompt, rag_context)\n        \n        if self.model_type == 'ollama':\n            return self._call_ollama(full_prompt)\n        elif self.model_type == 'openai':\n            return self._call_openai(full_prompt)\n       \n        \n    def _build_prompt(self, prompt: str, context: Optional[str]) -> str:\n        \"\"\"æ„å»ºæœ€ç»ˆæç¤ºè¯ï¼Œæ•´åˆRAGå†…å®¹\"\"\"\n        if context:\n            return (\n                f\"<|im_start|>system\\n\"\n                f\"ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚æ¯ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µåŒ…å«æ‘˜è¦å’Œè¯¦ç»†å†…å®¹ï¼š\\n\\n\"\n                f\"{context}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>user\\n\"\n                f\"{prompt}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>assistant\\n\"\n            )\n        return (\n            f\"<|im_start|>user\\n\"\n            f\"{prompt}\\n\"\n            f\"<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n    \n    def _call_ollama(self, prompt: str) -> str:\n        \"\"\"è°ƒç”¨æœ¬åœ°OllamaæœåŠ¡\"\"\"\n        try:\n            url = \"http://localhost:11434/api/generate\"\n            payload = {\n                \"model\": self.model_name,\n                \"prompt\": prompt,\n                \"stream\": False\n            }\n            response = requests.post(url, json=payload, timeout=300)\n            return response.json().get(\"response\", \"\")\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def _call_openai(self, prompt: str) -> str:\n        \"\"\"è°ƒç”¨OpenAI APIï¼ˆé¢„ç•™ï¼‰\"\"\"\n        # å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨\n        return f\"OpenAI response to: {prompt}\"\n    \n    # backend/ai_service.py\n    def update_config(self, new_config: Dict[str, Any]):\n        self.config.update(new_config)\n        self.model_type = self.config.get('model_type', self.model_type)\n        self.model_name = self.config.get('model_name', self.model_name)\n        # é‡æ–°åˆå§‹åŒ– Retriever\n        self.rag_retriever = initialize_rag_system()"
    },
    {
      "filename": "backend\\main.py",
      "content": "# backend/main.py\nfrom fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nimport sys\nfrom pathlib import Path\n\nBASE_DIR = Path(__file__).resolve().parent.parent\nsys.path.append(str(BASE_DIR / \"backend\"))\n\nfrom ai_service import AIService\nimport os\nfrom RAG.document_loader import DocumentLoader\nfrom fastapi.responses import FileResponse\nfrom RAG import initialize_rag_system, build_vector_store  # ç›´æ¥å¯¼å…¥build_vector_store\n\nBASE_DIR = Path(__file__).resolve().parent.parent\napp = FastAPI()\n# æ·»åŠ é™æ€æ–‡ä»¶æœåŠ¡\n    \napp.mount(\"/static\", StaticFiles(directory=BASE_DIR / \"frontend\"), name=\"static\")\n\n# å…è®¸è·¨åŸŸ\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# åˆå§‹åŒ–æœåŠ¡\nai_config = {\"model_type\": \"ollama\", \"model_name\": \"qwen:7b\"}\nai_service = AIService(ai_config)\n\nclass ChatRequest(BaseModel):\n    message: str\n    use_rag: bool = False\n\n@app.post(\"/chat\")\nasync def chat_endpoint(request: ChatRequest):\n    try:\n        response = ai_service.generate_response(request.message, request.use_rag)\n        return {\"response\": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/update_config\")\nasync def update_config(new_config: dict):\n    ai_service.update_config(new_config)\n    return {\"status\": \"config updated\"}\n\n# é‡å»ºç´¢å¼•ç«¯ç‚¹\n@app.post(\"/rebuild_index\")\nasync def rebuild_index():\n    # ç›´æ¥è°ƒç”¨æ„å»ºå‡½æ•°\n    success = build_vector_store()\n    if success:\n        # é‡å»ºå®Œæˆåæ›´æ–°æ£€ç´¢å™¨\n        ai_service.rag_retriever = initialize_rag_system()\n        return {\"status\": \"index rebuilt successfully\"}\n    else:\n        return {\"status\": \"failed to rebuild index\", \"error\": \"no documents or chunks found\"}\n\n# ä¸Šä¼ æ–‡æ¡£ç«¯ç‚¹\n@app.post(\"/upload_document\")\nasync def upload_document(file: UploadFile = File(...)):\n    try:\n        documents_dir = BASE_DIR / \"data\" / \"documents\"\n        documents_dir.mkdir(parents=True, exist_ok=True)\n        file_path = documents_dir / file.filename\n        with open(file_path, \"wb\") as f:\n            content = await file.read()\n            f.write(content)\n        \n        # è‡ªåŠ¨è§¦å‘é‡å»ºç´¢å¼•\n        success = build_vector_store()\n        if success:\n            # é‡å»ºå®Œæˆåæ›´æ–°æ£€ç´¢å™¨\n            ai_service.rag_retriever = initialize_rag_system()\n            return {\"status\": \"success\", \"file_path\": str(file_path)}\n        else:\n            return {\"status\": \"file uploaded but failed to rebuild index\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# æ‰‹åŠ¨æ„å»ºåµŒå…¥ç«¯ç‚¹\n@app.post(\"/build_embeddings\")\nasync def build_embeddings():\n    \"\"\"æ‰‹åŠ¨è§¦å‘å‘é‡åµŒå…¥è¿‡ç¨‹\"\"\"\n    success = build_vector_store()\n    if success:\n        # é‡å»ºå®Œæˆåæ›´æ–°æ£€ç´¢å™¨\n        ai_service.rag_retriever = initialize_rag_system()\n        return {\"status\": \"embeddings built successfully\"}\n    else:\n        return {\"status\": \"failed to build embeddings\", \"error\": \"no documents or chunks found\"}\n    \n# æ·»åŠ å‰ç«¯æœåŠ¡\n@app.get(\"/\")\nasync def serve_frontend():\n    return FileResponse(BASE_DIR / \"frontend\" / \"index.html\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
    },
    {
      "filename": "backend\\requirements.txt",
      "content": "# requirements.txt\nfastapi>=0.68.0\nuvicorn>=0.18.3\nrequests>=2.31.0\nnumpy>=1.24.3\nPyPDF2>=3.0.1\npython-docx>=0.8.11\nmarkdown>=3.4.1\nlangchain-text-splitters>=0.0.1\nannoy>=1.17.0\ntqdm>=4.66.1  # æ·»åŠ è¿›åº¦æ¡åº“"
    },
    {
      "filename": "build_embeddings.py",
      "content": "# build_embeddings.py\nimport logging\nfrom RAG import build_vector_store\nimport time\nimport sys\n\n# é…ç½®æ—¥å¿—\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"embedding_builder\")\n\n# ä¿®å¤è¿›åº¦æ˜¾ç¤ºå‡½æ•° - ç§»é™¤å†—ä½™çš„stageå‚æ•°\ndef print_progress(**kwargs):\n    \"\"\"æ‰“å°è¿›åº¦ä¿¡æ¯\"\"\"\n    stage = kwargs.get(\"stage\", \"process\")\n    total = kwargs.get(\"total\", 0)\n    current = kwargs.get(\"current\", 0)\n    message = kwargs.get(\"message\", \"\")\n    details = kwargs.get(\"details\", \"\")\n    status = kwargs.get(\"status\", \"progress\")\n    \n    if stage == \"load\":\n        prefix = \"åŠ è½½æ–‡æ¡£\"\n    elif stage == \"split\":\n        prefix = \"åˆ†å‰²æ–‡æœ¬\"\n    elif stage == \"embed\":\n        prefix = \"ç”ŸæˆåµŒå…¥\"\n    elif stage == \"index\":\n        prefix = \"æ„å»ºç´¢å¼•\"\n    else:\n        prefix = \"å¤„ç†ä¸­\"\n    \n    if status == \"error\":\n        symbol = \"âŒ\"\n    elif status == \"completed\":\n        symbol = \"âœ…\"\n    else:\n        symbol = \"ğŸ”„\"\n    \n    if total > 0:\n        percent = current / total * 100\n        progress_bar = f\"[{'=' * int(percent/5)}{' ' * (20 - int(percent/5))}] {percent:.1f}%\"\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {progress_bar} - {message} {details}\")\n    else:\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {message} {details}\")\n    \n    sys.stdout.flush()\n    \n    if status in [\"completed\", \"error\"]:\n        print()  # å®Œæˆæ—¶æ¢è¡Œ\n\ndef main():\n    logger.info(\"Starting manual embedding process...\")\n    start_time = time.time()\n    \n    # è°ƒç”¨æ„å»ºå‡½æ•°ï¼Œä¼ å…¥è¿›åº¦å›è°ƒ\n    success = build_vector_store(progress_callback=print_progress)\n    \n    elapsed = time.time() - start_time\n    if success:\n        logger.info(f\"Embedding process completed successfully in {elapsed:.2f} seconds!\")\n    else:\n        logger.error(f\"Embedding process failed in {elapsed:.2f} seconds. Check logs for details.\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "config.py",
      "content": "import os\n\n# åŸºç¡€è·¯å¾„é…ç½®\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nDOCUMENTS_DIR = os.path.join(DATA_DIR, 'documents')\nVECTOR_STORE_DIR = os.path.join(DATA_DIR, 'vector_store')\n\n# RAGé…ç½®\nRAG_CONFIG = {\n    # æ–‡æ¡£åŠ è½½é…ç½®\n    \"document_loader\": {\n        \"extensions\": [\".txt\", \".pdf\", \".docx\", \".pptx\", \".md\", \".json\", \".csv\"]\n    },\n    \n    # æ–‡æœ¬åˆ†å‰²é…ç½® \n    \"text_splitter\": {\n        \"chunk_size\": 1500, \n        \"chunk_overlap\": 100  \n    },\n    \n    # åµŒå…¥æ¨¡å‹é…ç½® \n    \"embeddings\": {\n        \"model_type\": \"ollama\",  # ollama æˆ– huggingface\n        \"model_name\": \"all-minilm\"  \n    },\n    \n    # å‘é‡å­˜å‚¨é…ç½®\n    \"vector_store\": {\n        \"type\": \"annoy\",  \n        \"index_name\": \"document_index\"\n    },\n    \n    # æ£€ç´¢å™¨é…ç½®\n    \"retriever\": {\n        \"top_k\": 20,\n        \"score_threshold\": -1.0\n    },\n    \n    # æ–°å¢æ‘˜è¦é…ç½®\n    \"summarizer\": {\n        \"model_name\": \"qwen:7b\",  # ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆæ‘˜è¦\n        \"max_summary_length\": 15  # æ‘˜è¦æœ€å¤§é•¿åº¦ï¼ˆå­—æ•°ï¼‰\n    }\n}"
    },
    {
      "filename": "diagnose.py",
      "content": "# diagnose.py\nimport logging\nfrom RAG import initialize_rag_system\nfrom RAG.retriever import Retriever\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_rag():\n    # 1. åˆå§‹åŒ–RAG\n    logger.info(\"=== åˆå§‹åŒ–RAGç³»ç»Ÿ ===\")\n    retriever = initialize_rag_system()\n    \n    # æ–°å¢æ‘˜è¦å±‚æµ‹è¯•\n    logger.info(\"\\n=== æµ‹è¯•è¯­ä¹‰æ‘˜è¦å±‚ ===\")\n    test_queries = [\n        \"äººå¦‚ä½•è‡ªå¾‹\",\n        \"htmlæ˜¯ä»€ä¹ˆ\",\n        \"what is codeAID\",\n        \"è€å­æœ‰å“ªäº›åè¨€\"\n    ]\n    \n    for query in test_queries:\n        logger.info(f\"\\næ‘˜è¦æŸ¥è¯¢: '{query}'\")\n        # ç›´æ¥è°ƒç”¨å‘é‡å­˜å‚¨çš„æ‘˜è¦æœç´¢\n        from RAG.vector_store import VectorStore\n        vector_store = VectorStore()\n        vector_store.load_index()\n        \n        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥\n        from RAG.embeddings import EmbeddingModel\n        embedding_model = EmbeddingModel()\n        query_embedding = embedding_model.embed_texts([query])[0]\n        \n        if query_embedding:\n            # ä½¿ç”¨æ‘˜è¦ç´¢å¼•æœç´¢\n            results = vector_store.summary_index.get_nns_by_vector(\n                query_embedding, \n                5,  # è·å–å‰5ä¸ªç»“æœ\n                include_distances=True\n            )\n            \n            indices, distances = results\n            logger.info(f\"æ‰¾åˆ° {len(indices)} ä¸ªç›¸å…³æ‘˜è¦:\")\n            for i, (idx, dist) in enumerate(zip(indices, distances)):\n                if idx < len(vector_store.metadata):\n                    metadata = vector_store.metadata[idx]\n                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n                    cosine_sim = 1 - (dist ** 2) / 2.0\n                    \n                    # è¾“å‡ºæ‘˜è¦å’Œå¯¹åº”å†…å®¹\n                    logger.info(f\"{i+1}. [ç›¸ä¼¼åº¦: {cosine_sim:.3f}] æ‘˜è¦: '{metadata['summary']}'\")\n                    logger.info(f\"   æ¥æº: {metadata['source']}\")\n                    logger.info(f\"   å†…å®¹: {metadata['text'][:500]}...\")  # è¾“å‡ºå‰200ä¸ªå­—ç¬¦\n                    logger.info(\"\\n\")\n        else:\n            logger.warning(f\"æ— æ³•ç”ŸæˆæŸ¥è¯¢ '{query}' çš„åµŒå…¥å‘é‡\")\n\nif __name__ == \"__main__\":\n    test_rag()"
    },
    {
      "filename": "frontend\\index.html",
      "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Chat Module</title>\n    <link rel=\"stylesheet\" href=\"/static/style.css\">\n    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/dompurify@3.0.5/dist/purify.min.js\"></script>\n</head>\n<body>\n    <div class=\"chat-container\">\n        <div id=\"chat-history\" class=\"chat-history\"></div>\n        \n        <div class=\"input-area\">\n            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message...\">\n            <button onclick=\"sendMessage()\">Send</button>\n            <label>\n                <input type=\"checkbox\" id=\"use-rag\"> Use RAG\n            </label>\n        </div>\n        \n        <div class=\"document-upload\">\n            <h3>Document Upload</h3>\n            <input type=\"file\" id=\"document-file\">\n            <button onclick=\"uploadDocument()\">Upload</button>\n            <div id=\"upload-status\"></div>\n        </div>\n\n        <div class=\"config-panel\">\n            <h3>Configuration</h3>\n            <select id=\"model-type\">\n                <option value=\"ollama\">Ollama (Local)</option>\n                <option value=\"openai\">OpenAI API</option>\n            </select>\n            <input type=\"text\" id=\"model-name\" placeholder=\"Model name\" value=\"qwen:7b\">\n            <button onclick=\"updateConfig()\">Update Config</button>\n        </div>\n    </div>\n\n    <script src=\"/static/script.js\"></script>\n</body>\n</html>"
    },
    {
      "filename": "frontend\\script.js",
      "content": "let chatHistory = [];\n\nasync function sendMessage() {\n    const input = document.getElementById('user-input');\n    const message = input.value.trim();\n    const useRag = document.getElementById('use-rag').checked;\n    \n    if (!message) return;\n    \n    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯\n    addMessage('user', message);\n    input.value = '';\n    \n    try {\n        // å‘é€åˆ°åç«¯\n        const response = await fetch('http://localhost:8000/chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                message: message,\n                use_rag: useRag\n            })\n        });\n        \n        const data = await response.json();\n        addMessage('ai', data.response);\n    } catch (error) {\n        addMessage('ai', `Error: ${error.message}`);\n    }\n}\n\nasync function updateConfig() {\n    const modelType = document.getElementById('model-type').value;\n    const modelName = document.getElementById('model-name').value;\n    \n    try {\n        await fetch('http://localhost:8000/update_config', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                model_type: modelType,\n                model_name: modelName\n            })\n        });\n        alert('Configuration updated successfully!');\n    } catch (error) {\n        alert(`Update failed: ${error.message}`);\n    }\n}\nasync function buildEmbeddings() {\n    const statusDiv = document.getElementById('embedding-status');\n    statusDiv.textContent = \"æ­£åœ¨ç”Ÿæˆ...\";\n    \n    try {\n        const response = await fetch('http://localhost:8000/build_embeddings', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            }\n        });\n        \n        const data = await response.json();\n        statusDiv.textContent = data.status;\n    } catch (error) {\n        statusDiv.textContent = `ç”Ÿæˆå¤±è´¥: ${error.message}`;\n    }\n}\nasync function uploadDocument() {\n    const fileInput = document.getElementById('document-file');\n    const file = fileInput.files[0];\n    const statusDiv = document.getElementById('upload-status');\n    \n    if (!file) {\n        statusDiv.textContent = \"Please select a file to upload.\";\n        return;\n    }\n    statusDiv.textContent = \"Uploading...\";\n    \n    \n    try {\n        const formData = new FormData();\n        formData.append('file', file);\n        \n        const response = await fetch('http://localhost:8000/upload_document', {\n            method: 'POST',\n            body: formData\n        });\n        \n        const data = await response.json();\n        if (data.status === 'success') {\n            statusDiv.textContent = `upload success: ${data.file_path}`;\n            // æ¸…ç©ºæ–‡ä»¶è¾“å…¥\n            fileInput.value = '';\n        } else {\n            statusDiv.textContent = `upload failed: ${data.detail || 'unknown error'}`;\n        }\n    } catch (error) {\n        statusDiv.textContent = `upload error: ${error.message}`;\n    }\n}\n\n// æ·»åŠ é¡µé¢åŠ è½½äº‹ä»¶\ndocument.addEventListener('DOMContentLoaded', () => {\n    // åˆå§‹åŠ è½½å®Œæˆåæ»šåŠ¨åˆ°åº•éƒ¨\n    const chatHistory = document.getElementById('chat-history');\n    chatHistory.scrollTop = chatHistory.scrollHeight;\n});\nfunction addMessage(role, content) {\n    const chatHistoryElement = document.getElementById('chat-history');\n    const messageDiv = document.createElement('div');\n    messageDiv.className = `message ${role}-message`;\n    \n    const contentDiv = document.createElement('div');\n    contentDiv.className = 'ai-message-content';\n    \n    // å®‰å…¨æ¸²æŸ“Markdown\n    if (role === 'ai') {\n        const sanitized = DOMPurify.sanitize(marked.parse(content));\n        contentDiv.innerHTML = sanitized;\n    } else {\n        contentDiv.textContent = content;\n    }\n    \n    messageDiv.appendChild(contentDiv);\n    chatHistoryElement.appendChild(messageDiv);\n    \n    // æ»šåŠ¨åˆ°åº•éƒ¨\n    chatHistoryElement.scrollTop = chatHistoryElement.scrollHeight;\n    \n    // ä¿å­˜å†å²\n    chatHistory.push({ role, content });\n}\n\n// è¾“å…¥æ¡†å›è½¦å‘é€\ndocument.getElementById('user-input').addEventListener('keypress', (e) => {\n    if (e.key === 'Enter') sendMessage();\n});"
    },
    {
      "filename": "frontend\\style.css",
      "content": "body {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    background-color: #f5f5f5;\n    display: flex;\n    justify-content: center;\n    padding: 20px;\n    line-height: 1.6;\n}\n\n.chat-container {\n    width: 100%;\n    max-width: 800px;\n    background: white;\n    border-radius: 10px;\n    box-shadow: 0 0 20px rgba(0,0,0,0.1);\n    overflow: hidden;\n    display: flex;\n    flex-direction: column;\n    height: 90vh;\n}\n.document-upload {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    flex-direction: column;\n    gap: 10px;\n}\n\n.document-upload h3 {\n    margin: 0 0 10px 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.document-upload input[type=\"file\"] {\n    padding: 8px;\n    background: white;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n}\n.chat-history {\n    flex: 1;\n    padding: 20px;\n    overflow-y: auto;\n    display: flex;\n    flex-direction: column;\n    gap: 15px;\n}\n\n.message {\n    max-width: 80%;\n    padding: 15px;\n    border-radius: 12px;\n    line-height: 1.6;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n}\n\n.user-message {\n    align-self: flex-end;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border-bottom-right-radius: 5px;\n}\n\n.ai-message {\n    align-self: flex-start;\n    background-color: #fafafa;\n    color: #333;\n    border-bottom-left-radius: 5px;\n}\n\n.ai-message-content {\n    overflow-wrap: break-word;\n}\n\n/* Markdownæ ·å¼å¢å¼º */\n.markdown-content h1, .markdown-content h2, .markdown-content h3 {\n    margin-top: 1.2em;\n    margin-bottom: 0.6em;\n    padding-bottom: 0.2em;\n    border-bottom: 1px solid #eee;\n    color: #2c3e50;\n}\n\n.markdown-content h1 {\n    font-size: 1.8em;\n}\n\n.markdown-content h2 {\n    font-size: 1.5em;\n}\n\n.markdown-content h3 {\n    font-size: 1.3em;\n}\n\n.markdown-content p {\n    margin: 0.8em 0;\n}\n\n.markdown-content ul, .markdown-content ol {\n    margin: 0.8em 0;\n    padding-left: 1.5em;\n}\n\n.markdown-content li {\n    margin: 0.4em 0;\n}\n\n.markdown-content blockquote {\n    margin: 1em 0;\n    padding: 0.8em 1em;\n    background-color: #f8f9fa;\n    border-left: 4px solid #4e8cff;\n    color: #555;\n    border-radius: 0 8px 8px 0;\n}\n\n/* ä»£ç å—æ ·å¼ - æ·¡è“è‰²èƒŒæ™¯ */\n.markdown-content pre {\n    background-color: #e6f7ff; /* æ·¡è“è‰²èƒŒæ™¯ */\n    padding: 15px;\n    border-radius: 8px;\n    overflow-x: auto;\n    margin: 1.2em 0;\n    box-shadow: inset 0 0 5px rgba(0,0,0,0.05);\n    border: 1px solid #c0e6ff;\n}\n\n.markdown-content code {\n    font-family: 'Fira Code', 'Consolas', monospace;\n    background-color: rgba(230, 247, 255, 0.3);\n    padding: 2px 6px;\n    border-radius: 4px;\n    color: #d63384;\n}\n\n.markdown-content pre code {\n    background: none;\n    padding: 0;\n    border-radius: 0;\n    color: #333;\n}\n\n/* è¡¨æ ¼æ ·å¼ */\n.markdown-content table {\n    width: 100%;\n    border-collapse: collapse;\n    margin: 1.2em 0;\n    background-color: #fff;\n    box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n}\n\n.markdown-content th, .markdown-content td {\n    padding: 10px 15px;\n    text-align: left;\n    border: 1px solid #e1e4e8;\n}\n\n.markdown-content th {\n    background-color: #f6f8fa;\n    font-weight: 600;\n}\n\n.markdown-content tr:nth-child(even) {\n    background-color: #fafbfc;\n}\n\n/* æµç¨‹å›¾/å›¾è¡¨å®¹å™¨æ ·å¼ */\n.markdown-content .mermaid, \n.markdown-content .flowchart, \n.markdown-content .graphviz {\n    background-color: #a5c0cd; /* æ·¡è“è‰²èƒŒæ™¯ */\n    padding: 15px;\n    border-radius: 8px;\n    margin: 1.2em 0;\n    overflow-x: auto;\n    text-align: center;\n    border: 1px solid #c0e6ff;\n}\n\n/* æ°´å¹³çº¿æ ·å¼ */\n.markdown-content hr {\n    border: 0;\n    height: 1px;\n    background: linear-gradient(to right, rgba(0,0,0,0), rgba(78,140,255,0.5), rgba(0,0,0,0));\n    margin: 1.5em 0;\n}\n\n/* é“¾æ¥æ ·å¼ */\n.markdown-content a {\n    color: #1e6bb8;\n    text-decoration: none;\n    border-bottom: 1px dashed #4e8cff;\n    transition: all 0.2s;\n}\n\n.markdown-content a:hover {\n    color: #0d4a9e;\n    border-bottom: 1px solid #0d4a9e;\n}\n\n/* é”®ç›˜æ ‡ç­¾æ ·å¼ */\n.markdown-content kbd {\n    background-color: #f6f8fa;\n    border: 1px solid #d1d5da;\n    border-radius: 4px;\n    box-shadow: inset 0 -1px 0 #d1d5da;\n    color: #444d56;\n    display: inline-block;\n    font-family: monospace;\n    font-size: 0.9em;\n    line-height: 1;\n    padding: 3px 5px;\n    vertical-align: middle;\n}\n\n/* æç¤ºæ¡†æ ·å¼ */\n.markdown-content .tip, \n.markdown-content .note, \n.markdown-content .warning {\n    padding: 12px 15px;\n    margin: 1.2em 0;\n    border-radius: 8px;\n    border-left: 4px solid;\n}\n\n.markdown-content .tip {\n    background-color: #e6f7ff;\n    border-color: #4e8cff;\n}\n\n.markdown-content .note {\n    background-color: #fff8e6;\n    border-color: #ffc53d;\n}\n\n.markdown-content .warning {\n    background-color: #ffebee;\n    border-color: #f44336;\n}\n\n.input-area {\n    display: flex;\n    padding: 15px;\n    background: #f9f9f9;\n    border-top: 1px solid #eee;\n    gap: 10px;\n    align-items: center;\n}\n\n.input-area input {\n    flex: 1;\n    padding: 12px 18px;\n    border: 1px solid #ddd;\n    border-radius: 25px;\n    font-size: 16px;\n    outline: none;\n    transition: border-color 0.3s;\n}\n\n.input-area input:focus {\n    border-color: #4e8cff;\n    box-shadow: 0 0 0 2px rgba(78, 140, 255, 0.2);\n}\n\n.input-area button {\n    padding: 12px 24px;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border: none;\n    border-radius: 25px;\n    cursor: pointer;\n    font-weight: bold;\n    transition: all 0.2s;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n}\n\n.input-area button:hover {\n    background: linear-gradient(135deg, #3a75e0, #2a65d0);\n    transform: translateY(-2px);\n    box-shadow: 0 4px 8px rgba(0,0,0,0.15);\n}\n\n.config-panel {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    gap: 10px;\n    align-items: center;\n    flex-wrap: wrap;\n}\n\n.config-panel h3 {\n    margin: 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.config-panel select, .config-panel input {\n    padding: 8px 12px;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n    background: white;\n    font-size: 14px;\n}\n\n.config-panel button {\n    padding: 8px 15px;\n    background: #5a6268;\n    color: white;\n    border: none;\n    border-radius: 4px;\n    cursor: pointer;\n    transition: background 0.2s;\n}\n\n.config-panel button:hover {\n    background: #484e53;\n}\n\n/* æ»šåŠ¨æ¡ç¾åŒ– */\n.chat-history::-webkit-scrollbar {\n    width: 8px;\n}\n\n.chat-history::-webkit-scrollbar-track {\n    background: #f1f1f1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb {\n    background: #c1c1c1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb:hover {\n    background: #a8a8a8;\n}"
    },
    {
      "filename": "run_demo.py",
      "content": "\"\"\"\nä¸€é”®å¯åŠ¨RAGç³»ç»Ÿï¼Œå¯åŠ¨åç«¯æœåŠ¡ï¼Œæ‰“å¼€å‰ç«¯é¡µé¢\n\"\"\"\n\"\"\"run_demo.py\nä¸€é”®å¯åŠ¨RAGç³»ç»Ÿï¼Œå¯åŠ¨åç«¯æœåŠ¡ï¼Œæ‰“å¼€å‰ç«¯é¡µé¢\n\"\"\"\nimport subprocess\nimport webbrowser\nimport time\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nimport requests\nimport logging\nimport threading\n\n# è®¾ç½®æ—¥å¿—\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif getattr(sys, 'frozen', False):\n    BASE_DIR = Path(sys.executable).parent\nelse:\n    BASE_DIR = Path(__file__).resolve().parent\n\n# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„\nsys.path.append(str(BASE_DIR))\n\n# è¿›åº¦æ˜¾ç¤ºå‡½æ•°\ndef demo_progress(stage, total=0, current=0, message=\"\", details=\"\", status=\"progress\"):\n    \"\"\"æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯\"\"\"\n    if stage == \"rag_init\":\n        prefix = \"ğŸ§© RAGåˆå§‹åŒ–\"\n    elif stage == \"server\":\n        prefix = \"ğŸš€ åç«¯æœåŠ¡\"\n    elif stage == \"browser\":\n        prefix = \"ğŸŒ æµè§ˆå™¨\"\n    else:\n        prefix = \"âš™ï¸  å¤„ç†ä¸­\"\n    \n    if status == \"error\":\n        symbol = \"âŒ\"\n    elif status == \"completed\":\n        symbol = \"âœ…\"\n    else:\n        symbol = \"ğŸ”„\"\n    \n    if total > 0:\n        percent = current / total * 100\n        progress_bar = f\"[{'=' * int(percent/5)}{' ' * (20 - int(percent/5))}] {percent:.1f}%\"\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {progress_bar} - {message} {details}\")\n    else:\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {message} {details}\")\n    \n    sys.stdout.flush()\n    \n    if status in [\"completed\", \"error\"]:\n        print()  # å®Œæˆæ—¶æ¢è¡Œ\n\ndef initialize_rag():\n    \"\"\"åˆå§‹åŒ–RAGç³»ç»Ÿ\"\"\"\n    # æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦è¿è¡Œ\n    demo_progress(\n        stage=\"rag_init\",\n        message=\"æ£€æŸ¥OllamaæœåŠ¡...\",\n        status=\"progress\"\n    )\n    \n    try:\n        response = requests.get(\"http://localhost:11434\", timeout=5)\n        if response.status_code != 200:\n            logger.error(\"Ollama service not running. Please start Ollama first.\")\n            demo_progress(\n                stage=\"rag_init\",\n                message=\"OllamaæœåŠ¡æœªè¿è¡Œ\",\n                status=\"error\"\n            )\n            return None\n    except Exception as e:\n        logger.error(f\"Ollama service not running: {str(e)}. Please start Ollama first.\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"OllamaæœåŠ¡é”™è¯¯: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n    \n    # å¯¼å…¥RAGåˆå§‹åŒ–å‡½æ•°\n    try:\n        from RAG import initialize_rag_system\n    except ImportError as e:\n        logger.error(f\"Error importing RAG module: {str(e)}\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"å¯¼å…¥RAGæ¨¡å—å¤±è´¥: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n    \n    # åˆ›å»ºRAGç³»ç»Ÿ\n    demo_progress(\n        stage=\"rag_init\",\n        message=\"åˆå§‹åŒ–ç³»ç»Ÿ...\",\n        status=\"progress\"\n    )\n    \n    try:\n        rag_retriever = initialize_rag_system(force_rebuild=False)\n        demo_progress(\n            stage=\"rag_init\",\n            message=\"RAGç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ\",\n            status=\"completed\"\n        )\n        return rag_retriever\n    except Exception as e:\n        logger.error(f\"Error initializing RAG system: {str(e)}\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"åˆå§‹åŒ–å¤±è´¥: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n\ndef start_server():\n    \"\"\"å¯åŠ¨åç«¯æœåŠ¡å™¨\"\"\"\n    demo_progress(\n        stage=\"server\",\n        message=\"æ­£åœ¨å¯åŠ¨åç«¯æœåŠ¡...\",\n        status=\"progress\"\n    )\n    \n    try:\n        # ä½¿ç”¨Popenå¯åŠ¨æœåŠ¡å™¨\n        process = subprocess.Popen(\n            [sys.executable, \"-m\", \"uvicorn\", \"backend.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n            cwd=BASE_DIR,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨\n        time.sleep(3)\n        \n        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å¯åŠ¨\n        try:\n            response = requests.get(\"http://localhost:8000\", timeout=5)\n            if response.status_code == 200:\n                demo_progress(\n                    stage=\"server\",\n                    message=\"åç«¯æœåŠ¡å¯åŠ¨æˆåŠŸ\",\n                    status=\"completed\"\n                )\n                return process\n            else:\n                demo_progress(\n                    stage=\"server\",\n                    message=f\"åç«¯è¿”å›çŠ¶æ€ç : {response.status_code}\",\n                    status=\"error\"\n                )\n                return None\n        except Exception as e:\n            demo_progress(\n                stage=\"server\",\n                message=f\"è¿æ¥åç«¯å¤±è´¥: {str(e)}\",\n                status=\"error\"\n            )\n            return None\n    except Exception as e:\n        demo_progress(\n            stage=\"server\",\n            message=f\"å¯åŠ¨æœåŠ¡å™¨å¤±è´¥: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n\ndef run_demo():\n    # åˆå§‹åŒ–RAGç³»ç»Ÿ\n    rag_retriever = initialize_rag()\n    if not rag_retriever:\n        logger.error(\"Failed to initialize RAG system. Exiting.\")\n        return\n    \n    # å¯åŠ¨åç«¯\n    server_process = start_server()\n    if not server_process:\n        logger.error(\"Failed to start backend server. Exiting.\")\n        return\n    \n    # æ‰“å¼€å‰ç«¯\n    demo_progress(\n        stage=\"browser\",\n        message=\"æ­£åœ¨æ‰“å¼€èŠå¤©ç•Œé¢...\",\n        status=\"progress\"\n    )\n    time.sleep(1)  # ç¡®ä¿æœåŠ¡å™¨å®Œå…¨å¯åŠ¨\n    \n    try:\n        webbrowser.open('http://localhost:8000')\n        demo_progress(\n            stage=\"browser\",\n            message=\"èŠå¤©ç•Œé¢å·²æ‰“å¼€\",\n            status=\"completed\"\n        )\n    except Exception as e:\n        demo_progress(\n            stage=\"browser\",\n            message=f\"æ‰“å¼€æµè§ˆå™¨å¤±è´¥: {str(e)}\",\n            status=\"error\"\n        )\n    \n    logger.info(\"Chat interface opened. Press Ctrl+C to stop.\")\n    \n    try:\n        # æ‰“å°æœåŠ¡å™¨æ—¥å¿—\n        def log_stream(stream, prefix):\n            for line in stream:\n                if line:  # ç¡®ä¿è¡Œä¸ä¸ºç©º\n                    logger.info(f\"{prefix}: {line.strip()}\")\n        \n        # å¯åŠ¨çº¿ç¨‹æ•è·stdoutå’Œstderr\n        stdout_thread = threading.Thread(\n            target=log_stream, \n            args=(server_process.stdout, \"SERVER\"),\n            daemon=True\n        )\n        stderr_thread = threading.Thread(\n            target=log_stream, \n            args=(server_process.stderr, \"ERROR\"),\n            daemon=True\n        )\n        \n        stdout_thread.start()\n        stderr_thread.start()\n        \n        # ç­‰å¾…æœåŠ¡å™¨è¿›ç¨‹ç»“æŸ\n        server_process.wait()\n    except KeyboardInterrupt:\n        logger.info(\"Stopping server...\")\n        server_process.terminate()\n        logger.info(\"Server stopped\")\n\nif __name__ == \"__main__\":\n    run_demo()"
    }
  ]
}