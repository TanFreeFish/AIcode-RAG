{
  "tree": {
    "name": ".",
    "directories": [
      {
        "name": "RAG",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 0
          },
          {
            "name": "document_loader.py",
            "content_index": 1
          },
          {
            "name": "embeddings.py",
            "content_index": 2
          },
          {
            "name": "retriever.py",
            "content_index": 3
          },
          {
            "name": "text_splitter.py",
            "content_index": 4
          },
          {
            "name": "vector_store.py",
            "content_index": 5
          }
        ]
      },
      {
        "name": "backend",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 6
          },
          {
            "name": "ai_service.py",
            "content_index": 7
          },
          {
            "name": "main.py",
            "content_index": 8
          },
          {
            "name": "requirements.txt",
            "content_index": 9
          }
        ]
      },
      {
        "name": "frontend",
        "directories": [],
        "files": [
          {
            "name": "index.html",
            "content_index": 13
          },
          {
            "name": "script.js",
            "content_index": 14
          },
          {
            "name": "style.css",
            "content_index": 15
          }
        ]
      }
    ],
    "files": [
      {
        "name": "build_embeddings.py",
        "content_index": 10
      },
      {
        "name": "config.py",
        "content_index": 11
      },
      {
        "name": "diagnose.py",
        "content_index": 12
      },
      {
        "name": "run_demo.py",
        "content_index": 16
      }
    ]
  },
  "code_blocks": [
    {
      "filename": "RAG\\__init__.py",
      "content": "# RAG/__init__.py\nfrom .document_loader import DocumentLoader\nfrom .text_splitter import TextSplitter\nfrom .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom .retriever import Retriever\nfrom config import DOCUMENTS_DIR, VECTOR_STORE_DIR\nimport os\nimport time\nfrom tqdm import tqdm\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef initialize_rag_system(force_rebuild=False):\n    \"\"\"初始化RAG系统\"\"\"\n    vector_store = VectorStore(rebuild_mode=force_rebuild)  # 添加重建模式参数\n    if not force_rebuild and vector_store.index_path.exists() and vector_store.metadata_path.exists():\n        logger.info(\"Using existing vector store\")\n        return Retriever()\n    \n    # 需要构建向量库\n    logger.info(\"Vector store not found or incomplete. Building new vector store...\")\n    if build_vector_store():\n        return Retriever()\n    else:\n        logger.error(\"Failed to build vector store\")\n        return Retriever()\n\ndef build_vector_store(progress_callback=None):\n    \"\"\"手动构建向量存储\"\"\"\n    logger.info(\"Building new vector store...\")\n    \n    # 加载文档\n    loader = DocumentLoader()\n    documents = loader.load_documents()\n    \n    # 添加进度回调\n    progress_callback = progress_callback or (lambda **kw: None)\n    progress_callback(stage=\"load\", total=len(documents), current=0, message=\"开始加载文档\")\n    \n    if not documents:\n        logger.warning(\"No documents found to build vector store\")\n        progress_callback(stage=\"load\", message=\"未找到文档\", status=\"error\")\n        return False\n    \n    # 分割文档\n    splitter = TextSplitter(progress_callback=progress_callback)\n    chunks = splitter.split_documents(documents)\n    \n    # 检查是否有文本块\n    if not chunks:\n        logger.warning(\"No text chunks created from documents\")\n        progress_callback(stage=\"split\", message=\"未生成文本块\", status=\"error\")\n        return False\n    \n    # 生成嵌入\n    embedding_model = EmbeddingModel()\n    texts = [chunk[\"text\"] for chunk in chunks]\n    \n    # 分批处理嵌入生成\n    batch_size = 32\n    embeddings = []\n    logger.info(\"Generating embeddings...\")\n    start_time = time.time()\n    \n    # 添加进度回调\n    total_batches = (len(texts) + batch_size - 1) // batch_size\n    progress_callback(\n        stage=\"embed\",\n        total=total_batches,\n        current=0,\n        message=\"开始生成嵌入向量\",\n        details=f\"共 {len(texts)} 个文本块，分 {total_batches} 批处理\"\n    )\n    \n    # 使用tqdm显示进度条\n    for batch_idx, i in enumerate(tqdm(range(0, len(texts), batch_size), desc=\"生成嵌入\")):\n        batch_texts = texts[i:i+batch_size]\n        batch_embeddings = embedding_model.embed_texts(batch_texts)\n        \n        # 更新进度\n        progress_callback(\n            stage=\"embed\",\n            current=batch_idx + 1,\n            total=total_batches,\n            message=f\"正在处理第 {batch_idx+1}/{total_batches} 批\",\n            details=f\"文本块 {i+1}-{min(i+batch_size, len(texts))}\"\n        )\n        \n        # 检查嵌入是否有效\n        valid_embeddings = []\n        for emb in batch_embeddings:\n            if emb and len(emb) == embedding_model.dim:\n                valid_embeddings.append(emb)\n            else:\n                logger.warning(f\"Invalid embedding at batch index {i}\")\n                valid_embeddings.append([0.0] * embedding_model.dim)  # 使用零向量占位\n        \n        embeddings.extend(valid_embeddings)\n    \n    # 检查嵌入数量是否匹配\n    if len(embeddings) != len(chunks):\n        logger.warning(f\"Embeddings count ({len(embeddings)}) doesn't match chunks count ({len(chunks)})\")\n        # 填充缺失的嵌入\n        embeddings.extend([[0.0] * embedding_model.dim] * (len(chunks) - len(embeddings)))\n    \n    logger.info(f\"Embeddings generated in {time.time()-start_time:.2f} seconds\")\n    \n    # 添加到向量存储（使用重建模式）\n    vector_store = VectorStore(rebuild_mode=True)  # 启用重建模式\n    \n    # 添加索引构建进度回调 - 修复这里的问题\n    def index_progress(**kwargs):\n        # 直接传递所有参数，不再添加额外的stage参数\n        progress_callback(**kwargs)\n    \n    # 调用add_chunks并检查返回值\n    success = vector_store.add_chunks(chunks, embeddings, progress_callback=index_progress)\n    \n    if success:\n        logger.info(\"Vector store built successfully\")\n        return True\n    else:\n        logger.error(\"Failed to build vector store\")\n        return False"
    },
    {
      "filename": "RAG\\document_loader.py",
      "content": "import os\nfrom pathlib import Path\nfrom config import DOCUMENTS_DIR, RAG_CONFIG\nimport PyPDF2\nfrom docx import Document\nimport markdown\nimport re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentLoader:\n    def __init__(self):\n        self.extensions = RAG_CONFIG[\"document_loader\"][\"extensions\"]\n        self.documents_dir = Path(DOCUMENTS_DIR)\n        self.documents_dir.mkdir(parents=True, exist_ok=True)\n    \n    def load_documents(self):\n        \"\"\"加载文档目录中的所有支持文档\"\"\"\n        documents = []\n        \n        for ext in self.extensions:\n            for file_path in self.documents_dir.glob(f\"*{ext}\"):\n                content = self._load_file(file_path)\n                if content:\n                    documents.append({\n                        \"file_path\": str(file_path),\n                        \"content\": content\n                    })\n                    logger.info(f\"Loaded document: {file_path.name}\")\n        \n        return documents\n    \n    def _load_file(self, file_path):\n        \"\"\"根据文件类型加载内容\"\"\"\n        ext = file_path.suffix.lower()\n        \n        try:\n            if ext == \".txt\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return f.read()\n            \n            elif ext == \".pdf\":\n                content = []\n                with open(file_path, 'rb') as f:\n                    pdf_reader = PyPDF2.PdfReader(f)\n                    for page in pdf_reader.pages:\n                        text = page.extract_text()\n                        if text:\n                            content.append(text)\n                return \"\\n\".join(content)\n            \n            elif ext == \".json\":  \n                with open(file_path, 'r', encoding='utf-8') as f:\n                    import json\n                    data = json.load(f)\n                    return str(data)  \n            elif ext == \".docx\":\n                doc = Document(file_path)\n                return \"\\n\".join([para.text for para in doc.paragraphs])\n            \n            elif ext == \".md\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return markdown.markdown(f.read())\n            \n            else:\n                logger.warning(f\"Unsupported file type: {ext}\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error loading {file_path}: {str(e)}\")\n            return None\n\n    def add_document(self, file_path):\n        \"\"\"添加单个文档到存储\"\"\"\n        dest_path = self.documents_dir / Path(file_path).name\n        try:\n            with open(file_path, 'rb') as src, open(dest_path, 'wb') as dst:\n                dst.write(src.read())\n            logger.info(f\"Added document: {dest_path}\")\n            return str(dest_path)\n        except Exception as e:\n            logger.error(f\"Failed to add document: {str(e)}\")\n            return None"
    },
    {
      "filename": "RAG\\embeddings.py",
      "content": "import requests\nimport numpy as np\nfrom typing import List\nfrom config import RAG_CONFIG\nimport logging\nimport time\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingModel:\n    def __init__(self):\n        config = RAG_CONFIG[\"embeddings\"]\n        self.model_type = config[\"model_type\"]\n        self.model_name = config[\"model_name\"]\n        self.dim = config.get(\"dim\", 384)\n        self.api_url = \"http://localhost:11434/api/embeddings\"\n    \n    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"将文本列表转换为嵌入向量\"\"\"\n        if not texts:\n            return []\n            \n        if self.model_type == \"ollama\":\n            return self._embed_with_ollama(texts)\n        elif self.model_type == \"huggingface\":\n            return self._embed_with_huggingface(texts)\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    \n    def _embed_with_ollama(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"使用Ollama API生成嵌入 - 增强错误处理和重试机制\"\"\"\n        embeddings = []\n        for text in texts:\n            if not text.strip():\n                embeddings.append([])\n                continue\n                \n            for attempt in range(3):  \n                try:\n                    response = requests.post(\n                        self.api_url,\n                        json={\n                            \"model\": self.model_name,\n                            \"prompt\": text\n                        },\n                        timeout=30\n                    )\n                    \n                    if response.status_code == 200:\n                        data = response.json()\n                        if \"embedding\" in data and data[\"embedding\"]:\n                            embedding = data[\"embedding\"]\n                            # 检查嵌入维度\n                            if len(embedding) == self.dim:\n                                embeddings.append(embedding)\n                                break\n                            else:\n                                logger.warning(f\"Embedding dimension mismatch: expected {self.dim}, got {len(embedding)}\")\n                                embeddings.append([])\n                                break\n                        else:\n                            logger.warning(f\"Empty embedding for text: {text[:50]}...\")\n                    else:\n                        logger.error(f\"Error embedding text: {response.status_code} - {response.text}\")\n                    \n                   \n                    if attempt == 2:\n                        logger.error(f\"Failed to generate embedding after 3 attempts for text: {text[:50]}...\")\n                        embeddings.append([])\n                except Exception as e:\n                    logger.error(f\"Ollama embedding error: {str(e)}\")\n                    if attempt == 2:\n                        embeddings.append([])\n        return embeddings\n    \n    def _embed_with_huggingface(self, texts: List[str]) -> List[List[float]]:\n        \n        return [[] for _ in texts] "
    },
    {
      "filename": "RAG\\retriever.py",
      "content": "from .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom config import RAG_CONFIG\nfrom pathlib import Path\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Retriever:\n    def __init__(self):\n        self.embedding_model = EmbeddingModel()\n        self.vector_store = VectorStore()\n        self.vector_store.load_index()\n        self.top_k = RAG_CONFIG[\"retriever\"][\"top_k\"]\n        self.score_threshold = RAG_CONFIG[\"retriever\"][\"score_threshold\"]\n    \n    def retrieve(self, query: str) -> str:\n        \"\"\"检索与查询相关的上下文\"\"\"\n        # 生成查询嵌入\n        query_embedding = self.embedding_model.embed_texts([query])\n        if not query_embedding or not query_embedding[0]:\n            logger.warning(f\"Failed to generate embedding for query: '{query}'\")\n            return \"\"\n        \n        # 确保嵌入向量格式正确\n        if not isinstance(query_embedding[0], list) or not all(isinstance(x, float) for x in query_embedding[0]):\n            logger.error(f\"Invalid embedding format for query: '{query}'\")\n            return \"\"\n        \n        # 执行相似度搜索\n        results = self.vector_store.similarity_search(\n            query_embedding[0], \n            top_k=self.top_k\n        )\n        \n        # 构建上下文\n        context = []\n        for score, chunk_id, chunk_data in results:\n            if score >= self.score_threshold:\n                context.append({\n                    \"text\": chunk_data[\"text\"],\n                    \"summary\": chunk_data[\"summary\"],  # 包含摘要\n                    \"source\": chunk_data[\"source\"],\n                    \"score\": round(score, 3)\n                })\n        \n        # 格式化上下文\n        return self._format_context(context)\n    \n    def _format_context(self, context_items) -> str:\n        \"\"\"格式化检索到的上下文\"\"\"\n        if not context_items:\n            return \"没有找到相关上下文信息\"\n        \n        context_str = \"检索到的相关上下文信息：\\n\\n\"\n        for i, item in enumerate(context_items, 1):\n            source_name = Path(item[\"source\"]).name\n            context_str += f\"### 上下文片段 {i} (来源: {source_name}, 相似度: {item['score']}, 摘要: {item['summary']})\\n\"\n            context_str += f\"{item['text']}\\n\\n\"\n        \n        return context_str.strip()"
    },
    {
      "filename": "RAG\\text_splitter.py",
      "content": "import re\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom config import RAG_CONFIG\nfrom pathlib import Path\nimport logging\nimport requests\nimport time\nfrom tqdm import tqdm  # 导入进度条库\nfrom typing import List\n\nlogger = logging.getLogger(__name__)\n\nclass TextSplitter:\n    def __init__(self, progress_callback=None):\n        config = RAG_CONFIG[\"text_splitter\"]\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=config[\"chunk_size\"],\n            chunk_overlap=config[\"chunk_overlap\"],\n            separators=[\"\\n\\n\", \"\\n\", \"。\", \"？\", \"！\", \"；\", \" \", \"\"],\n            keep_separator=True\n        )\n        self.summarizer_config = RAG_CONFIG.get(\"summarizer\", {})\n        self.progress_callback = progress_callback or (lambda **kw: None)\n    \n    def generate_summary(self, text: str) -> str:\n        \"\"\"使用Qwen模型生成短语级摘要\"\"\"\n        try:\n            prompt = f\"请用5-10个字的短语总结以下文本的核心内容，不要解释，只输出短语：\\n{text}\"\n            \n            response = requests.post(\n                \"http://localhost:11434/api/generate\",\n                json={\n                    \"model\": self.summarizer_config.get(\"model_name\", \"qwen:7b\"),\n                    \"prompt\": prompt,\n                    \"stream\": False\n                },\n                timeout=30\n            )\n            \n            if response.status_code == 200:\n                return response.json().get(\"response\", \"\").strip().replace('\"', '')\n            else:\n                logger.warning(f\"摘要生成失败: {response.status_code} - {response.text}\")\n        except Exception as e:\n            logger.error(f\"摘要生成错误: {str(e)}\")\n        \n        # 失败时回退：使用前N个词\n        return \" \".join(text.split()[:5])\n    # 在 TextSplitter 类中添加智能分割方法\n    def _smart_split(self, text: str) -> List[str]:\n        \"\"\"改进的分割逻辑，确保句子完整性\"\"\"\n        # 定义中文句子结束符\n        sentence_endings = {'。', '？', '！', '；', '.', '?', '!', ';'}\n        \n        chunks = []\n        current_chunk = \"\"\n        char_count = 0\n        \n        # 按字符迭代，保持句子完整性\n        for char in text:\n            current_chunk += char\n            char_count += 1\n            \n            # 达到最小分割长度且遇到句子结束符\n            if char_count >= self.splitter._chunk_size * 0.7 and char in sentence_endings:\n                chunks.append(current_chunk.strip())\n                current_chunk = \"\"\n                char_count = 0\n                \n            # 达到最大长度强制分割（避免过长）\n            elif char_count >= self.splitter._chunk_size:\n                # 寻找最近的句子边界\n                for i in range(len(current_chunk)-1, -1, -1):\n                    if current_chunk[i] in sentence_endings:\n                        chunks.append(current_chunk[:i+1].strip())\n                        current_chunk = current_chunk[i+1:]\n                        char_count = len(current_chunk)\n                        break\n                else:  # 没有找到边界则硬分割\n                    chunks.append(current_chunk.strip())\n                    current_chunk = \"\"\n                    char_count = 0\n        \n        # 添加剩余内容\n        if current_chunk:\n            chunks.append(current_chunk.strip())\n        \n        return chunks\n    def split_documents(self, documents):\n        \"\"\"分割文档为文本块并生成摘要\"\"\"\n        chunks = []\n        total_docs = len(documents)\n        \n        # 添加进度回调\n        self.progress_callback(stage=\"split\", total=total_docs, current=0, message=\"开始分割文档\")\n        \n        for doc_idx, doc in enumerate(tqdm(documents, desc=\"分割文档\")):\n            content = doc[\"content\"]\n            # 清理多余空白\n            content = re.sub(r'\\s+', ' ', content).strip()\n            # 分割文本\n            split_texts = self._smart_split(content)\n            \n            # 更新进度\n            self.progress_callback(\n                stage=\"split\",\n                current=doc_idx + 1,\n                total=total_docs,\n                message=f\"正在处理文档: {Path(doc['file_path']).name}\",\n                details=f\"分割成 {len(split_texts)} 个片段\"\n            )\n            \n            for i, text in enumerate(split_texts):\n                # 生成语义摘要\n                summary = self.generate_summary(text)\n                \n                chunks.append({\n                    \"text\": text,\n                    \"summary\": summary,\n                    \"source\": doc[\"file_path\"],\n                    \"chunk_id\": f\"{Path(doc['file_path']).stem}_{i}\"\n                })\n        \n        # 完成进度\n        self.progress_callback(\n            stage=\"split\",\n            current=total_docs,\n            total=total_docs,\n            message=f\"文档分割完成\",\n            details=f\"共生成 {len(chunks)} 个文本块\"\n        )\n        \n        return chunks"
    },
    {
      "filename": "RAG\\vector_store.py",
      "content": "# RAG/vector_store.py\nfrom annoy import AnnoyIndex\nimport json\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom config import VECTOR_STORE_DIR, RAG_CONFIG\nimport logging\nfrom tqdm import tqdm\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass VectorStore:\n    def __init__(self, rebuild_mode=False):\n        self.store_dir = Path(VECTOR_STORE_DIR)\n        self.store_dir.mkdir(parents=True, exist_ok=True)\n        config = RAG_CONFIG[\"vector_store\"]\n        self.store_type = config[\"type\"]\n        self.index_name = config[\"index_name\"]\n        self.index_path = self.store_dir / f\"{self.index_name}.ann\"\n        self.summary_index_path = self.store_dir / f\"{self.index_name}_summary.ann\"\n        self.metadata_path = self.store_dir / f\"{self.index_name}_metadata.json\"\n        self.index = None\n        self.summary_index = None\n        self.metadata = []\n        self.chunk_ids = []\n        self.rebuild_mode = rebuild_mode\n        \n        # 从嵌入配置获取维度\n        embedding_config = RAG_CONFIG[\"embeddings\"]\n        self.dim = embedding_config.get(\"dim\", 384)\n        \n        self.distance_metric = \"angular\"\n        \n        # 确保索引对象被正确创建\n        self.load_index()\n        \n    def load_index(self):\n        try:\n            # 重建模式或索引不存在时创建新索引\n            if self.rebuild_mode or not (self.index_path.exists() and self.summary_index_path.exists() and self.metadata_path.exists()):\n                logger.info(\"Creating new index (rebuild mode or no existing index)\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n                self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n                self.metadata = []\n                self.chunk_ids = []\n            else:\n                logger.info(f\"Loading existing index from {self.index_path}\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n                self.index.load(str(self.index_path))\n                \n                # 加载摘要索引\n                self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n                self.summary_index.load(str(self.summary_index_path))\n                \n                with open(self.metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                # 正确加载元数据和块ID\n                self.metadata = metadata.get(\"chunks\", [])\n                self.chunk_ids = metadata.get(\"chunk_ids\", [])\n                logger.info(f\"Loaded Annoy index with {len(self.metadata)} chunks\")\n        except Exception as e:\n            logger.error(f\"Error loading index: {str(e)}\")\n            # 创建新的索引作为回退\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n            self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n            self.metadata = []\n            self.chunk_ids = []\n    def add_chunks(self, chunks, embeddings, progress_callback=None):\n        if not embeddings:\n            logger.warning(\"No embeddings provided, skipping add_chunks\")\n            return False\n            \n        # 确保索引对象存在\n        if self.index is None:\n            logger.warning(\"Index is None, creating new index\")\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n        if self.summary_index is None:\n            logger.warning(\"Summary index is None, creating new summary index\")\n            self.summary_index = AnnoyIndex(self.dim, self.distance_metric)\n        \n        # 设置进度回调\n        progress_callback = progress_callback or (lambda **kw: None)\n        total_chunks = len(chunks)\n        \n        # 发送进度开始消息\n        progress_callback(\n            stage=\"index\",\n            total=total_chunks,\n            current=0,\n            message=\"开始构建索引\",\n            details=f\"共 {total_chunks} 个文本块\"\n        )\n        \n        # 重置元数据和块ID\n        self.metadata = []\n        self.chunk_ids = []\n        \n        # 添加向量到Annoy索引 - 使用连续索引ID\n        valid_count = 0\n        for i, (chunk, embedding) in enumerate(tqdm(zip(chunks, embeddings), desc=\"构建索引\")):\n            if not embedding or len(embedding) != self.dim:\n                logger.warning(f\"Skipping invalid embedding for chunk {i}\")\n                continue\n                \n            try:\n                # 确保嵌入是浮点数列表\n                embedding = [float(x) for x in embedding]\n                \n                # 归一化嵌入向量\n                embedding_arr = np.array(embedding, dtype=np.float32)\n                norm = np.linalg.norm(embedding_arr)\n                if norm > 0:\n                    embedding_arr = embedding_arr / norm\n                else:\n                    # 零向量处理 - 跳过无效嵌入\n                    logger.warning(f\"Zero vector embedding for chunk {i}, skipping\")\n                    continue\n                \n                # 使用连续索引ID (valid_count) 而不是文件索引(i)\n                self.index.add_item(valid_count, embedding_arr)\n                \n                # 生成摘要嵌入并添加到摘要索引\n                summary_embedding = self._get_summary_embedding(chunk[\"summary\"])\n                if summary_embedding and len(summary_embedding) == self.dim:\n                    summary_arr = np.array(summary_embedding, dtype=np.float32)\n                    summary_norm = np.linalg.norm(summary_arr)\n                    if summary_norm > 0:\n                        summary_arr = summary_arr / summary_norm\n                    self.summary_index.add_item(valid_count, summary_arr)\n                else:\n                    # 使用主嵌入作为回退\n                    self.summary_index.add_item(valid_count, embedding_arr)\n                \n                self.metadata.append({\n                    \"text\": chunk[\"text\"],\n                    \"summary\": chunk[\"summary\"],\n                    \"source\": chunk[\"source\"]\n                })\n                self.chunk_ids.append(chunk[\"chunk_id\"])\n                valid_count += 1\n                \n                # 更新进度 - 每10个块更新一次\n                if (i + 1) % 10 == 0 or (i + 1) == total_chunks:\n                    progress_callback(\n                        stage=\"index\",\n                        current=i + 1,\n                        total=total_chunks,\n                        message=f\"正在添加文本块 {i+1}/{total_chunks}\",\n                        details=f\"有效块: {valid_count}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Error adding chunk {i}: {str(e)}\")\n        \n        if valid_count == 0:\n            logger.error(\"No valid embeddings added to index\")\n            progress_callback(\n                stage=\"index\",\n                message=\"未添加有效嵌入\",\n                status=\"error\"\n            )\n            return False\n            \n        # 构建索引\n        logger.info(f\"Building index with {valid_count} items...\")\n        progress_callback(\n            stage=\"index\",\n            message=\"正在构建索引结构...\",\n            details=f\"共 {valid_count} 个项目\"\n        )\n        \n        try:\n            self.index.build(10)\n            self.summary_index.build(10)\n        except Exception as e:\n            logger.error(f\"Error building index: {str(e)}\")\n            progress_callback(\n                stage=\"index\",\n                message=\"索引构建失败\",\n                status=\"error\"\n            )\n            return False\n        \n        logger.info(\"Index built successfully\")\n        progress_callback(\n            stage=\"index\",\n            message=\"索引构建完成\",\n            status=\"completed\"\n        )\n        \n        # 保存索引\n        self.save_index()\n        return True\n\n    def _get_summary_embedding(self, summary: str) -> list:\n        \"\"\"获取摘要的嵌入向量\"\"\"\n        try:\n            # 动态导入避免循环依赖\n            from RAG.embeddings import EmbeddingModel\n            embedding_model = EmbeddingModel()\n            if summary and summary.strip():\n                return embedding_model.embed_texts([summary])[0]\n            return None\n        except ImportError as e:\n            logger.error(f\"导入EmbeddingModel失败: {str(e)}\")\n            return None\n        except Exception as e:\n            logger.error(f\"生成摘要嵌入失败: {str(e)}\")\n            return None\n\n    def similarity_search(self, query_embedding, top_k=5):\n        if self.index is None or self.summary_index is None:\n            logger.warning(\"Index is None, cannot perform search\")\n            return []\n            \n        if not query_embedding or len(query_embedding) != self.dim:\n            logger.error(f\"Invalid query embedding: expected dim={self.dim}, got {len(query_embedding) if query_embedding else 'none'}\")\n            return []\n            \n        # 确保查询嵌入是浮点数列表\n        try:\n            query_embedding = [float(x) for x in query_embedding]\n        except Exception as e:\n            logger.error(f\"Error converting query embedding: {str(e)}\")\n            return []\n        \n        # 归一化查询向量\n        query_embedding_arr = np.array(query_embedding, dtype=np.float32)\n        query_norm = np.linalg.norm(query_embedding_arr)\n        if query_norm > 0:\n            query_embedding_arr = query_embedding_arr / query_norm\n        else:\n            # 零向量处理\n            query_embedding_arr = np.zeros(self.dim, dtype=np.float32)\n        \n        # 1. 使用摘要索引进行初步筛选（获取更多结果）\n        try:\n            summary_indices, summary_distances = self.summary_index.get_nns_by_vector(\n                query_embedding_arr, \n                top_k * 3,  # 获取更多结果用于二次筛选\n                include_distances=True,\n                search_k=-1\n            )\n        except Exception as e:\n            logger.error(f\"Error in summary similarity search: {str(e)}\")\n            summary_indices, summary_distances = [], []\n        \n        # 2. 对摘要筛选结果使用主索引进行精排\n        results = []\n        for idx, angular_dist in zip(summary_indices, summary_distances):\n            if idx < len(self.metadata) and idx < len(self.chunk_ids):\n                # 计算余弦相似度\n                cosine_sim = 1 - (angular_dist ** 2) / 2.0\n                cosine_sim = max(-1.0, min(1.0, cosine_sim))\n                \n                # 获取主索引相似度作为精排分数\n                try:\n                    item_vector = self.index.get_item_vector(idx)\n                    main_cosine_sim = np.dot(query_embedding_arr, item_vector)\n                    main_cosine_sim = max(-1.0, min(1.0, main_cosine_sim))\n                except:\n                    main_cosine_sim = cosine_sim\n                \n                # 使用主索引相似度作为最终分数\n                results.append((\n                    main_cosine_sim,  \n                    self.chunk_ids[idx], \n                    self.metadata[idx],\n                    cosine_sim  # 摘要相似度（用于调试）\n                ))\n        \n        # 按精排分数降序排序并取top_k\n        sorted_results = sorted(results, key=lambda x: x[0], reverse=True)[:top_k]\n        return [(score, chunk_id, data) for score, chunk_id, data, _ in sorted_results]\n    \n    def save_index(self):\n        if self.index is None or self.summary_index is None:\n            logger.warning(\"Index is None, cannot save\")\n            return\n            \n        self.index.save(str(self.index_path))\n        self.summary_index.save(str(self.summary_index_path))\n        # 保存元数据和块ID\n        with open(self.metadata_path, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"chunks\": self.metadata,\n                \"chunk_ids\": self.chunk_ids\n            }, f, ensure_ascii=False, indent=2)\n        logger.info(f\"Saved Annoy index with {len(self.metadata)} chunks\")"
    },
    {
      "filename": "backend\\__init__.py",
      "content": ""
    },
    {
      "filename": "backend\\ai_service.py",
      "content": "import requests\nfrom typing import Dict, Any, Optional\nfrom RAG import initialize_rag_system\n\nclass AIService:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.model_type = config.get('model_type', 'ollama')\n        self.model_name = config.get('model_name', 'qwen:7b')\n        self.rag_retriever = initialize_rag_system()\n        \n    def generate_response(self, prompt: str, use_rag: bool = False) -> str:\n        \"\"\"生成AI回复，支持RAG\"\"\"\n        rag_context = None\n        if use_rag:\n            rag_context = self.rag_retriever.retrieve(prompt)\n        \n        full_prompt = self._build_prompt(prompt, rag_context)\n        \n        if self.model_type == 'ollama':\n            return self._call_ollama(full_prompt)\n        elif self.model_type == 'openai':\n            return self._call_openai(full_prompt)\n       \n        \n    def _build_prompt(self, prompt: str, context: Optional[str]) -> str:\n        \"\"\"构建最终提示词，整合RAG内容\"\"\"\n        if context:\n            return (\n                f\"<|im_start|>system\\n\"\n                f\"你是一个AI助手，请基于以下上下文信息回答问题。每个上下文片段包含摘要和详细内容：\\n\\n\"\n                f\"{context}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>user\\n\"\n                f\"{prompt}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>assistant\\n\"\n            )\n        return (\n            f\"<|im_start|>user\\n\"\n            f\"{prompt}\\n\"\n            f\"<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n    \n    def _call_ollama(self, prompt: str) -> str:\n        \"\"\"调用本地Ollama服务\"\"\"\n        try:\n            url = \"http://localhost:11434/api/generate\"\n            payload = {\n                \"model\": self.model_name,\n                \"prompt\": prompt,\n                \"stream\": False\n            }\n            response = requests.post(url, json=payload, timeout=300)\n            return response.json().get(\"response\", \"\")\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def _call_openai(self, prompt: str) -> str:\n        \"\"\"调用OpenAI API（预留）\"\"\"\n        # 实际使用时替换为真实API调用\n        return f\"OpenAI response to: {prompt}\"\n    \n    # backend/ai_service.py\n    def update_config(self, new_config: Dict[str, Any]):\n        self.config.update(new_config)\n        self.model_type = self.config.get('model_type', self.model_type)\n        self.model_name = self.config.get('model_name', self.model_name)\n        # 重新初始化 Retriever\n        self.rag_retriever = initialize_rag_system()"
    },
    {
      "filename": "backend\\main.py",
      "content": "# backend/main.py\nfrom fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nimport sys\nfrom pathlib import Path\n\nBASE_DIR = Path(__file__).resolve().parent.parent\nsys.path.append(str(BASE_DIR / \"backend\"))\n\nfrom ai_service import AIService\nimport os\nfrom RAG.document_loader import DocumentLoader\nfrom fastapi.responses import FileResponse\nfrom RAG import initialize_rag_system, build_vector_store  # 直接导入build_vector_store\n\nBASE_DIR = Path(__file__).resolve().parent.parent\napp = FastAPI()\n# 添加静态文件服务\n    \napp.mount(\"/static\", StaticFiles(directory=BASE_DIR / \"frontend\"), name=\"static\")\n\n# 允许跨域\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 初始化服务\nai_config = {\"model_type\": \"ollama\", \"model_name\": \"qwen:7b\"}\nai_service = AIService(ai_config)\n\nclass ChatRequest(BaseModel):\n    message: str\n    use_rag: bool = False\n\n@app.post(\"/chat\")\nasync def chat_endpoint(request: ChatRequest):\n    try:\n        response = ai_service.generate_response(request.message, request.use_rag)\n        return {\"response\": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/update_config\")\nasync def update_config(new_config: dict):\n    ai_service.update_config(new_config)\n    return {\"status\": \"config updated\"}\n\n# 重建索引端点\n@app.post(\"/rebuild_index\")\nasync def rebuild_index():\n    # 直接调用构建函数\n    success = build_vector_store()\n    if success:\n        # 重建完成后更新检索器\n        ai_service.rag_retriever = initialize_rag_system()\n        return {\"status\": \"index rebuilt successfully\"}\n    else:\n        return {\"status\": \"failed to rebuild index\", \"error\": \"no documents or chunks found\"}\n\n# 上传文档端点\n@app.post(\"/upload_document\")\nasync def upload_document(file: UploadFile = File(...)):\n    try:\n        documents_dir = BASE_DIR / \"data\" / \"documents\"\n        documents_dir.mkdir(parents=True, exist_ok=True)\n        file_path = documents_dir / file.filename\n        with open(file_path, \"wb\") as f:\n            content = await file.read()\n            f.write(content)\n        \n        # 自动触发重建索引\n        success = build_vector_store()\n        if success:\n            # 重建完成后更新检索器\n            ai_service.rag_retriever = initialize_rag_system()\n            return {\"status\": \"success\", \"file_path\": str(file_path)}\n        else:\n            return {\"status\": \"file uploaded but failed to rebuild index\"}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# 手动构建嵌入端点\n@app.post(\"/build_embeddings\")\nasync def build_embeddings():\n    \"\"\"手动触发向量嵌入过程\"\"\"\n    success = build_vector_store()\n    if success:\n        # 重建完成后更新检索器\n        ai_service.rag_retriever = initialize_rag_system()\n        return {\"status\": \"embeddings built successfully\"}\n    else:\n        return {\"status\": \"failed to build embeddings\", \"error\": \"no documents or chunks found\"}\n    \n# 添加前端服务\n@app.get(\"/\")\nasync def serve_frontend():\n    return FileResponse(BASE_DIR / \"frontend\" / \"index.html\")\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
    },
    {
      "filename": "backend\\requirements.txt",
      "content": "# requirements.txt\nfastapi>=0.68.0\nuvicorn>=0.18.3\nrequests>=2.31.0\nnumpy>=1.24.3\nPyPDF2>=3.0.1\npython-docx>=0.8.11\nmarkdown>=3.4.1\nlangchain-text-splitters>=0.0.1\nannoy>=1.17.0\ntqdm>=4.66.1  # 添加进度条库"
    },
    {
      "filename": "build_embeddings.py",
      "content": "# build_embeddings.py\nimport logging\nfrom RAG import build_vector_store\nimport time\nimport sys\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"embedding_builder\")\n\n# 修复进度显示函数 - 移除冗余的stage参数\ndef print_progress(**kwargs):\n    \"\"\"打印进度信息\"\"\"\n    stage = kwargs.get(\"stage\", \"process\")\n    total = kwargs.get(\"total\", 0)\n    current = kwargs.get(\"current\", 0)\n    message = kwargs.get(\"message\", \"\")\n    details = kwargs.get(\"details\", \"\")\n    status = kwargs.get(\"status\", \"progress\")\n    \n    if stage == \"load\":\n        prefix = \"加载文档\"\n    elif stage == \"split\":\n        prefix = \"分割文本\"\n    elif stage == \"embed\":\n        prefix = \"生成嵌入\"\n    elif stage == \"index\":\n        prefix = \"构建索引\"\n    else:\n        prefix = \"处理中\"\n    \n    if status == \"error\":\n        symbol = \"❌\"\n    elif status == \"completed\":\n        symbol = \"✅\"\n    else:\n        symbol = \"🔄\"\n    \n    if total > 0:\n        percent = current / total * 100\n        progress_bar = f\"[{'=' * int(percent/5)}{' ' * (20 - int(percent/5))}] {percent:.1f}%\"\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {progress_bar} - {message} {details}\")\n    else:\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {message} {details}\")\n    \n    sys.stdout.flush()\n    \n    if status in [\"completed\", \"error\"]:\n        print()  # 完成时换行\n\ndef main():\n    logger.info(\"Starting manual embedding process...\")\n    start_time = time.time()\n    \n    # 调用构建函数，传入进度回调\n    success = build_vector_store(progress_callback=print_progress)\n    \n    elapsed = time.time() - start_time\n    if success:\n        logger.info(f\"Embedding process completed successfully in {elapsed:.2f} seconds!\")\n    else:\n        logger.error(f\"Embedding process failed in {elapsed:.2f} seconds. Check logs for details.\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "config.py",
      "content": "import os\n\n# 基础路径配置\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nDOCUMENTS_DIR = os.path.join(DATA_DIR, 'documents')\nVECTOR_STORE_DIR = os.path.join(DATA_DIR, 'vector_store')\n\n# RAG配置\nRAG_CONFIG = {\n    # 文档加载配置\n    \"document_loader\": {\n        \"extensions\": [\".txt\", \".pdf\", \".docx\", \".pptx\", \".md\", \".json\", \".csv\"]\n    },\n    \n    # 文本分割配置 \n    \"text_splitter\": {\n        \"chunk_size\": 1500, \n        \"chunk_overlap\": 100  \n    },\n    \n    # 嵌入模型配置 \n    \"embeddings\": {\n        \"model_type\": \"ollama\",  # ollama 或 huggingface\n        \"model_name\": \"all-minilm\"  \n    },\n    \n    # 向量存储配置\n    \"vector_store\": {\n        \"type\": \"annoy\",  \n        \"index_name\": \"document_index\"\n    },\n    \n    # 检索器配置\n    \"retriever\": {\n        \"top_k\": 20,\n        \"score_threshold\": -1.0\n    },\n    \n    # 新增摘要配置\n    \"summarizer\": {\n        \"model_name\": \"qwen:7b\",  # 使用Qwen模型生成摘要\n        \"max_summary_length\": 15  # 摘要最大长度（字数）\n    }\n}"
    },
    {
      "filename": "diagnose.py",
      "content": "# diagnose.py\nimport logging\nfrom RAG import initialize_rag_system\nfrom RAG.retriever import Retriever\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef test_rag():\n    # 1. 初始化RAG\n    logger.info(\"=== 初始化RAG系统 ===\")\n    retriever = initialize_rag_system()\n    \n    # 新增摘要层测试\n    logger.info(\"\\n=== 测试语义摘要层 ===\")\n    test_queries = [\n        \"人如何自律\",\n        \"html是什么\",\n        \"what is codeAID\",\n        \"荀子有哪些名言\"\n    ]\n    \n    for query in test_queries:\n        logger.info(f\"\\n摘要查询: '{query}'\")\n        # 直接调用向量存储的摘要搜索\n        from RAG.vector_store import VectorStore\n        vector_store = VectorStore()\n        vector_store.load_index()\n        \n        # 生成查询嵌入\n        from RAG.embeddings import EmbeddingModel\n        embedding_model = EmbeddingModel()\n        query_embedding = embedding_model.embed_texts([query])[0]\n        \n        if query_embedding:\n            # 使用摘要索引搜索\n            results = vector_store.summary_index.get_nns_by_vector(\n                query_embedding, \n                5,  # 获取前5个结果\n                include_distances=True\n            )\n            \n            indices, distances = results\n            logger.info(f\"找到 {len(indices)} 个相关摘要:\")\n            for i, (idx, dist) in enumerate(zip(indices, distances)):\n                if idx < len(vector_store.metadata):\n                    metadata = vector_store.metadata[idx]\n                    # 计算余弦相似度\n                    cosine_sim = 1 - (dist ** 2) / 2.0\n                    \n                    # 输出摘要和对应内容\n                    logger.info(f\"{i+1}. [相似度: {cosine_sim:.3f}] 摘要: '{metadata['summary']}'\")\n                    logger.info(f\"   来源: {metadata['source']}\")\n                    logger.info(f\"   内容: {metadata['text'][:500]}...\")  # 输出前200个字符\n                    logger.info(\"\\n\")\n        else:\n            logger.warning(f\"无法生成查询 '{query}' 的嵌入向量\")\n\nif __name__ == \"__main__\":\n    test_rag()"
    },
    {
      "filename": "frontend\\index.html",
      "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Chat Module</title>\n    <link rel=\"stylesheet\" href=\"/static/style.css\">\n    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/dompurify@3.0.5/dist/purify.min.js\"></script>\n</head>\n<body>\n    <div class=\"chat-container\">\n        <div id=\"chat-history\" class=\"chat-history\"></div>\n        \n        <div class=\"input-area\">\n            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message...\">\n            <button onclick=\"sendMessage()\">Send</button>\n            <label>\n                <input type=\"checkbox\" id=\"use-rag\"> Use RAG\n            </label>\n        </div>\n        \n        <div class=\"document-upload\">\n            <h3>Document Upload</h3>\n            <input type=\"file\" id=\"document-file\">\n            <button onclick=\"uploadDocument()\">Upload</button>\n            <div id=\"upload-status\"></div>\n        </div>\n\n        <div class=\"config-panel\">\n            <h3>Configuration</h3>\n            <select id=\"model-type\">\n                <option value=\"ollama\">Ollama (Local)</option>\n                <option value=\"openai\">OpenAI API</option>\n            </select>\n            <input type=\"text\" id=\"model-name\" placeholder=\"Model name\" value=\"qwen:7b\">\n            <button onclick=\"updateConfig()\">Update Config</button>\n        </div>\n    </div>\n\n    <script src=\"/static/script.js\"></script>\n</body>\n</html>"
    },
    {
      "filename": "frontend\\script.js",
      "content": "let chatHistory = [];\n\nasync function sendMessage() {\n    const input = document.getElementById('user-input');\n    const message = input.value.trim();\n    const useRag = document.getElementById('use-rag').checked;\n    \n    if (!message) return;\n    \n    // 添加用户消息\n    addMessage('user', message);\n    input.value = '';\n    \n    try {\n        // 发送到后端\n        const response = await fetch('http://localhost:8000/chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                message: message,\n                use_rag: useRag\n            })\n        });\n        \n        const data = await response.json();\n        addMessage('ai', data.response);\n    } catch (error) {\n        addMessage('ai', `Error: ${error.message}`);\n    }\n}\n\nasync function updateConfig() {\n    const modelType = document.getElementById('model-type').value;\n    const modelName = document.getElementById('model-name').value;\n    \n    try {\n        await fetch('http://localhost:8000/update_config', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                model_type: modelType,\n                model_name: modelName\n            })\n        });\n        alert('Configuration updated successfully!');\n    } catch (error) {\n        alert(`Update failed: ${error.message}`);\n    }\n}\nasync function buildEmbeddings() {\n    const statusDiv = document.getElementById('embedding-status');\n    statusDiv.textContent = \"正在生成...\";\n    \n    try {\n        const response = await fetch('http://localhost:8000/build_embeddings', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            }\n        });\n        \n        const data = await response.json();\n        statusDiv.textContent = data.status;\n    } catch (error) {\n        statusDiv.textContent = `生成失败: ${error.message}`;\n    }\n}\nasync function uploadDocument() {\n    const fileInput = document.getElementById('document-file');\n    const file = fileInput.files[0];\n    const statusDiv = document.getElementById('upload-status');\n    \n    if (!file) {\n        statusDiv.textContent = \"Please select a file to upload.\";\n        return;\n    }\n    statusDiv.textContent = \"Uploading...\";\n    \n    \n    try {\n        const formData = new FormData();\n        formData.append('file', file);\n        \n        const response = await fetch('http://localhost:8000/upload_document', {\n            method: 'POST',\n            body: formData\n        });\n        \n        const data = await response.json();\n        if (data.status === 'success') {\n            statusDiv.textContent = `upload success: ${data.file_path}`;\n            // 清空文件输入\n            fileInput.value = '';\n        } else {\n            statusDiv.textContent = `upload failed: ${data.detail || 'unknown error'}`;\n        }\n    } catch (error) {\n        statusDiv.textContent = `upload error: ${error.message}`;\n    }\n}\n\n// 添加页面加载事件\ndocument.addEventListener('DOMContentLoaded', () => {\n    // 初始加载完成后滚动到底部\n    const chatHistory = document.getElementById('chat-history');\n    chatHistory.scrollTop = chatHistory.scrollHeight;\n});\nfunction addMessage(role, content) {\n    const chatHistoryElement = document.getElementById('chat-history');\n    const messageDiv = document.createElement('div');\n    messageDiv.className = `message ${role}-message`;\n    \n    const contentDiv = document.createElement('div');\n    contentDiv.className = 'ai-message-content';\n    \n    // 安全渲染Markdown\n    if (role === 'ai') {\n        const sanitized = DOMPurify.sanitize(marked.parse(content));\n        contentDiv.innerHTML = sanitized;\n    } else {\n        contentDiv.textContent = content;\n    }\n    \n    messageDiv.appendChild(contentDiv);\n    chatHistoryElement.appendChild(messageDiv);\n    \n    // 滚动到底部\n    chatHistoryElement.scrollTop = chatHistoryElement.scrollHeight;\n    \n    // 保存历史\n    chatHistory.push({ role, content });\n}\n\n// 输入框回车发送\ndocument.getElementById('user-input').addEventListener('keypress', (e) => {\n    if (e.key === 'Enter') sendMessage();\n});"
    },
    {
      "filename": "frontend\\style.css",
      "content": "body {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    background-color: #f5f5f5;\n    display: flex;\n    justify-content: center;\n    padding: 20px;\n    line-height: 1.6;\n}\n\n.chat-container {\n    width: 100%;\n    max-width: 800px;\n    background: white;\n    border-radius: 10px;\n    box-shadow: 0 0 20px rgba(0,0,0,0.1);\n    overflow: hidden;\n    display: flex;\n    flex-direction: column;\n    height: 90vh;\n}\n.document-upload {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    flex-direction: column;\n    gap: 10px;\n}\n\n.document-upload h3 {\n    margin: 0 0 10px 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.document-upload input[type=\"file\"] {\n    padding: 8px;\n    background: white;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n}\n.chat-history {\n    flex: 1;\n    padding: 20px;\n    overflow-y: auto;\n    display: flex;\n    flex-direction: column;\n    gap: 15px;\n}\n\n.message {\n    max-width: 80%;\n    padding: 15px;\n    border-radius: 12px;\n    line-height: 1.6;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n}\n\n.user-message {\n    align-self: flex-end;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border-bottom-right-radius: 5px;\n}\n\n.ai-message {\n    align-self: flex-start;\n    background-color: #fafafa;\n    color: #333;\n    border-bottom-left-radius: 5px;\n}\n\n.ai-message-content {\n    overflow-wrap: break-word;\n}\n\n/* Markdown样式增强 */\n.markdown-content h1, .markdown-content h2, .markdown-content h3 {\n    margin-top: 1.2em;\n    margin-bottom: 0.6em;\n    padding-bottom: 0.2em;\n    border-bottom: 1px solid #eee;\n    color: #2c3e50;\n}\n\n.markdown-content h1 {\n    font-size: 1.8em;\n}\n\n.markdown-content h2 {\n    font-size: 1.5em;\n}\n\n.markdown-content h3 {\n    font-size: 1.3em;\n}\n\n.markdown-content p {\n    margin: 0.8em 0;\n}\n\n.markdown-content ul, .markdown-content ol {\n    margin: 0.8em 0;\n    padding-left: 1.5em;\n}\n\n.markdown-content li {\n    margin: 0.4em 0;\n}\n\n.markdown-content blockquote {\n    margin: 1em 0;\n    padding: 0.8em 1em;\n    background-color: #f8f9fa;\n    border-left: 4px solid #4e8cff;\n    color: #555;\n    border-radius: 0 8px 8px 0;\n}\n\n/* 代码块样式 - 淡蓝色背景 */\n.markdown-content pre {\n    background-color: #e6f7ff; /* 淡蓝色背景 */\n    padding: 15px;\n    border-radius: 8px;\n    overflow-x: auto;\n    margin: 1.2em 0;\n    box-shadow: inset 0 0 5px rgba(0,0,0,0.05);\n    border: 1px solid #c0e6ff;\n}\n\n.markdown-content code {\n    font-family: 'Fira Code', 'Consolas', monospace;\n    background-color: rgba(230, 247, 255, 0.3);\n    padding: 2px 6px;\n    border-radius: 4px;\n    color: #d63384;\n}\n\n.markdown-content pre code {\n    background: none;\n    padding: 0;\n    border-radius: 0;\n    color: #333;\n}\n\n/* 表格样式 */\n.markdown-content table {\n    width: 100%;\n    border-collapse: collapse;\n    margin: 1.2em 0;\n    background-color: #fff;\n    box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n}\n\n.markdown-content th, .markdown-content td {\n    padding: 10px 15px;\n    text-align: left;\n    border: 1px solid #e1e4e8;\n}\n\n.markdown-content th {\n    background-color: #f6f8fa;\n    font-weight: 600;\n}\n\n.markdown-content tr:nth-child(even) {\n    background-color: #fafbfc;\n}\n\n/* 流程图/图表容器样式 */\n.markdown-content .mermaid, \n.markdown-content .flowchart, \n.markdown-content .graphviz {\n    background-color: #a5c0cd; /* 淡蓝色背景 */\n    padding: 15px;\n    border-radius: 8px;\n    margin: 1.2em 0;\n    overflow-x: auto;\n    text-align: center;\n    border: 1px solid #c0e6ff;\n}\n\n/* 水平线样式 */\n.markdown-content hr {\n    border: 0;\n    height: 1px;\n    background: linear-gradient(to right, rgba(0,0,0,0), rgba(78,140,255,0.5), rgba(0,0,0,0));\n    margin: 1.5em 0;\n}\n\n/* 链接样式 */\n.markdown-content a {\n    color: #1e6bb8;\n    text-decoration: none;\n    border-bottom: 1px dashed #4e8cff;\n    transition: all 0.2s;\n}\n\n.markdown-content a:hover {\n    color: #0d4a9e;\n    border-bottom: 1px solid #0d4a9e;\n}\n\n/* 键盘标签样式 */\n.markdown-content kbd {\n    background-color: #f6f8fa;\n    border: 1px solid #d1d5da;\n    border-radius: 4px;\n    box-shadow: inset 0 -1px 0 #d1d5da;\n    color: #444d56;\n    display: inline-block;\n    font-family: monospace;\n    font-size: 0.9em;\n    line-height: 1;\n    padding: 3px 5px;\n    vertical-align: middle;\n}\n\n/* 提示框样式 */\n.markdown-content .tip, \n.markdown-content .note, \n.markdown-content .warning {\n    padding: 12px 15px;\n    margin: 1.2em 0;\n    border-radius: 8px;\n    border-left: 4px solid;\n}\n\n.markdown-content .tip {\n    background-color: #e6f7ff;\n    border-color: #4e8cff;\n}\n\n.markdown-content .note {\n    background-color: #fff8e6;\n    border-color: #ffc53d;\n}\n\n.markdown-content .warning {\n    background-color: #ffebee;\n    border-color: #f44336;\n}\n\n.input-area {\n    display: flex;\n    padding: 15px;\n    background: #f9f9f9;\n    border-top: 1px solid #eee;\n    gap: 10px;\n    align-items: center;\n}\n\n.input-area input {\n    flex: 1;\n    padding: 12px 18px;\n    border: 1px solid #ddd;\n    border-radius: 25px;\n    font-size: 16px;\n    outline: none;\n    transition: border-color 0.3s;\n}\n\n.input-area input:focus {\n    border-color: #4e8cff;\n    box-shadow: 0 0 0 2px rgba(78, 140, 255, 0.2);\n}\n\n.input-area button {\n    padding: 12px 24px;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border: none;\n    border-radius: 25px;\n    cursor: pointer;\n    font-weight: bold;\n    transition: all 0.2s;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n}\n\n.input-area button:hover {\n    background: linear-gradient(135deg, #3a75e0, #2a65d0);\n    transform: translateY(-2px);\n    box-shadow: 0 4px 8px rgba(0,0,0,0.15);\n}\n\n.config-panel {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    gap: 10px;\n    align-items: center;\n    flex-wrap: wrap;\n}\n\n.config-panel h3 {\n    margin: 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.config-panel select, .config-panel input {\n    padding: 8px 12px;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n    background: white;\n    font-size: 14px;\n}\n\n.config-panel button {\n    padding: 8px 15px;\n    background: #5a6268;\n    color: white;\n    border: none;\n    border-radius: 4px;\n    cursor: pointer;\n    transition: background 0.2s;\n}\n\n.config-panel button:hover {\n    background: #484e53;\n}\n\n/* 滚动条美化 */\n.chat-history::-webkit-scrollbar {\n    width: 8px;\n}\n\n.chat-history::-webkit-scrollbar-track {\n    background: #f1f1f1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb {\n    background: #c1c1c1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb:hover {\n    background: #a8a8a8;\n}"
    },
    {
      "filename": "run_demo.py",
      "content": "\"\"\"\n一键启动RAG系统，启动后端服务，打开前端页面\n\"\"\"\n\"\"\"run_demo.py\n一键启动RAG系统，启动后端服务，打开前端页面\n\"\"\"\nimport subprocess\nimport webbrowser\nimport time\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nimport requests\nimport logging\nimport threading\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif getattr(sys, 'frozen', False):\n    BASE_DIR = Path(sys.executable).parent\nelse:\n    BASE_DIR = Path(__file__).resolve().parent\n\n# 添加项目根目录到Python路径\nsys.path.append(str(BASE_DIR))\n\n# 进度显示函数\ndef demo_progress(stage, total=0, current=0, message=\"\", details=\"\", status=\"progress\"):\n    \"\"\"显示进度信息\"\"\"\n    if stage == \"rag_init\":\n        prefix = \"🧩 RAG初始化\"\n    elif stage == \"server\":\n        prefix = \"🚀 后端服务\"\n    elif stage == \"browser\":\n        prefix = \"🌐 浏览器\"\n    else:\n        prefix = \"⚙️  处理中\"\n    \n    if status == \"error\":\n        symbol = \"❌\"\n    elif status == \"completed\":\n        symbol = \"✅\"\n    else:\n        symbol = \"🔄\"\n    \n    if total > 0:\n        percent = current / total * 100\n        progress_bar = f\"[{'=' * int(percent/5)}{' ' * (20 - int(percent/5))}] {percent:.1f}%\"\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {progress_bar} - {message} {details}\")\n    else:\n        sys.stdout.write(f\"\\r{symbol} {prefix}: {message} {details}\")\n    \n    sys.stdout.flush()\n    \n    if status in [\"completed\", \"error\"]:\n        print()  # 完成时换行\n\ndef initialize_rag():\n    \"\"\"初始化RAG系统\"\"\"\n    # 检查Ollama服务是否运行\n    demo_progress(\n        stage=\"rag_init\",\n        message=\"检查Ollama服务...\",\n        status=\"progress\"\n    )\n    \n    try:\n        response = requests.get(\"http://localhost:11434\", timeout=5)\n        if response.status_code != 200:\n            logger.error(\"Ollama service not running. Please start Ollama first.\")\n            demo_progress(\n                stage=\"rag_init\",\n                message=\"Ollama服务未运行\",\n                status=\"error\"\n            )\n            return None\n    except Exception as e:\n        logger.error(f\"Ollama service not running: {str(e)}. Please start Ollama first.\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"Ollama服务错误: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n    \n    # 导入RAG初始化函数\n    try:\n        from RAG import initialize_rag_system\n    except ImportError as e:\n        logger.error(f\"Error importing RAG module: {str(e)}\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"导入RAG模块失败: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n    \n    # 创建RAG系统\n    demo_progress(\n        stage=\"rag_init\",\n        message=\"初始化系统...\",\n        status=\"progress\"\n    )\n    \n    try:\n        rag_retriever = initialize_rag_system(force_rebuild=False)\n        demo_progress(\n            stage=\"rag_init\",\n            message=\"RAG系统初始化成功\",\n            status=\"completed\"\n        )\n        return rag_retriever\n    except Exception as e:\n        logger.error(f\"Error initializing RAG system: {str(e)}\")\n        demo_progress(\n            stage=\"rag_init\",\n            message=f\"初始化失败: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n\ndef start_server():\n    \"\"\"启动后端服务器\"\"\"\n    demo_progress(\n        stage=\"server\",\n        message=\"正在启动后端服务...\",\n        status=\"progress\"\n    )\n    \n    try:\n        # 使用Popen启动服务器\n        process = subprocess.Popen(\n            [sys.executable, \"-m\", \"uvicorn\", \"backend.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n            cwd=BASE_DIR,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True\n        )\n        \n        # 等待服务器启动\n        time.sleep(3)\n        \n        # 检查服务器是否启动\n        try:\n            response = requests.get(\"http://localhost:8000\", timeout=5)\n            if response.status_code == 200:\n                demo_progress(\n                    stage=\"server\",\n                    message=\"后端服务启动成功\",\n                    status=\"completed\"\n                )\n                return process\n            else:\n                demo_progress(\n                    stage=\"server\",\n                    message=f\"后端返回状态码: {response.status_code}\",\n                    status=\"error\"\n                )\n                return None\n        except Exception as e:\n            demo_progress(\n                stage=\"server\",\n                message=f\"连接后端失败: {str(e)}\",\n                status=\"error\"\n            )\n            return None\n    except Exception as e:\n        demo_progress(\n            stage=\"server\",\n            message=f\"启动服务器失败: {str(e)}\",\n            status=\"error\"\n        )\n        return None\n\ndef run_demo():\n    # 初始化RAG系统\n    rag_retriever = initialize_rag()\n    if not rag_retriever:\n        logger.error(\"Failed to initialize RAG system. Exiting.\")\n        return\n    \n    # 启动后端\n    server_process = start_server()\n    if not server_process:\n        logger.error(\"Failed to start backend server. Exiting.\")\n        return\n    \n    # 打开前端\n    demo_progress(\n        stage=\"browser\",\n        message=\"正在打开聊天界面...\",\n        status=\"progress\"\n    )\n    time.sleep(1)  # 确保服务器完全启动\n    \n    try:\n        webbrowser.open('http://localhost:8000')\n        demo_progress(\n            stage=\"browser\",\n            message=\"聊天界面已打开\",\n            status=\"completed\"\n        )\n    except Exception as e:\n        demo_progress(\n            stage=\"browser\",\n            message=f\"打开浏览器失败: {str(e)}\",\n            status=\"error\"\n        )\n    \n    logger.info(\"Chat interface opened. Press Ctrl+C to stop.\")\n    \n    try:\n        # 打印服务器日志\n        def log_stream(stream, prefix):\n            for line in stream:\n                if line:  # 确保行不为空\n                    logger.info(f\"{prefix}: {line.strip()}\")\n        \n        # 启动线程捕获stdout和stderr\n        stdout_thread = threading.Thread(\n            target=log_stream, \n            args=(server_process.stdout, \"SERVER\"),\n            daemon=True\n        )\n        stderr_thread = threading.Thread(\n            target=log_stream, \n            args=(server_process.stderr, \"ERROR\"),\n            daemon=True\n        )\n        \n        stdout_thread.start()\n        stderr_thread.start()\n        \n        # 等待服务器进程结束\n        server_process.wait()\n    except KeyboardInterrupt:\n        logger.info(\"Stopping server...\")\n        server_process.terminate()\n        logger.info(\"Server stopped\")\n\nif __name__ == \"__main__\":\n    run_demo()"
    }
  ]
}