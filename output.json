{
  "tree": {
    "name": ".",
    "directories": [
      {
        "name": "RAG",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 0
          },
          {
            "name": "document_loader.py",
            "content_index": 1
          },
          {
            "name": "embeddings.py",
            "content_index": 2
          },
          {
            "name": "retriever.py",
            "content_index": 3
          },
          {
            "name": "text_splitter.py",
            "content_index": 4
          },
          {
            "name": "vector_store.py",
            "content_index": 5
          }
        ]
      },
      {
        "name": "backend",
        "directories": [],
        "files": [
          {
            "name": "__init__.py",
            "content_index": 6
          },
          {
            "name": "ai_service.py",
            "content_index": 7
          },
          {
            "name": "main.py",
            "content_index": 8
          },
          {
            "name": "requirements.txt",
            "content_index": 9
          }
        ]
      },
      {
        "name": "frontend",
        "directories": [],
        "files": [
          {
            "name": "index.html",
            "content_index": 12
          },
          {
            "name": "script.js",
            "content_index": 13
          },
          {
            "name": "style.css",
            "content_index": 14
          }
        ]
      }
    ],
    "files": [
      {
        "name": "build_embeddings.py",
        "content_index": 10
      },
      {
        "name": "config.py",
        "content_index": 11
      },
      {
        "name": "run_demo.py",
        "content_index": 15
      }
    ]
  },
  "code_blocks": [
    {
      "filename": "RAG\\__init__.py",
      "content": "# RAG/__init__.py\nfrom .document_loader import DocumentLoader\nfrom .text_splitter import TextSplitter\nfrom .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom .retriever import Retriever\nfrom config import DOCUMENTS_DIR, VECTOR_STORE_DIR\nimport os\nimport time\nfrom tqdm import tqdm\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef initialize_rag_system(force_rebuild=False):\n    \"\"\"初始化RAG系统\"\"\"\n    vector_store = VectorStore()\n    if not force_rebuild and vector_store.index_path.exists() and vector_store.metadata_path.exists():\n        logger.info(\"Using existing vector store\")\n        return Retriever()\n    \n    # 需要构建向量库\n    logger.info(\"Vector store not found or incomplete. Building new vector store...\")\n    if build_vector_store():\n        return Retriever()\n    else:\n        logger.error(\"Failed to build vector store\")\n        return Retriever()\n\ndef build_vector_store():\n    \"\"\"手动构建向量存储\"\"\"\n    logger.info(\"Building new vector store...\")\n    \n    # 加载文档\n    loader = DocumentLoader()\n    documents = loader.load_documents()\n    if not documents:\n        logger.warning(\"No documents found to build vector store\")\n        return False\n    \n    # 分割文档\n    splitter = TextSplitter()\n    chunks = splitter.split_documents(documents)\n    logger.info(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n    \n    # 检查是否有文本块\n    if not chunks:\n        logger.warning(\"No text chunks created from documents\")\n        return False\n    \n    # 生成嵌入\n    embedding_model = EmbeddingModel()\n    texts = [chunk[\"text\"] for chunk in chunks]\n    \n    # 分批处理嵌入生成\n    batch_size = 32\n    embeddings = []\n    logger.info(\"Generating embeddings...\")\n    start_time = time.time()\n    \n    # 使用tqdm显示进度条\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding chunks\"):\n        batch_texts = texts[i:i+batch_size]\n        batch_embeddings = embedding_model.embed_texts(batch_texts)\n        \n        # 检查嵌入是否有效\n        valid_embeddings = []\n        for emb in batch_embeddings:\n            if emb and len(emb) == embedding_model.dim:\n                valid_embeddings.append(emb)\n            else:\n                logger.warning(f\"Invalid embedding at batch index {i}\")\n                valid_embeddings.append([0.0] * embedding_model.dim)  # 使用零向量占位\n        \n        embeddings.extend(valid_embeddings)\n    \n    # 检查嵌入数量是否匹配\n    if len(embeddings) != len(chunks):\n        logger.warning(f\"Embeddings count ({len(embeddings)}) doesn't match chunks count ({len(chunks)})\")\n        # 填充缺失的嵌入\n        embeddings.extend([[0.0] * embedding_model.dim] * (len(chunks) - len(embeddings)))\n    \n    logger.info(f\"Embeddings generated in {time.time()-start_time:.2f} seconds\")\n    \n    # 添加到向量存储\n    vector_store = VectorStore()\n    vector_store.add_chunks(chunks, embeddings)\n    \n    logger.info(\"Vector store built successfully\")\n    return True"
    },
    {
      "filename": "RAG\\document_loader.py",
      "content": "import os\nfrom pathlib import Path\nfrom config import DOCUMENTS_DIR, RAG_CONFIG\nimport PyPDF2\nfrom docx import Document\nimport markdown\nimport re\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass DocumentLoader:\n    def __init__(self):\n        self.extensions = RAG_CONFIG[\"document_loader\"][\"extensions\"]\n        self.documents_dir = Path(DOCUMENTS_DIR)\n        self.documents_dir.mkdir(parents=True, exist_ok=True)\n    \n    def load_documents(self):\n        \"\"\"加载文档目录中的所有支持文档\"\"\"\n        documents = []\n        \n        for ext in self.extensions:\n            for file_path in self.documents_dir.glob(f\"*{ext}\"):\n                content = self._load_file(file_path)\n                if content:\n                    documents.append({\n                        \"file_path\": str(file_path),\n                        \"content\": content\n                    })\n                    logger.info(f\"Loaded document: {file_path.name}\")\n        \n        return documents\n    \n    def _load_file(self, file_path):\n        \"\"\"根据文件类型加载内容\"\"\"\n        ext = file_path.suffix.lower()\n        \n        try:\n            if ext == \".txt\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return f.read()\n            \n            elif ext == \".pdf\":\n                content = []\n                with open(file_path, 'rb') as f:\n                    pdf_reader = PyPDF2.PdfReader(f)\n                    for page in pdf_reader.pages:\n                        text = page.extract_text()\n                        if text:\n                            content.append(text)\n                return \"\\n\".join(content)\n            \n            elif ext == \".docx\":\n                doc = Document(file_path)\n                return \"\\n\".join([para.text for para in doc.paragraphs])\n            \n            elif ext == \".md\":\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    return markdown.markdown(f.read())\n            \n            else:\n                logger.warning(f\"Unsupported file type: {ext}\")\n                return None\n        except Exception as e:\n            logger.error(f\"Error loading {file_path}: {str(e)}\")\n            return None\n\n    def add_document(self, file_path):\n        \"\"\"添加单个文档到存储\"\"\"\n        dest_path = self.documents_dir / Path(file_path).name\n        try:\n            with open(file_path, 'rb') as src, open(dest_path, 'wb') as dst:\n                dst.write(src.read())\n            logger.info(f\"Added document: {dest_path}\")\n            return str(dest_path)\n        except Exception as e:\n            logger.error(f\"Failed to add document: {str(e)}\")\n            return None"
    },
    {
      "filename": "RAG\\embeddings.py",
      "content": "import requests\nimport numpy as np\nfrom typing import List\nfrom config import RAG_CONFIG\nimport logging\nimport time\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingModel:\n    def __init__(self):\n        config = RAG_CONFIG[\"embeddings\"]\n        self.model_type = config[\"model_type\"]\n        self.model_name = config[\"model_name\"]\n        self.dim = config.get(\"dim\", 384)\n        self.api_url = \"http://localhost:11434/api/embeddings\"\n    \n    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"将文本列表转换为嵌入向量\"\"\"\n        if not texts:\n            return []\n            \n        if self.model_type == \"ollama\":\n            return self._embed_with_ollama(texts)\n        elif self.model_type == \"huggingface\":\n            return self._embed_with_huggingface(texts)\n        else:\n            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n    \n    def _embed_with_ollama(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"使用Ollama API生成嵌入 - 增强错误处理和重试机制\"\"\"\n        embeddings = []\n        for text in texts:\n            if not text.strip():\n                embeddings.append([])\n                continue\n                \n            for attempt in range(3):  # 最多重试3次\n                try:\n                    response = requests.post(\n                        self.api_url,\n                        json={\n                            \"model\": self.model_name,\n                            \"prompt\": text\n                        },\n                        timeout=30\n                    )\n                    \n                    if response.status_code == 200:\n                        data = response.json()\n                        if \"embedding\" in data and data[\"embedding\"]:\n                            embedding = data[\"embedding\"]\n                            # 检查嵌入维度\n                            if len(embedding) == self.dim:\n                                embeddings.append(embedding)\n                                break\n                            else:\n                                logger.warning(f\"Embedding dimension mismatch: expected {self.dim}, got {len(embedding)}\")\n                                embeddings.append([])\n                                break\n                        else:\n                            logger.warning(f\"Empty embedding for text: {text[:50]}...\")\n                    else:\n                        logger.error(f\"Error embedding text: {response.status_code} - {response.text}\")\n                    \n                    # 最后一次尝试仍然失败\n                    if attempt == 2:\n                        logger.error(f\"Failed to generate embedding after 3 attempts for text: {text[:50]}...\")\n                        embeddings.append([])\n                except Exception as e:\n                    logger.error(f\"Ollama embedding error: {str(e)}\")\n                    if attempt == 2:\n                        embeddings.append([])\n        return embeddings\n    \n    def _embed_with_huggingface(self, texts: List[str]) -> List[List[float]]:\n        # 实现HuggingFace嵌入逻辑（根据您的需求）\n        return [[] for _ in texts]  # 示例返回"
    },
    {
      "filename": "RAG\\retriever.py",
      "content": "from .embeddings import EmbeddingModel\nfrom .vector_store import VectorStore\nfrom config import RAG_CONFIG\nfrom pathlib import Path\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass Retriever:\n    def __init__(self):\n        self.embedding_model = EmbeddingModel()\n        self.vector_store = VectorStore()\n        self.vector_store.load_index()\n        self.top_k = RAG_CONFIG[\"retriever\"][\"top_k\"]\n        self.score_threshold = RAG_CONFIG[\"retriever\"][\"score_threshold\"]\n    \n    def retrieve(self, query: str) -> str:\n        \"\"\"检索与查询相关的上下文\"\"\"\n        # 生成查询嵌入\n        query_embedding = self.embedding_model.embed_texts([query])\n        if not query_embedding or not query_embedding[0]:\n            logger.warning(f\"Failed to generate embedding for query: '{query}'\")\n            return \"\"\n        \n        # 确保嵌入向量格式正确\n        if not isinstance(query_embedding[0], list) or not all(isinstance(x, float) for x in query_embedding[0]):\n            logger.error(f\"Invalid embedding format for query: '{query}'\")\n            return \"\"\n        \n        # 执行相似度搜索\n        results = self.vector_store.similarity_search(\n            query_embedding[0], \n            top_k=self.top_k\n        )\n        \n        # 构建上下文\n        context = []\n        for score, chunk_id, chunk_data in results:\n            if score >= self.score_threshold:\n                context.append({\n                    \"text\": chunk_data[\"text\"],\n                    \"source\": chunk_data[\"source\"],\n                    \"score\": round(score, 3)\n                })\n        \n        # 格式化上下文\n        return self._format_context(context)\n    \n    def _format_context(self, context_items) -> str:\n        \"\"\"格式化检索到的上下文\"\"\"\n        if not context_items:\n            return \"没有找到相关上下文信息\"\n        \n        context_str = \"检索到的相关上下文信息：\\n\\n\"\n        for i, item in enumerate(context_items, 1):\n            source_name = Path(item[\"source\"]).name\n            context_str += f\"### 上下文片段 {i} (来源: {source_name}, 相似度: {item['score']})\\n\"\n            context_str += f\"{item['text']}\\n\\n\"\n        \n        return context_str.strip()"
    },
    {
      "filename": "RAG\\text_splitter.py",
      "content": "import re\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom config import RAG_CONFIG\nfrom pathlib import Path\n\nclass TextSplitter:\n    def __init__(self):\n        config = RAG_CONFIG[\"text_splitter\"]\n        self.splitter = RecursiveCharacterTextSplitter(\n            chunk_size=config[\"chunk_size\"],\n            chunk_overlap=config[\"chunk_overlap\"],\n            separators=[\"\\n\\n\", \"\\n\", \"。\", \"？\", \"！\", \"；\", \" \", \"\"],\n            keep_separator=True\n        )\n    \n    def split_documents(self, documents):\n        \"\"\"分割文档为文本块\"\"\"\n        chunks = []\n        for doc in documents:\n            content = doc[\"content\"]\n            # 清理多余空白\n            content = re.sub(r'\\s+', ' ', content).strip()\n            # 分割文本\n            split_texts = self.splitter.split_text(content)\n            \n            for i, text in enumerate(split_texts):\n                chunks.append({\n                    \"text\": text,\n                    \"source\": doc[\"file_path\"],\n                    \"chunk_id\": f\"{Path(doc['file_path']).stem}_{i}\"\n                })\n        \n        return chunks"
    },
    {
      "filename": "RAG\\vector_store.py",
      "content": "from annoy import AnnoyIndex\nimport json\nimport numpy as np\nimport os\nfrom pathlib import Path\nfrom config import VECTOR_STORE_DIR, RAG_CONFIG\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass VectorStore:\n    def __init__(self):\n        self.store_dir = Path(VECTOR_STORE_DIR)\n        self.store_dir.mkdir(parents=True, exist_ok=True)\n        config = RAG_CONFIG[\"vector_store\"]\n        self.store_type = config[\"type\"]\n        self.index_name = config[\"index_name\"]\n        self.index_path = self.store_dir / f\"{self.index_name}.ann\"\n        self.metadata_path = self.store_dir / f\"{self.index_name}_metadata.json\"\n        self.index = None\n        self.metadata = []\n        self.chunk_ids = []  # 存储块ID列表\n        \n        # 从嵌入配置获取维度\n        embedding_config = RAG_CONFIG[\"embeddings\"]\n        self.dim = embedding_config.get(\"dim\", 384)\n        \n        self.distance_metric = \"angular\"  # 余弦相似度\n        \n        # 确保索引对象被正确创建\n        self.load_index()\n        \n        # 双重检查索引对象\n        if self.index is None:\n            logger.warning(\"Index was None after load_index(), creating new index\")\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n\n    def load_index(self):\n        try:\n            if self.index_path.exists() and self.metadata_path.exists():\n                logger.info(f\"Loading existing index from {self.index_path}\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n                self.index.load(str(self.index_path))\n                \n                with open(self.metadata_path, 'r', encoding='utf-8') as f:\n                    metadata = json.load(f)\n                # 正确加载元数据和块ID\n                self.metadata = metadata.get(\"chunks\", [])\n                self.chunk_ids = metadata.get(\"chunk_ids\", [])\n                logger.info(f\"Loaded Annoy index with {len(self.metadata)} chunks\")\n            else:\n                logger.info(\"No existing index found, creating new index\")\n                self.index = AnnoyIndex(self.dim, self.distance_metric)\n        except Exception as e:\n            logger.error(f\"Error loading index: {str(e)}\")\n            # 创建新的索引作为回退\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n\n    def add_chunks(self, chunks, embeddings):\n        if not embeddings:\n            logger.warning(\"No embeddings provided, skipping add_chunks\")\n            return\n            \n        # 确保索引对象存在\n        if self.index is None:\n            logger.warning(\"Index is None, creating new index\")\n            self.index = AnnoyIndex(self.dim, self.distance_metric)\n        \n        # 重置元数据和块ID\n        self.metadata = []\n        self.chunk_ids = []\n        \n        # 添加向量到Annoy索引\n        valid_count = 0\n        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n            if not embedding or len(embedding) != self.dim:\n                logger.warning(f\"Skipping invalid embedding for chunk {i}\")\n                continue\n                \n            # 确保嵌入是浮点数列表\n            try:\n                embedding = [float(x) for x in embedding]\n                self.index.add_item(i, embedding)\n                self.metadata.append({\n                    \"text\": chunk[\"text\"],\n                    \"source\": chunk[\"source\"]\n                })\n                self.chunk_ids.append(chunk[\"chunk_id\"])\n                valid_count += 1\n            except Exception as e:\n                logger.error(f\"Error adding chunk {i}: {str(e)}\")\n        \n        if valid_count == 0:\n            logger.error(\"No valid embeddings added to index\")\n            return\n            \n        # 构建索引\n        logger.info(f\"Building index with {valid_count} items...\")\n        self.index.build(10)  # 默认10棵树\n        logger.info(\"Index built successfully\")\n        \n        # 保存索引\n        self.save_index()\n\n    def similarity_search(self, query_embedding, top_k=5):\n        if self.index is None:\n            logger.warning(\"Index is None, cannot perform search\")\n            return []\n            \n        if not query_embedding or len(query_embedding) != self.dim:\n            logger.error(f\"Invalid query embedding: expected dim={self.dim}, got {len(query_embedding) if query_embedding else 'none'}\")\n            return []\n            \n        # 确保查询嵌入是浮点数列表\n        try:\n            query_embedding = [float(x) for x in query_embedding]\n        except Exception as e:\n            logger.error(f\"Error converting query embedding: {str(e)}\")\n            return []\n        \n        # 获取最近邻\n        try:\n            indices, distances = self.index.get_nns_by_vector(\n                query_embedding, \n                top_k, \n                include_distances=True\n            )\n        except Exception as e:\n            logger.error(f\"Error in similarity_search: {str(e)}\")\n            return []\n            \n        results = []\n        for idx, distance in zip(indices, distances):\n            if idx < len(self.metadata) and idx < len(self.chunk_ids):\n                chunk_data = self.metadata[idx]\n                # 余弦距离转相似度 (1 - distance)\n                similarity = 1 - distance\n                results.append((similarity, self.chunk_ids[idx], chunk_data))\n        \n        return sorted(results, key=lambda x: x[0], reverse=True)\n\n    def save_index(self):\n        if self.index is None:\n            logger.warning(\"Index is None, cannot save\")\n            return\n            \n        self.index.save(str(self.index_path))\n        # 保存元数据和块ID\n        with open(self.metadata_path, 'w', encoding='utf-8') as f:\n            json.dump({\n                \"chunks\": self.metadata,\n                \"chunk_ids\": self.chunk_ids\n            }, f, ensure_ascii=False, indent=2)\n        logger.info(f\"Saved Annoy index with {len(self.metadata)} chunks\")"
    },
    {
      "filename": "backend\\__init__.py",
      "content": ""
    },
    {
      "filename": "backend\\ai_service.py",
      "content": "import requests\nfrom typing import Dict, Any, Optional\nfrom RAG import initialize_rag_system\n\nclass AIService:\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.model_type = config.get('model_type', 'ollama')\n        self.model_name = config.get('model_name', 'qwen:7b')\n        self.rag_retriever = initialize_rag_system()\n        \n    def generate_response(self, prompt: str, use_rag: bool = False) -> str:\n        \"\"\"生成AI回复，支持RAG\"\"\"\n        rag_context = None\n        if use_rag:\n            rag_context = self.rag_retriever.retrieve(prompt)\n        \n        full_prompt = self._build_prompt(prompt, rag_context)\n        \n        if self.model_type == 'ollama':\n            return self._call_ollama(full_prompt)\n        elif self.model_type == 'openai':\n            return self._call_openai(full_prompt)\n        # 可扩展其他API\n        \n    def _build_prompt(self, prompt: str, context: Optional[str]) -> str:\n        \"\"\"构建最终提示词，整合RAG内容\"\"\"\n        if context:\n            return (\n                f\"<|im_start|>system\\n\"\n                f\"你是一个AI助手，请基于以下上下文信息回答问题：\\n\\n\"\n                f\"{context}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>user\\n\"\n                f\"{prompt}\\n\"\n                f\"<|im_end|>\\n\"\n                f\"<|im_start|>assistant\\n\"\n            )\n        return (\n            f\"<|im_start|>user\\n\"\n            f\"{prompt}\\n\"\n            f\"<|im_end|>\\n\"\n            f\"<|im_start|>assistant\\n\"\n        )\n    \n    def _call_ollama(self, prompt: str) -> str:\n        \"\"\"调用本地Ollama服务\"\"\"\n        try:\n            url = \"http://localhost:11434/api/generate\"\n            payload = {\n                \"model\": self.model_name,\n                \"prompt\": prompt,\n                \"stream\": False\n            }\n            response = requests.post(url, json=payload, timeout=300)\n            return response.json().get(\"response\", \"\")\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def _call_openai(self, prompt: str) -> str:\n        \"\"\"调用OpenAI API（预留）\"\"\"\n        # 实际使用时替换为真实API调用\n        return f\"OpenAI response to: {prompt}\"\n    \n    # backend/ai_service.py\n    def update_config(self, new_config: Dict[str, Any]):\n        self.config.update(new_config)\n        self.model_type = self.config.get('model_type', self.model_type)\n        self.model_name = self.config.get('model_name', self.model_name)\n        # 重新初始化 Retriever\n        self.rag_retriever = initialize_rag_system()"
    },
    {
      "filename": "backend\\main.py",
      "content": "from fastapi import FastAPI, HTTPException, UploadFile, File\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom pydantic import BaseModel\nimport sys\nfrom pathlib import Path\n\nBASE_DIR = Path(__file__).resolve().parent.parent\nsys.path.append(str(BASE_DIR / \"backend\"))\n\nfrom ai_service import AIService\nimport os\nfrom RAG.document_loader import DocumentLoader\nfrom fastapi.responses import FileResponse\nfrom RAG import initialize_rag_system\n\nBASE_DIR = Path(__file__).resolve().parent.parent\napp = FastAPI()\n# 添加静态文件服务\n    \napp.mount(\"/static\", StaticFiles(directory=BASE_DIR / \"frontend\"), name=\"static\")\n\n# 允许跨域\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# 初始化服务\nai_config = {\"model_type\": \"ollama\", \"model_name\": \"qwen:7b\"}\nai_service = AIService(ai_config)\n\nclass ChatRequest(BaseModel):\n    message: str\n    use_rag: bool = False\n\n@app.post(\"/chat\")\nasync def chat_endpoint(request: ChatRequest):\n    try:\n        response = ai_service.generate_response(request.message, request.use_rag)\n        return {\"response\": response}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.post(\"/update_config\")\nasync def update_config(new_config: dict):\n    ai_service.update_config(new_config)\n    return {\"status\": \"config updated\"}\n\n# main.py中新增\n@app.post(\"/rebuild_index\")\nasync def rebuild_index():\n    ai_service.rag_retriever = initialize_rag_system(force_rebuild=True)\n    return {\"status\": \"index rebuilt\"}\n# 修改上传文档端点\n\n@app.post(\"/upload_document\")\nasync def upload_document(file: UploadFile = File(...)):\n    try:\n        documents_dir = BASE_DIR / \"data\" / \"documents\"\n        documents_dir.mkdir(parents=True, exist_ok=True)\n        file_path = documents_dir / file.filename\n        with open(file_path, \"wb\") as f:\n            content = await file.read()\n            f.write(content)\n        # 重新加载向量存储\n        ai_service.rag_retriever.vector_store.load_index()\n        return {\"status\": \"success\", \"file_path\": str(file_path)}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n    \n\n# backend/main.py (添加新端点)\n@app.post(\"/build_embeddings\")\nasync def build_embeddings():\n    \"\"\"手动触发向量嵌入过程\"\"\"\n    from RAG import build_vector_store\n    result = build_vector_store()\n    if result:\n        # 重建完成后更新检索器\n        ai_service.rag_retriever = initialize_rag_system()\n        return {\"status\": \"embeddings built successfully\"}\n    else:\n        return {\"status\": \"failed to build embeddings\", \"error\": \"no documents or chunks found\"}\n    \n    \n# 添加前端服务\n@app.get(\"/\")\nasync def serve_frontend():\n    return FileResponse(BASE_DIR / \"frontend\" / \"index.html\")\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n     "
    },
    {
      "filename": "backend\\requirements.txt",
      "content": "# requirements.txt\nfastapi>=0.68.0\nuvicorn>=0.18.3\nrequests>=2.31.0\nnumpy>=1.24.3\nPyPDF2>=3.0.1\npython-docx>=0.8.11\nmarkdown>=3.4.1\nlangchain-text-splitters>=0.0.1\nannoy>=1.17.0\ntqdm>=4.66.1  # 添加进度条库"
    },
    {
      "filename": "build_embeddings.py",
      "content": "# build_embeddings.py\nimport logging\nfrom RAG import build_vector_store\n\n# 配置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n)\nlogger = logging.getLogger(\"embedding_builder\")\n\ndef main():\n    logger.info(\"Starting manual embedding process...\")\n    \n    # 调用RAG模块中的构建函数\n    success = build_vector_store()\n    \n    if success:\n        logger.info(\"Embedding process completed successfully!\")\n    else:\n        logger.error(\"Embedding process failed. Check logs for details.\")\n        exit(1)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "filename": "config.py",
      "content": "import os\n\n# 基础路径配置\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nDOCUMENTS_DIR = os.path.join(DATA_DIR, 'documents')\nVECTOR_STORE_DIR = os.path.join(DATA_DIR, 'vector_store')\n\n# RAG配置\nRAG_CONFIG = {\n    # 文档加载配置\n    \"document_loader\": {\n        \"extensions\": [\".txt\", \".pdf\", \".docx\", \".pptx\", \".md\"]\n    },\n    \n    # 文本分割配置 \n    \"text_splitter\": {\n        \"chunk_size\": 1500, \n        \"chunk_overlap\": 100  \n    },\n    \n    # 嵌入模型配置 \n    \"embeddings\": {\n        \"model_type\": \"ollama\",  # ollama 或 huggingface\n        \"model_name\": \"all-minilm\"  \n    },\n    \n    # 向量存储配置\n    \"vector_store\": {\n        \"type\": \"annoy\",  \n        \"index_name\": \"document_index\"\n    },\n    \n    # 检索器配置\n    \"retriever\": {\n        \"top_k\": 4,\n        \"score_threshold\": 0.3\n    }\n}"
    },
    {
      "filename": "frontend\\index.html",
      "content": "<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>AI Chat Module</title>\n    <link rel=\"stylesheet\" href=\"/static/style.css\">\n    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/dompurify@3.0.5/dist/purify.min.js\"></script>\n</head>\n<body>\n    <div class=\"chat-container\">\n        <div id=\"chat-history\" class=\"chat-history\"></div>\n        \n        <div class=\"input-area\">\n            <input type=\"text\" id=\"user-input\" placeholder=\"Type your message...\">\n            <button onclick=\"sendMessage()\">Send</button>\n            <label>\n                <input type=\"checkbox\" id=\"use-rag\"> Use RAG\n            </label>\n        </div>\n        <!-- 在配置面板下方添加 -->\n<div class=\"document-upload\">\n    <h3>上传文档</h3>\n    <input type=\"file\" id=\"document-file\">\n    <button onclick=\"uploadDocument()\">上传</button>\n    <div id=\"upload-status\"></div>\n</div>\n\n        <div class=\"config-panel\">\n            <h3>Configuration</h3>\n            <select id=\"model-type\">\n                <option value=\"ollama\">Ollama (Local)</option>\n                <option value=\"openai\">OpenAI API</option>\n            </select>\n            <input type=\"text\" id=\"model-name\" placeholder=\"Model name\" value=\"qwen:7b\">\n            <button onclick=\"updateConfig()\">Update Config</button>\n        </div>\n    </div>\n\n    <script src=\"/static/script.js\"></script>\n</body>\n</html>"
    },
    {
      "filename": "frontend\\script.js",
      "content": "let chatHistory = [];\n\nasync function sendMessage() {\n    const input = document.getElementById('user-input');\n    const message = input.value.trim();\n    const useRag = document.getElementById('use-rag').checked;\n    \n    if (!message) return;\n    \n    // 添加用户消息\n    addMessage('user', message);\n    input.value = '';\n    \n    try {\n        // 发送到后端\n        const response = await fetch('http://localhost:8000/chat', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                message: message,\n                use_rag: useRag\n            })\n        });\n        \n        const data = await response.json();\n        addMessage('ai', data.response);\n    } catch (error) {\n        addMessage('ai', `Error: ${error.message}`);\n    }\n}\n\nasync function updateConfig() {\n    const modelType = document.getElementById('model-type').value;\n    const modelName = document.getElementById('model-name').value;\n    \n    try {\n        await fetch('http://localhost:8000/update_config', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            },\n            body: JSON.stringify({\n                model_type: modelType,\n                model_name: modelName\n            })\n        });\n        alert('Configuration updated successfully!');\n    } catch (error) {\n        alert(`Update failed: ${error.message}`);\n    }\n}\nasync function buildEmbeddings() {\n    const statusDiv = document.getElementById('embedding-status');\n    statusDiv.textContent = \"正在生成向量嵌入...这可能需要一段时间\";\n    \n    try {\n        const response = await fetch('http://localhost:8000/build_embeddings', {\n            method: 'POST',\n            headers: {\n                'Content-Type': 'application/json'\n            }\n        });\n        \n        const data = await response.json();\n        statusDiv.textContent = data.status;\n    } catch (error) {\n        statusDiv.textContent = `生成失败: ${error.message}`;\n    }\n}\nasync function uploadDocument() {\n    const fileInput = document.getElementById('document-file');\n    const file = fileInput.files[0];\n    const statusDiv = document.getElementById('upload-status');\n    \n    if (!file) {\n        statusDiv.textContent = \"请选择文件\";\n        return;\n    }\n    \n    statusDiv.textContent = \"上传中...\";\n    \n    try {\n        const formData = new FormData();\n        formData.append('file', file);\n        \n        const response = await fetch('http://localhost:8000/upload_document', {\n            method: 'POST',\n            body: formData\n        });\n        \n        const data = await response.json();\n        if (data.status === 'success') {\n            statusDiv.textContent = `上传成功: ${data.file_path}`;\n            // 清空文件输入\n            fileInput.value = '';\n        } else {\n            statusDiv.textContent = `上传失败: ${data.detail || '未知错误'}`;\n        }\n    } catch (error) {\n        statusDiv.textContent = `上传错误: ${error.message}`;\n    }\n}\n\n// 添加页面加载事件\ndocument.addEventListener('DOMContentLoaded', () => {\n    // 初始加载完成后滚动到底部\n    const chatHistory = document.getElementById('chat-history');\n    chatHistory.scrollTop = chatHistory.scrollHeight;\n});\nfunction addMessage(role, content) {\n    const chatHistoryElement = document.getElementById('chat-history');\n    const messageDiv = document.createElement('div');\n    messageDiv.className = `message ${role}-message`;\n    \n    const contentDiv = document.createElement('div');\n    contentDiv.className = 'ai-message-content';\n    \n    // 安全渲染Markdown\n    if (role === 'ai') {\n        const sanitized = DOMPurify.sanitize(marked.parse(content));\n        contentDiv.innerHTML = sanitized;\n    } else {\n        contentDiv.textContent = content;\n    }\n    \n    messageDiv.appendChild(contentDiv);\n    chatHistoryElement.appendChild(messageDiv);\n    \n    // 滚动到底部\n    chatHistoryElement.scrollTop = chatHistoryElement.scrollHeight;\n    \n    // 保存历史\n    chatHistory.push({ role, content });\n}\n\n// 输入框回车发送\ndocument.getElementById('user-input').addEventListener('keypress', (e) => {\n    if (e.key === 'Enter') sendMessage();\n});"
    },
    {
      "filename": "frontend\\style.css",
      "content": "body {\n    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n    background-color: #f5f5f5;\n    display: flex;\n    justify-content: center;\n    padding: 20px;\n    line-height: 1.6;\n}\n\n.chat-container {\n    width: 100%;\n    max-width: 800px;\n    background: white;\n    border-radius: 10px;\n    box-shadow: 0 0 20px rgba(0,0,0,0.1);\n    overflow: hidden;\n    display: flex;\n    flex-direction: column;\n    height: 90vh;\n}\n.document-upload {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    flex-direction: column;\n    gap: 10px;\n}\n\n.document-upload h3 {\n    margin: 0 0 10px 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.document-upload input[type=\"file\"] {\n    padding: 8px;\n    background: white;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n}\n.chat-history {\n    flex: 1;\n    padding: 20px;\n    overflow-y: auto;\n    display: flex;\n    flex-direction: column;\n    gap: 15px;\n}\n\n.message {\n    max-width: 80%;\n    padding: 15px;\n    border-radius: 12px;\n    line-height: 1.6;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.05);\n}\n\n.user-message {\n    align-self: flex-end;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border-bottom-right-radius: 5px;\n}\n\n.ai-message {\n    align-self: flex-start;\n    background-color: #fafafa;\n    color: #333;\n    border-bottom-left-radius: 5px;\n}\n\n.ai-message-content {\n    overflow-wrap: break-word;\n}\n\n/* Markdown样式增强 */\n.markdown-content h1, .markdown-content h2, .markdown-content h3 {\n    margin-top: 1.2em;\n    margin-bottom: 0.6em;\n    padding-bottom: 0.2em;\n    border-bottom: 1px solid #eee;\n    color: #2c3e50;\n}\n\n.markdown-content h1 {\n    font-size: 1.8em;\n}\n\n.markdown-content h2 {\n    font-size: 1.5em;\n}\n\n.markdown-content h3 {\n    font-size: 1.3em;\n}\n\n.markdown-content p {\n    margin: 0.8em 0;\n}\n\n.markdown-content ul, .markdown-content ol {\n    margin: 0.8em 0;\n    padding-left: 1.5em;\n}\n\n.markdown-content li {\n    margin: 0.4em 0;\n}\n\n.markdown-content blockquote {\n    margin: 1em 0;\n    padding: 0.8em 1em;\n    background-color: #f8f9fa;\n    border-left: 4px solid #4e8cff;\n    color: #555;\n    border-radius: 0 8px 8px 0;\n}\n\n/* 代码块样式 - 淡蓝色背景 */\n.markdown-content pre {\n    background-color: #e6f7ff; /* 淡蓝色背景 */\n    padding: 15px;\n    border-radius: 8px;\n    overflow-x: auto;\n    margin: 1.2em 0;\n    box-shadow: inset 0 0 5px rgba(0,0,0,0.05);\n    border: 1px solid #c0e6ff;\n}\n\n.markdown-content code {\n    font-family: 'Fira Code', 'Consolas', monospace;\n    background-color: rgba(230, 247, 255, 0.3);\n    padding: 2px 6px;\n    border-radius: 4px;\n    color: #d63384;\n}\n\n.markdown-content pre code {\n    background: none;\n    padding: 0;\n    border-radius: 0;\n    color: #333;\n}\n\n/* 表格样式 */\n.markdown-content table {\n    width: 100%;\n    border-collapse: collapse;\n    margin: 1.2em 0;\n    background-color: #fff;\n    box-shadow: 0 1px 3px rgba(0,0,0,0.1);\n}\n\n.markdown-content th, .markdown-content td {\n    padding: 10px 15px;\n    text-align: left;\n    border: 1px solid #e1e4e8;\n}\n\n.markdown-content th {\n    background-color: #f6f8fa;\n    font-weight: 600;\n}\n\n.markdown-content tr:nth-child(even) {\n    background-color: #fafbfc;\n}\n\n/* 流程图/图表容器样式 */\n.markdown-content .mermaid, \n.markdown-content .flowchart, \n.markdown-content .graphviz {\n    background-color: #a5c0cd; /* 淡蓝色背景 */\n    padding: 15px;\n    border-radius: 8px;\n    margin: 1.2em 0;\n    overflow-x: auto;\n    text-align: center;\n    border: 1px solid #c0e6ff;\n}\n\n/* 水平线样式 */\n.markdown-content hr {\n    border: 0;\n    height: 1px;\n    background: linear-gradient(to right, rgba(0,0,0,0), rgba(78,140,255,0.5), rgba(0,0,0,0));\n    margin: 1.5em 0;\n}\n\n/* 链接样式 */\n.markdown-content a {\n    color: #1e6bb8;\n    text-decoration: none;\n    border-bottom: 1px dashed #4e8cff;\n    transition: all 0.2s;\n}\n\n.markdown-content a:hover {\n    color: #0d4a9e;\n    border-bottom: 1px solid #0d4a9e;\n}\n\n/* 键盘标签样式 */\n.markdown-content kbd {\n    background-color: #f6f8fa;\n    border: 1px solid #d1d5da;\n    border-radius: 4px;\n    box-shadow: inset 0 -1px 0 #d1d5da;\n    color: #444d56;\n    display: inline-block;\n    font-family: monospace;\n    font-size: 0.9em;\n    line-height: 1;\n    padding: 3px 5px;\n    vertical-align: middle;\n}\n\n/* 提示框样式 */\n.markdown-content .tip, \n.markdown-content .note, \n.markdown-content .warning {\n    padding: 12px 15px;\n    margin: 1.2em 0;\n    border-radius: 8px;\n    border-left: 4px solid;\n}\n\n.markdown-content .tip {\n    background-color: #e6f7ff;\n    border-color: #4e8cff;\n}\n\n.markdown-content .note {\n    background-color: #fff8e6;\n    border-color: #ffc53d;\n}\n\n.markdown-content .warning {\n    background-color: #ffebee;\n    border-color: #f44336;\n}\n\n.input-area {\n    display: flex;\n    padding: 15px;\n    background: #f9f9f9;\n    border-top: 1px solid #eee;\n    gap: 10px;\n    align-items: center;\n}\n\n.input-area input {\n    flex: 1;\n    padding: 12px 18px;\n    border: 1px solid #ddd;\n    border-radius: 25px;\n    font-size: 16px;\n    outline: none;\n    transition: border-color 0.3s;\n}\n\n.input-area input:focus {\n    border-color: #4e8cff;\n    box-shadow: 0 0 0 2px rgba(78, 140, 255, 0.2);\n}\n\n.input-area button {\n    padding: 12px 24px;\n    background: linear-gradient(135deg, #4e8cff, #3a75e0);\n    color: white;\n    border: none;\n    border-radius: 25px;\n    cursor: pointer;\n    font-weight: bold;\n    transition: all 0.2s;\n    box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n}\n\n.input-area button:hover {\n    background: linear-gradient(135deg, #3a75e0, #2a65d0);\n    transform: translateY(-2px);\n    box-shadow: 0 4px 8px rgba(0,0,0,0.15);\n}\n\n.config-panel {\n    padding: 15px;\n    background: #f0f0f0;\n    border-top: 1px solid #ddd;\n    display: flex;\n    gap: 10px;\n    align-items: center;\n    flex-wrap: wrap;\n}\n\n.config-panel h3 {\n    margin: 0;\n    font-size: 16px;\n    color: #444;\n}\n\n.config-panel select, .config-panel input {\n    padding: 8px 12px;\n    border: 1px solid #ccc;\n    border-radius: 4px;\n    background: white;\n    font-size: 14px;\n}\n\n.config-panel button {\n    padding: 8px 15px;\n    background: #5a6268;\n    color: white;\n    border: none;\n    border-radius: 4px;\n    cursor: pointer;\n    transition: background 0.2s;\n}\n\n.config-panel button:hover {\n    background: #484e53;\n}\n\n/* 滚动条美化 */\n.chat-history::-webkit-scrollbar {\n    width: 8px;\n}\n\n.chat-history::-webkit-scrollbar-track {\n    background: #f1f1f1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb {\n    background: #c1c1c1;\n    border-radius: 4px;\n}\n\n.chat-history::-webkit-scrollbar-thumb:hover {\n    background: #a8a8a8;\n}"
    },
    {
      "filename": "run_demo.py",
      "content": "# run_demo.py\n\n\"\"\"\n一键启动RAG系统，启动后端服务，打开前端页面\n\"\"\"\nimport subprocess\nimport webbrowser\nimport time\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\nimport requests\nimport logging\n\n# 设置日志\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif getattr(sys, 'frozen', False):\n    BASE_DIR = Path(sys.executable).parent\nelse:\n    BASE_DIR = Path(__file__).resolve().parent\n\n# 添加项目根目录到Python路径\nsys.path.append(str(BASE_DIR))\n\ndef initialize_rag():\n    \"\"\"初始化RAG系统，强制重建向量索引\"\"\"\n    # # 删除现有向量库\n    # vector_store_dir = BASE_DIR / \"data\" / \"vector_store\"\n    # if vector_store_dir.exists():\n    #     logger.info(\"Removing existing vector store...\")\n    #     shutil.rmtree(vector_store_dir)\n    \n    # 检查Ollama服务是否运行\n    logger.info(\"Checking Ollama service...\")\n    try:\n        response = requests.get(\"http://localhost:11434\", timeout=5)\n        if response.status_code != 200:\n            logger.error(\"Ollama service not running. Please start Ollama first.\")\n            return None\n    except Exception as e:\n        logger.error(f\"Ollama service not running: {str(e)}. Please start Ollama first.\")\n        return None\n    \n    # 导入RAG初始化函数\n    try:\n        from RAG import initialize_rag_system\n    except ImportError as e:\n        logger.error(f\"Error importing RAG module: {str(e)}\")\n        return None\n    \n    # 创建RAG系统\n    logger.info(\"Initializing RAG system...\")\n    try:\n        rag_retriever = initialize_rag_system(force_rebuild=False)\n        logger.info(\"RAG system initialized successfully\")\n        return rag_retriever\n    except Exception as e:\n        logger.error(f\"Error initializing RAG system: {str(e)}\")\n        return None\n\ndef run_demo():\n    # 初始化RAG系统\n    rag_retriever = initialize_rag()\n    if not rag_retriever:\n        logger.error(\"Failed to initialize RAG system. Exiting.\")\n        return\n    \n    # 启动后端\n    logger.info(\"Starting backend server...\")\n    backend_process = subprocess.Popen(\n        [sys.executable, \"-m\", \"uvicorn\", \"backend.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n        cwd=BASE_DIR,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    \n    # 等待服务器启动\n    time.sleep(5)\n    \n    # 检查服务器是否启动\n    try:\n        response = requests.get(\"http://localhost:8000\", timeout=5)\n        if response.status_code == 200:\n            logger.info(\"Backend server started successfully\")\n        else:\n            logger.warning(f\"Backend returned status code: {response.status_code}\")\n    except Exception as e:\n        logger.error(f\"Failed to connect to backend: {str(e)}\")\n        backend_process.terminate()\n        return\n    \n    # 打开前端\n    logger.info(\"Opening chat interface in browser...\")\n    webbrowser.open('http://localhost:8000')\n    \n    logger.info(\"Chat interface opened. Press Ctrl+C to stop.\")\n    \n    try:\n        backend_process.wait()\n    except KeyboardInterrupt:\n        logger.info(\"Stopping server...\")\n        backend_process.terminate()\n        logger.info(\"Server stopped\")\n\nif __name__ == \"__main__\":\n    run_demo()"
    }
  ]
}